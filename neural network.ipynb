{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from typing import Literal, Union\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchinfo import summary\n",
    "\n",
    "import optuna\n",
    "from skorch import NeuralNetClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_scaler(X_train, scaler):\n",
    "    scaler.fit(X_train)\n",
    "    return scaler\n",
    "\n",
    "def scale_features(data, scaler):\n",
    "    return scaler.transform(data)\n",
    "\n",
    "def make_dataloader(X, y, batch_size: int = 1, shuffle: bool = True, seed: int = 0):\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y).type(torch.LongTensor)\n",
    "    tensor_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    loader = DataLoader(tensor_dataset, batch_size=batch_size, shuffle=shuffle, generator=torch.Generator().manual_seed(seed))\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/processed/embed_and_cat_multilingual.pkl\", \"rb\") as f:\n",
    "    embed_and_cat_multilingual = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, X_val, y_val  = [], [], [], [], [], []\n",
    "for lang, split_dict in embed_and_cat_multilingual.items(): \n",
    "    X_train += split_dict[\"train\"][\"embedding\"]\n",
    "    y_train += split_dict[\"train\"][\"category\"]\n",
    "    X_test += split_dict[\"test\"][\"embedding\"]\n",
    "    y_test += split_dict[\"test\"][\"category\"]\n",
    "    X_val += split_dict[\"validation\"][\"embedding\"]\n",
    "    y_val += split_dict[\"validation\"][\"category\"]\n",
    "X_train, y_train, X_test, y_test, X_val, y_val = np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test), np.array(X_val), np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler = fit_scaler(X_train=X_train, scaler=scaler)\n",
    "\n",
    "X_train_scaled = scale_features(X_train, scaler)\n",
    "X_test_scaled = scale_features(X_test, scaler)\n",
    "X_val_scaled = scale_features(X_val, scaler)\n",
    "\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "# y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "train_loader = make_dataloader(X=X_train_scaled, y=y_train, batch_size=64, shuffle=True, seed=42)\n",
    "test_loader = make_dataloader(X=X_test_scaled, y=y_test, batch_size=16, shuffle=False, seed=42)\n",
    "val_loader = make_dataloader(X=X_val_scaled, y=y_val, batch_size=1, shuffle=False, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Neural Network Constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    if torch.backends.mps.is_built():\n",
    "        device = \"mps\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "torch.manual_seed(42)\n",
    "if device==\"mps\":\n",
    "    torch.mps.manual_seed(42)\n",
    "\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "NNClassifier                             [1, 1, 7]                 --\n",
       "├─Sequential: 1-1                        [1, 1, 7]                 --\n",
       "│    └─Linear: 2-1                       [1, 1, 50]                51,250\n",
       "│    └─ReLU: 2-2                         [1, 1, 50]                --\n",
       "│    └─Linear: 2-3                       [1, 1, 100]               5,100\n",
       "│    └─ReLU: 2-4                         [1, 1, 100]               --\n",
       "│    └─Linear: 2-5                       [1, 1, 50]                5,050\n",
       "│    └─ReLU: 2-6                         [1, 1, 50]                --\n",
       "│    └─Linear: 2-7                       [1, 1, 15]                765\n",
       "│    └─ReLU: 2-8                         [1, 1, 15]                --\n",
       "│    └─Linear: 2-9                       [1, 1, 7]                 112\n",
       "==========================================================================================\n",
       "Total params: 62,277\n",
       "Trainable params: 62,277\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.06\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.25\n",
       "Estimated Total Size (MB): 0.26\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NNClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim_size: int,\n",
    "                 output_dim_size: int,\n",
    "                 layer_dims: list = [50,100,50,15],\n",
    "                 layer_acts: Union[list, str] = \"ReLU\",\n",
    "                 weight_init: init = init.kaiming_uniform_):\n",
    "        super(NNClassifier, self).__init__()\n",
    "        self.layers = []\n",
    "        if type(layer_acts)==str:\n",
    "            layer_acts = [getattr(torch.nn.modules.activation, layer_acts)()]*len(layer_dims)\n",
    "\n",
    "        for layer_no, layer_dim in enumerate(layer_dims):\n",
    "            if layer_no==0:\n",
    "                self.layers.append(nn.Linear(input_dim_size, layer_dim))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(layer_dims[layer_no-1], layer_dim))\n",
    "            self.layers.append(layer_acts[layer_no])\n",
    "        self.layers.append(nn.Linear(layer_dims[layer_no], output_dim_size))\n",
    "        for layer in self.layers:\n",
    "            if not isinstance(layer, tuple({getattr(torch.nn.modules.activation, act) for act in torch.nn.modules.activation.__all__})):\n",
    "                weight_init(layer.weight)\n",
    "\n",
    "        self.linear_layer_stack = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_layer_stack(x)\n",
    "\n",
    "# Example Regressor\n",
    "model = NNClassifier(input_dim_size=X_train.shape[1],\n",
    "                  output_dim_size=7,\n",
    "                  layer_dims=[50,100,50,15],\n",
    "                  layer_acts=[nn.ReLU(),nn.ReLU(),nn.ReLU(),nn.ReLU()])\n",
    "\n",
    "summary(model, input_size=(1,1,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy (a classification metric)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "  correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "  acc = (correct / len(y_pred)) * 100 \n",
    "  return acc\n",
    "\n",
    "def train_step(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = device,\n",
    "               verbose: bool = False):\n",
    "  train_loss, train_acc = 0, 0\n",
    "  model.to(device)\n",
    "  model.train()\n",
    "  for batch, (X_train, y_train) in enumerate(data_loader):\n",
    "    # Send data to GPU\n",
    "    X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "    # 1. Forward pass\n",
    "    y_logits = model(X_train) # model outputs raw logits \n",
    "    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -> prediction probabilities -> prediction labels\n",
    "    # Accumulate the loss values per batch\n",
    "    loss = loss_fn(y_logits, y_train)\n",
    "    # Accumulate loss and accuracy values per batch\n",
    "    train_loss += loss\n",
    "    train_acc += accuracy_fn(y_true=y_train, y_pred=y_pred)\n",
    "    \n",
    "    # 3. Calculate gradients and update parameters\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  # Calculate loss and accuracy per epoch and print out what's happening\n",
    "  train_loss /= len(data_loader)\n",
    "  train_acc /= len(data_loader)\n",
    "  if verbose:\n",
    "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "  return train_loss, train_acc\n",
    "\n",
    "\n",
    "def test_step(data_loader: torch.utils.data.DataLoader,\n",
    "              model: torch.nn.Module,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              accuracy_fn,\n",
    "              device: torch.device = device,\n",
    "              verbose: bool = False):\n",
    "  test_loss, test_acc = 0, 0\n",
    "  model.to(device)\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for X_test, y_test in data_loader:\n",
    "      # Send data to GPU\n",
    "      X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "      # 1. Forward pass\n",
    "      y_logits = model(X_test)\n",
    "      y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -> prediction probabilities -> prediction labels\n",
    "      # Accumulate the loss and accuracy values per batch\n",
    "      test_loss += loss_fn(y_logits, y_test)\n",
    "      test_acc += accuracy_fn(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "    # Adjust metrics and print out\n",
    "    test_loss /= len(data_loader)\n",
    "    test_acc /= len(data_loader)\n",
    "    if verbose:\n",
    "      print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")\n",
    "\n",
    "  return test_loss, test_acc\n",
    "\n",
    "\n",
    "def eval_model(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = device):\n",
    "  loss, acc = 0, 0\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for X, y in data_loader:\n",
    "      # Send data to GPU\n",
    "      X, y = X.to(device), y.to(device)\n",
    "      # 1. Forward pass\n",
    "      y_logits = model(X)\n",
    "      y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -> prediction probabilities -> prediction labels\n",
    "      # Accumulate the loss and accuracy values per batch\n",
    "      loss += loss_fn(y_logits, y)\n",
    "      acc += accuracy_fn(y_true=y, y_pred=y_pred)\n",
    "\n",
    "    # Scale loss and acc to find the average loss/acc per batch\n",
    "    loss /= len(data_loader)\n",
    "    acc /= len(data_loader)\n",
    "\n",
    "  return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
    "          \"model_loss\": loss.item(),\n",
    "          \"model_acc (%)\": acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "---------\n",
      "Train loss: 0.00286 | Train accuracy: 100.00%\n",
      "Test loss: 1.00398 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 1\n",
      "---------\n",
      "Train loss: 0.00287 | Train accuracy: 100.00%\n",
      "Test loss: 1.00427 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 2\n",
      "---------\n",
      "Train loss: 0.00287 | Train accuracy: 100.00%\n",
      "Test loss: 1.00459 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 3\n",
      "---------\n",
      "Train loss: 0.00286 | Train accuracy: 100.00%\n",
      "Test loss: 1.00488 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 4\n",
      "---------\n",
      "Train loss: 0.00285 | Train accuracy: 100.00%\n",
      "Test loss: 1.00517 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 5\n",
      "---------\n",
      "Train loss: 0.00284 | Train accuracy: 100.00%\n",
      "Test loss: 1.00546 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 6\n",
      "---------\n",
      "Train loss: 0.00285 | Train accuracy: 100.00%\n",
      "Test loss: 1.00577 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 7\n",
      "---------\n",
      "Train loss: 0.00284 | Train accuracy: 100.00%\n",
      "Test loss: 1.00601 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 8\n",
      "---------\n",
      "Train loss: 0.00282 | Train accuracy: 100.00%\n",
      "Test loss: 1.00630 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 9\n",
      "---------\n",
      "Train loss: 0.00280 | Train accuracy: 100.00%\n",
      "Test loss: 1.00657 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 10\n",
      "---------\n",
      "Train loss: 0.00287 | Train accuracy: 100.00%\n",
      "Test loss: 1.00688 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 11\n",
      "---------\n",
      "Train loss: 0.00278 | Train accuracy: 100.00%\n",
      "Test loss: 1.00712 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 12\n",
      "---------\n",
      "Train loss: 0.00278 | Train accuracy: 100.00%\n",
      "Test loss: 1.00743 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 13\n",
      "---------\n",
      "Train loss: 0.00276 | Train accuracy: 100.00%\n",
      "Test loss: 1.00771 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 14\n",
      "---------\n",
      "Train loss: 0.00278 | Train accuracy: 100.00%\n",
      "Test loss: 1.00798 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 15\n",
      "---------\n",
      "Train loss: 0.00274 | Train accuracy: 100.00%\n",
      "Test loss: 1.00825 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 16\n",
      "---------\n",
      "Train loss: 0.00280 | Train accuracy: 100.00%\n",
      "Test loss: 1.00844 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 17\n",
      "---------\n",
      "Train loss: 0.00274 | Train accuracy: 100.00%\n",
      "Test loss: 1.00872 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 18\n",
      "---------\n",
      "Train loss: 0.00274 | Train accuracy: 100.00%\n",
      "Test loss: 1.00899 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 19\n",
      "---------\n",
      "Train loss: 0.00276 | Train accuracy: 100.00%\n",
      "Test loss: 1.00925 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 20\n",
      "---------\n",
      "Train loss: 0.00273 | Train accuracy: 100.00%\n",
      "Test loss: 1.00951 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 21\n",
      "---------\n",
      "Train loss: 0.00273 | Train accuracy: 100.00%\n",
      "Test loss: 1.00981 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 22\n",
      "---------\n",
      "Train loss: 0.00277 | Train accuracy: 100.00%\n",
      "Test loss: 1.01002 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 23\n",
      "---------\n",
      "Train loss: 0.00269 | Train accuracy: 100.00%\n",
      "Test loss: 1.01030 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 24\n",
      "---------\n",
      "Train loss: 0.00267 | Train accuracy: 100.00%\n",
      "Test loss: 1.01058 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 25\n",
      "---------\n",
      "Train loss: 0.00268 | Train accuracy: 100.00%\n",
      "Test loss: 1.01084 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 26\n",
      "---------\n",
      "Train loss: 0.00267 | Train accuracy: 100.00%\n",
      "Test loss: 1.01109 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 27\n",
      "---------\n",
      "Train loss: 0.00266 | Train accuracy: 100.00%\n",
      "Test loss: 1.01137 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 28\n",
      "---------\n",
      "Train loss: 0.00269 | Train accuracy: 100.00%\n",
      "Test loss: 1.01162 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 29\n",
      "---------\n",
      "Train loss: 0.00268 | Train accuracy: 100.00%\n",
      "Test loss: 1.01193 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 30\n",
      "---------\n",
      "Train loss: 0.00265 | Train accuracy: 100.00%\n",
      "Test loss: 1.01222 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 31\n",
      "---------\n",
      "Train loss: 0.00260 | Train accuracy: 100.00%\n",
      "Test loss: 1.01247 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 32\n",
      "---------\n",
      "Train loss: 0.00266 | Train accuracy: 100.00%\n",
      "Test loss: 1.01274 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 33\n",
      "---------\n",
      "Train loss: 0.00261 | Train accuracy: 100.00%\n",
      "Test loss: 1.01304 | Test accuracy: 80.75%\n",
      "\n",
      "Epoch: 34\n",
      "---------\n",
      "Train loss: 0.00264 | Train accuracy: 100.00%\n",
      "Test loss: 1.01327 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 35\n",
      "---------\n",
      "Train loss: 0.00260 | Train accuracy: 100.00%\n",
      "Test loss: 1.01353 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 36\n",
      "---------\n",
      "Train loss: 0.00258 | Train accuracy: 100.00%\n",
      "Test loss: 1.01380 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 37\n",
      "---------\n",
      "Train loss: 0.00257 | Train accuracy: 100.00%\n",
      "Test loss: 1.01405 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 38\n",
      "---------\n",
      "Train loss: 0.00256 | Train accuracy: 100.00%\n",
      "Test loss: 1.01431 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 39\n",
      "---------\n",
      "Train loss: 0.00257 | Train accuracy: 100.00%\n",
      "Test loss: 1.01459 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 40\n",
      "---------\n",
      "Train loss: 0.00253 | Train accuracy: 100.00%\n",
      "Test loss: 1.01484 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 41\n",
      "---------\n",
      "Train loss: 0.00255 | Train accuracy: 100.00%\n",
      "Test loss: 1.01512 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 42\n",
      "---------\n",
      "Train loss: 0.00255 | Train accuracy: 100.00%\n",
      "Test loss: 1.01544 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 43\n",
      "---------\n",
      "Train loss: 0.00253 | Train accuracy: 100.00%\n",
      "Test loss: 1.01570 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 44\n",
      "---------\n",
      "Train loss: 0.00252 | Train accuracy: 100.00%\n",
      "Test loss: 1.01593 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 45\n",
      "---------\n",
      "Train loss: 0.00253 | Train accuracy: 100.00%\n",
      "Test loss: 1.01619 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 46\n",
      "---------\n",
      "Train loss: 0.00252 | Train accuracy: 100.00%\n",
      "Test loss: 1.01646 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 47\n",
      "---------\n",
      "Train loss: 0.00251 | Train accuracy: 100.00%\n",
      "Test loss: 1.01677 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 48\n",
      "---------\n",
      "Train loss: 0.00248 | Train accuracy: 100.00%\n",
      "Test loss: 1.01702 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 49\n",
      "---------\n",
      "Train loss: 0.00247 | Train accuracy: 100.00%\n",
      "Test loss: 1.01724 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 50\n",
      "---------\n",
      "Train loss: 0.00250 | Train accuracy: 100.00%\n",
      "Test loss: 1.01751 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 51\n",
      "---------\n",
      "Train loss: 0.00247 | Train accuracy: 100.00%\n",
      "Test loss: 1.01777 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 52\n",
      "---------\n",
      "Train loss: 0.00248 | Train accuracy: 100.00%\n",
      "Test loss: 1.01805 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 53\n",
      "---------\n",
      "Train loss: 0.00246 | Train accuracy: 100.00%\n",
      "Test loss: 1.01829 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 54\n",
      "---------\n",
      "Train loss: 0.00246 | Train accuracy: 100.00%\n",
      "Test loss: 1.01855 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 55\n",
      "---------\n",
      "Train loss: 0.00247 | Train accuracy: 100.00%\n",
      "Test loss: 1.01881 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 56\n",
      "---------\n",
      "Train loss: 0.00244 | Train accuracy: 100.00%\n",
      "Test loss: 1.01905 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 57\n",
      "---------\n",
      "Train loss: 0.00242 | Train accuracy: 100.00%\n",
      "Test loss: 1.01927 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 58\n",
      "---------\n",
      "Train loss: 0.00244 | Train accuracy: 100.00%\n",
      "Test loss: 1.01950 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 59\n",
      "---------\n",
      "Train loss: 0.00244 | Train accuracy: 100.00%\n",
      "Test loss: 1.01979 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 60\n",
      "---------\n",
      "Train loss: 0.00241 | Train accuracy: 100.00%\n",
      "Test loss: 1.02001 | Test accuracy: 80.86%\n",
      "\n",
      "Epoch: 61\n",
      "---------\n",
      "Train loss: 0.00246 | Train accuracy: 100.00%\n",
      "Test loss: 1.02025 | Test accuracy: 80.98%\n",
      "\n",
      "Epoch: 62\n",
      "---------\n",
      "Train loss: 0.00237 | Train accuracy: 100.00%\n",
      "Test loss: 1.02049 | Test accuracy: 80.98%\n",
      "\n",
      "Epoch: 63\n",
      "---------\n",
      "Train loss: 0.00241 | Train accuracy: 100.00%\n",
      "Test loss: 1.02073 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 64\n",
      "---------\n",
      "Train loss: 0.00237 | Train accuracy: 100.00%\n",
      "Test loss: 1.02093 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 65\n",
      "---------\n",
      "Train loss: 0.00242 | Train accuracy: 100.00%\n",
      "Test loss: 1.02118 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 66\n",
      "---------\n",
      "Train loss: 0.00236 | Train accuracy: 100.00%\n",
      "Test loss: 1.02142 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 67\n",
      "---------\n",
      "Train loss: 0.00235 | Train accuracy: 100.00%\n",
      "Test loss: 1.02165 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 68\n",
      "---------\n",
      "Train loss: 0.00233 | Train accuracy: 100.00%\n",
      "Test loss: 1.02188 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 69\n",
      "---------\n",
      "Train loss: 0.00236 | Train accuracy: 100.00%\n",
      "Test loss: 1.02207 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 70\n",
      "---------\n",
      "Train loss: 0.00232 | Train accuracy: 100.00%\n",
      "Test loss: 1.02232 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 71\n",
      "---------\n",
      "Train loss: 0.00233 | Train accuracy: 100.00%\n",
      "Test loss: 1.02258 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 72\n",
      "---------\n",
      "Train loss: 0.00236 | Train accuracy: 100.00%\n",
      "Test loss: 1.02280 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 73\n",
      "---------\n",
      "Train loss: 0.00233 | Train accuracy: 100.00%\n",
      "Test loss: 1.02306 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 74\n",
      "---------\n",
      "Train loss: 0.00237 | Train accuracy: 100.00%\n",
      "Test loss: 1.02331 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 75\n",
      "---------\n",
      "Train loss: 0.00234 | Train accuracy: 100.00%\n",
      "Test loss: 1.02353 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 76\n",
      "---------\n",
      "Train loss: 0.00232 | Train accuracy: 100.00%\n",
      "Test loss: 1.02378 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 77\n",
      "---------\n",
      "Train loss: 0.00228 | Train accuracy: 100.00%\n",
      "Test loss: 1.02400 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 78\n",
      "---------\n",
      "Train loss: 0.00231 | Train accuracy: 100.00%\n",
      "Test loss: 1.02426 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 79\n",
      "---------\n",
      "Train loss: 0.00229 | Train accuracy: 100.00%\n",
      "Test loss: 1.02449 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 80\n",
      "---------\n",
      "Train loss: 0.00228 | Train accuracy: 100.00%\n",
      "Test loss: 1.02469 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 81\n",
      "---------\n",
      "Train loss: 0.00226 | Train accuracy: 100.00%\n",
      "Test loss: 1.02491 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 82\n",
      "---------\n",
      "Train loss: 0.00228 | Train accuracy: 100.00%\n",
      "Test loss: 1.02514 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 83\n",
      "---------\n",
      "Train loss: 0.00224 | Train accuracy: 100.00%\n",
      "Test loss: 1.02538 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 84\n",
      "---------\n",
      "Train loss: 0.00225 | Train accuracy: 100.00%\n",
      "Test loss: 1.02562 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 85\n",
      "---------\n",
      "Train loss: 0.00225 | Train accuracy: 100.00%\n",
      "Test loss: 1.02586 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 86\n",
      "---------\n",
      "Train loss: 0.00226 | Train accuracy: 100.00%\n",
      "Test loss: 1.02611 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 87\n",
      "---------\n",
      "Train loss: 0.00226 | Train accuracy: 100.00%\n",
      "Test loss: 1.02630 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 88\n",
      "---------\n",
      "Train loss: 0.00226 | Train accuracy: 100.00%\n",
      "Test loss: 1.02650 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 89\n",
      "---------\n",
      "Train loss: 0.00222 | Train accuracy: 100.00%\n",
      "Test loss: 1.02674 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 90\n",
      "---------\n",
      "Train loss: 0.00222 | Train accuracy: 100.00%\n",
      "Test loss: 1.02695 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 91\n",
      "---------\n",
      "Train loss: 0.00220 | Train accuracy: 100.00%\n",
      "Test loss: 1.02718 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 92\n",
      "---------\n",
      "Train loss: 0.00220 | Train accuracy: 100.00%\n",
      "Test loss: 1.02744 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 93\n",
      "---------\n",
      "Train loss: 0.00220 | Train accuracy: 100.00%\n",
      "Test loss: 1.02767 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 94\n",
      "---------\n",
      "Train loss: 0.00220 | Train accuracy: 100.00%\n",
      "Test loss: 1.02792 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 95\n",
      "---------\n",
      "Train loss: 0.00219 | Train accuracy: 100.00%\n",
      "Test loss: 1.02817 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 96\n",
      "---------\n",
      "Train loss: 0.00219 | Train accuracy: 100.00%\n",
      "Test loss: 1.02837 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 97\n",
      "---------\n",
      "Train loss: 0.00217 | Train accuracy: 100.00%\n",
      "Test loss: 1.02861 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 98\n",
      "---------\n",
      "Train loss: 0.00215 | Train accuracy: 100.00%\n",
      "Test loss: 1.02884 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 99\n",
      "---------\n",
      "Train loss: 0.00218 | Train accuracy: 100.00%\n",
      "Test loss: 1.02906 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 100\n",
      "---------\n",
      "Train loss: 0.00216 | Train accuracy: 100.00%\n",
      "Test loss: 1.02930 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 101\n",
      "---------\n",
      "Train loss: 0.00215 | Train accuracy: 100.00%\n",
      "Test loss: 1.02953 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 102\n",
      "---------\n",
      "Train loss: 0.00216 | Train accuracy: 100.00%\n",
      "Test loss: 1.02976 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 103\n",
      "---------\n",
      "Train loss: 0.00216 | Train accuracy: 100.00%\n",
      "Test loss: 1.03000 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 104\n",
      "---------\n",
      "Train loss: 0.00214 | Train accuracy: 100.00%\n",
      "Test loss: 1.03023 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 105\n",
      "---------\n",
      "Train loss: 0.00213 | Train accuracy: 100.00%\n",
      "Test loss: 1.03049 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 106\n",
      "---------\n",
      "Train loss: 0.00215 | Train accuracy: 100.00%\n",
      "Test loss: 1.03065 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 107\n",
      "---------\n",
      "Train loss: 0.00219 | Train accuracy: 100.00%\n",
      "Test loss: 1.03090 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 108\n",
      "---------\n",
      "Train loss: 0.00211 | Train accuracy: 100.00%\n",
      "Test loss: 1.03114 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 109\n",
      "---------\n",
      "Train loss: 0.00211 | Train accuracy: 100.00%\n",
      "Test loss: 1.03135 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 110\n",
      "---------\n",
      "Train loss: 0.00212 | Train accuracy: 100.00%\n",
      "Test loss: 1.03156 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 111\n",
      "---------\n",
      "Train loss: 0.00210 | Train accuracy: 100.00%\n",
      "Test loss: 1.03180 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 112\n",
      "---------\n",
      "Train loss: 0.00216 | Train accuracy: 100.00%\n",
      "Test loss: 1.03199 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 113\n",
      "---------\n",
      "Train loss: 0.00210 | Train accuracy: 100.00%\n",
      "Test loss: 1.03221 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 114\n",
      "---------\n",
      "Train loss: 0.00212 | Train accuracy: 100.00%\n",
      "Test loss: 1.03241 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 115\n",
      "---------\n",
      "Train loss: 0.00207 | Train accuracy: 100.00%\n",
      "Test loss: 1.03264 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 116\n",
      "---------\n",
      "Train loss: 0.00207 | Train accuracy: 100.00%\n",
      "Test loss: 1.03286 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 117\n",
      "---------\n",
      "Train loss: 0.00209 | Train accuracy: 100.00%\n",
      "Test loss: 1.03310 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 118\n",
      "---------\n",
      "Train loss: 0.00205 | Train accuracy: 100.00%\n",
      "Test loss: 1.03332 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 119\n",
      "---------\n",
      "Train loss: 0.00206 | Train accuracy: 100.00%\n",
      "Test loss: 1.03349 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 120\n",
      "---------\n",
      "Train loss: 0.00204 | Train accuracy: 100.00%\n",
      "Test loss: 1.03373 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 121\n",
      "---------\n",
      "Train loss: 0.00205 | Train accuracy: 100.00%\n",
      "Test loss: 1.03395 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 122\n",
      "---------\n",
      "Train loss: 0.00204 | Train accuracy: 100.00%\n",
      "Test loss: 1.03420 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 123\n",
      "---------\n",
      "Train loss: 0.00204 | Train accuracy: 100.00%\n",
      "Test loss: 1.03442 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 124\n",
      "---------\n",
      "Train loss: 0.00201 | Train accuracy: 100.00%\n",
      "Test loss: 1.03463 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 125\n",
      "---------\n",
      "Train loss: 0.00204 | Train accuracy: 100.00%\n",
      "Test loss: 1.03488 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 126\n",
      "---------\n",
      "Train loss: 0.00203 | Train accuracy: 100.00%\n",
      "Test loss: 1.03509 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 127\n",
      "---------\n",
      "Train loss: 0.00203 | Train accuracy: 100.00%\n",
      "Test loss: 1.03532 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 128\n",
      "---------\n",
      "Train loss: 0.00203 | Train accuracy: 100.00%\n",
      "Test loss: 1.03557 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 129\n",
      "---------\n",
      "Train loss: 0.00202 | Train accuracy: 100.00%\n",
      "Test loss: 1.03579 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 130\n",
      "---------\n",
      "Train loss: 0.00202 | Train accuracy: 100.00%\n",
      "Test loss: 1.03602 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 131\n",
      "---------\n",
      "Train loss: 0.00200 | Train accuracy: 100.00%\n",
      "Test loss: 1.03620 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 132\n",
      "---------\n",
      "Train loss: 0.00203 | Train accuracy: 100.00%\n",
      "Test loss: 1.03640 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 133\n",
      "---------\n",
      "Train loss: 0.00198 | Train accuracy: 100.00%\n",
      "Test loss: 1.03661 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 134\n",
      "---------\n",
      "Train loss: 0.00198 | Train accuracy: 100.00%\n",
      "Test loss: 1.03686 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 135\n",
      "---------\n",
      "Train loss: 0.00197 | Train accuracy: 100.00%\n",
      "Test loss: 1.03707 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 136\n",
      "---------\n",
      "Train loss: 0.00198 | Train accuracy: 100.00%\n",
      "Test loss: 1.03728 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 137\n",
      "---------\n",
      "Train loss: 0.00196 | Train accuracy: 100.00%\n",
      "Test loss: 1.03750 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 138\n",
      "---------\n",
      "Train loss: 0.00198 | Train accuracy: 100.00%\n",
      "Test loss: 1.03769 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 139\n",
      "---------\n",
      "Train loss: 0.00195 | Train accuracy: 100.00%\n",
      "Test loss: 1.03790 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 140\n",
      "---------\n",
      "Train loss: 0.00196 | Train accuracy: 100.00%\n",
      "Test loss: 1.03812 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 141\n",
      "---------\n",
      "Train loss: 0.00195 | Train accuracy: 100.00%\n",
      "Test loss: 1.03832 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 142\n",
      "---------\n",
      "Train loss: 0.00195 | Train accuracy: 100.00%\n",
      "Test loss: 1.03856 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 143\n",
      "---------\n",
      "Train loss: 0.00193 | Train accuracy: 100.00%\n",
      "Test loss: 1.03877 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 144\n",
      "---------\n",
      "Train loss: 0.00192 | Train accuracy: 100.00%\n",
      "Test loss: 1.03899 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 145\n",
      "---------\n",
      "Train loss: 0.00192 | Train accuracy: 100.00%\n",
      "Test loss: 1.03922 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 146\n",
      "---------\n",
      "Train loss: 0.00193 | Train accuracy: 100.00%\n",
      "Test loss: 1.03943 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 147\n",
      "---------\n",
      "Train loss: 0.00192 | Train accuracy: 100.00%\n",
      "Test loss: 1.03965 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 148\n",
      "---------\n",
      "Train loss: 0.00193 | Train accuracy: 100.00%\n",
      "Test loss: 1.03984 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 149\n",
      "---------\n",
      "Train loss: 0.00192 | Train accuracy: 100.00%\n",
      "Test loss: 1.04006 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 150\n",
      "---------\n",
      "Train loss: 0.00190 | Train accuracy: 100.00%\n",
      "Test loss: 1.04028 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 151\n",
      "---------\n",
      "Train loss: 0.00188 | Train accuracy: 100.00%\n",
      "Test loss: 1.04048 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 152\n",
      "---------\n",
      "Train loss: 0.00189 | Train accuracy: 100.00%\n",
      "Test loss: 1.04070 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 153\n",
      "---------\n",
      "Train loss: 0.00189 | Train accuracy: 100.00%\n",
      "Test loss: 1.04089 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 154\n",
      "---------\n",
      "Train loss: 0.00188 | Train accuracy: 100.00%\n",
      "Test loss: 1.04112 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 155\n",
      "---------\n",
      "Train loss: 0.00188 | Train accuracy: 100.00%\n",
      "Test loss: 1.04129 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 156\n",
      "---------\n",
      "Train loss: 0.00188 | Train accuracy: 100.00%\n",
      "Test loss: 1.04149 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 157\n",
      "---------\n",
      "Train loss: 0.00187 | Train accuracy: 100.00%\n",
      "Test loss: 1.04169 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 158\n",
      "---------\n",
      "Train loss: 0.00187 | Train accuracy: 100.00%\n",
      "Test loss: 1.04193 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 159\n",
      "---------\n",
      "Train loss: 0.00187 | Train accuracy: 100.00%\n",
      "Test loss: 1.04212 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 160\n",
      "---------\n",
      "Train loss: 0.00189 | Train accuracy: 100.00%\n",
      "Test loss: 1.04228 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 161\n",
      "---------\n",
      "Train loss: 0.00186 | Train accuracy: 100.00%\n",
      "Test loss: 1.04250 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 162\n",
      "---------\n",
      "Train loss: 0.00188 | Train accuracy: 100.00%\n",
      "Test loss: 1.04268 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 163\n",
      "---------\n",
      "Train loss: 0.00184 | Train accuracy: 100.00%\n",
      "Test loss: 1.04287 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 164\n",
      "---------\n",
      "Train loss: 0.00185 | Train accuracy: 100.00%\n",
      "Test loss: 1.04304 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 165\n",
      "---------\n",
      "Train loss: 0.00184 | Train accuracy: 100.00%\n",
      "Test loss: 1.04324 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 166\n",
      "---------\n",
      "Train loss: 0.00184 | Train accuracy: 100.00%\n",
      "Test loss: 1.04346 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 167\n",
      "---------\n",
      "Train loss: 0.00183 | Train accuracy: 100.00%\n",
      "Test loss: 1.04366 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 168\n",
      "---------\n",
      "Train loss: 0.00184 | Train accuracy: 100.00%\n",
      "Test loss: 1.04384 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 169\n",
      "---------\n",
      "Train loss: 0.00183 | Train accuracy: 100.00%\n",
      "Test loss: 1.04408 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 170\n",
      "---------\n",
      "Train loss: 0.00181 | Train accuracy: 100.00%\n",
      "Test loss: 1.04429 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 171\n",
      "---------\n",
      "Train loss: 0.00182 | Train accuracy: 100.00%\n",
      "Test loss: 1.04450 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 172\n",
      "---------\n",
      "Train loss: 0.00183 | Train accuracy: 100.00%\n",
      "Test loss: 1.04474 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 173\n",
      "---------\n",
      "Train loss: 0.00181 | Train accuracy: 100.00%\n",
      "Test loss: 1.04495 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 174\n",
      "---------\n",
      "Train loss: 0.00181 | Train accuracy: 100.00%\n",
      "Test loss: 1.04517 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 175\n",
      "---------\n",
      "Train loss: 0.00181 | Train accuracy: 100.00%\n",
      "Test loss: 1.04537 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 176\n",
      "---------\n",
      "Train loss: 0.00179 | Train accuracy: 100.00%\n",
      "Test loss: 1.04557 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 177\n",
      "---------\n",
      "Train loss: 0.00180 | Train accuracy: 100.00%\n",
      "Test loss: 1.04576 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 178\n",
      "---------\n",
      "Train loss: 0.00178 | Train accuracy: 100.00%\n",
      "Test loss: 1.04598 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 179\n",
      "---------\n",
      "Train loss: 0.00179 | Train accuracy: 100.00%\n",
      "Test loss: 1.04617 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 180\n",
      "---------\n",
      "Train loss: 0.00178 | Train accuracy: 100.00%\n",
      "Test loss: 1.04638 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 181\n",
      "---------\n",
      "Train loss: 0.00178 | Train accuracy: 100.00%\n",
      "Test loss: 1.04660 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 182\n",
      "---------\n",
      "Train loss: 0.00183 | Train accuracy: 100.00%\n",
      "Test loss: 1.04682 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 183\n",
      "---------\n",
      "Train loss: 0.00176 | Train accuracy: 100.00%\n",
      "Test loss: 1.04701 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 184\n",
      "---------\n",
      "Train loss: 0.00178 | Train accuracy: 100.00%\n",
      "Test loss: 1.04719 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 185\n",
      "---------\n",
      "Train loss: 0.00181 | Train accuracy: 100.00%\n",
      "Test loss: 1.04741 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 186\n",
      "---------\n",
      "Train loss: 0.00174 | Train accuracy: 100.00%\n",
      "Test loss: 1.04760 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 187\n",
      "---------\n",
      "Train loss: 0.00175 | Train accuracy: 100.00%\n",
      "Test loss: 1.04783 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 188\n",
      "---------\n",
      "Train loss: 0.00175 | Train accuracy: 100.00%\n",
      "Test loss: 1.04801 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 189\n",
      "---------\n",
      "Train loss: 0.00173 | Train accuracy: 100.00%\n",
      "Test loss: 1.04820 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 190\n",
      "---------\n",
      "Train loss: 0.00173 | Train accuracy: 100.00%\n",
      "Test loss: 1.04840 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 191\n",
      "---------\n",
      "Train loss: 0.00173 | Train accuracy: 100.00%\n",
      "Test loss: 1.04857 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 192\n",
      "---------\n",
      "Train loss: 0.00175 | Train accuracy: 100.00%\n",
      "Test loss: 1.04876 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 193\n",
      "---------\n",
      "Train loss: 0.00175 | Train accuracy: 100.00%\n",
      "Test loss: 1.04895 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 194\n",
      "---------\n",
      "Train loss: 0.00173 | Train accuracy: 100.00%\n",
      "Test loss: 1.04914 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 195\n",
      "---------\n",
      "Train loss: 0.00175 | Train accuracy: 100.00%\n",
      "Test loss: 1.04935 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 196\n",
      "---------\n",
      "Train loss: 0.00171 | Train accuracy: 100.00%\n",
      "Test loss: 1.04955 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 197\n",
      "---------\n",
      "Train loss: 0.00173 | Train accuracy: 100.00%\n",
      "Test loss: 1.04976 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 198\n",
      "---------\n",
      "Train loss: 0.00171 | Train accuracy: 100.00%\n",
      "Test loss: 1.04995 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 199\n",
      "---------\n",
      "Train loss: 0.00173 | Train accuracy: 100.00%\n",
      "Test loss: 1.05018 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 200\n",
      "---------\n",
      "Train loss: 0.00171 | Train accuracy: 100.00%\n",
      "Test loss: 1.05039 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 201\n",
      "---------\n",
      "Train loss: 0.00174 | Train accuracy: 100.00%\n",
      "Test loss: 1.05057 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 202\n",
      "---------\n",
      "Train loss: 0.00171 | Train accuracy: 100.00%\n",
      "Test loss: 1.05082 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 203\n",
      "---------\n",
      "Train loss: 0.00168 | Train accuracy: 100.00%\n",
      "Test loss: 1.05099 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 204\n",
      "---------\n",
      "Train loss: 0.00169 | Train accuracy: 100.00%\n",
      "Test loss: 1.05116 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 205\n",
      "---------\n",
      "Train loss: 0.00169 | Train accuracy: 100.00%\n",
      "Test loss: 1.05137 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 206\n",
      "---------\n",
      "Train loss: 0.00168 | Train accuracy: 100.00%\n",
      "Test loss: 1.05159 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 207\n",
      "---------\n",
      "Train loss: 0.00167 | Train accuracy: 100.00%\n",
      "Test loss: 1.05178 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 208\n",
      "---------\n",
      "Train loss: 0.00171 | Train accuracy: 100.00%\n",
      "Test loss: 1.05197 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 209\n",
      "---------\n",
      "Train loss: 0.00169 | Train accuracy: 100.00%\n",
      "Test loss: 1.05215 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 210\n",
      "---------\n",
      "Train loss: 0.00170 | Train accuracy: 100.00%\n",
      "Test loss: 1.05237 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 211\n",
      "---------\n",
      "Train loss: 0.00166 | Train accuracy: 100.00%\n",
      "Test loss: 1.05253 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 212\n",
      "---------\n",
      "Train loss: 0.00167 | Train accuracy: 100.00%\n",
      "Test loss: 1.05273 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 213\n",
      "---------\n",
      "Train loss: 0.00165 | Train accuracy: 100.00%\n",
      "Test loss: 1.05291 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 214\n",
      "---------\n",
      "Train loss: 0.00166 | Train accuracy: 100.00%\n",
      "Test loss: 1.05309 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 215\n",
      "---------\n",
      "Train loss: 0.00166 | Train accuracy: 100.00%\n",
      "Test loss: 1.05328 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 216\n",
      "---------\n",
      "Train loss: 0.00164 | Train accuracy: 100.00%\n",
      "Test loss: 1.05348 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 217\n",
      "---------\n",
      "Train loss: 0.00164 | Train accuracy: 100.00%\n",
      "Test loss: 1.05367 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 218\n",
      "---------\n",
      "Train loss: 0.00164 | Train accuracy: 100.00%\n",
      "Test loss: 1.05387 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 219\n",
      "---------\n",
      "Train loss: 0.00163 | Train accuracy: 100.00%\n",
      "Test loss: 1.05405 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 220\n",
      "---------\n",
      "Train loss: 0.00163 | Train accuracy: 100.00%\n",
      "Test loss: 1.05422 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 221\n",
      "---------\n",
      "Train loss: 0.00163 | Train accuracy: 100.00%\n",
      "Test loss: 1.05442 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 222\n",
      "---------\n",
      "Train loss: 0.00162 | Train accuracy: 100.00%\n",
      "Test loss: 1.05460 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 223\n",
      "---------\n",
      "Train loss: 0.00161 | Train accuracy: 100.00%\n",
      "Test loss: 1.05477 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 224\n",
      "---------\n",
      "Train loss: 0.00162 | Train accuracy: 100.00%\n",
      "Test loss: 1.05496 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 225\n",
      "---------\n",
      "Train loss: 0.00161 | Train accuracy: 100.00%\n",
      "Test loss: 1.05515 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 226\n",
      "---------\n",
      "Train loss: 0.00163 | Train accuracy: 100.00%\n",
      "Test loss: 1.05530 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 227\n",
      "---------\n",
      "Train loss: 0.00161 | Train accuracy: 100.00%\n",
      "Test loss: 1.05549 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 228\n",
      "---------\n",
      "Train loss: 0.00161 | Train accuracy: 100.00%\n",
      "Test loss: 1.05567 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 229\n",
      "---------\n",
      "Train loss: 0.00160 | Train accuracy: 100.00%\n",
      "Test loss: 1.05586 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 230\n",
      "---------\n",
      "Train loss: 0.00160 | Train accuracy: 100.00%\n",
      "Test loss: 1.05604 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 231\n",
      "---------\n",
      "Train loss: 0.00159 | Train accuracy: 100.00%\n",
      "Test loss: 1.05623 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 232\n",
      "---------\n",
      "Train loss: 0.00159 | Train accuracy: 100.00%\n",
      "Test loss: 1.05643 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 233\n",
      "---------\n",
      "Train loss: 0.00158 | Train accuracy: 100.00%\n",
      "Test loss: 1.05662 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 234\n",
      "---------\n",
      "Train loss: 0.00158 | Train accuracy: 100.00%\n",
      "Test loss: 1.05681 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 235\n",
      "---------\n",
      "Train loss: 0.00157 | Train accuracy: 100.00%\n",
      "Test loss: 1.05700 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 236\n",
      "---------\n",
      "Train loss: 0.00157 | Train accuracy: 100.00%\n",
      "Test loss: 1.05718 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 237\n",
      "---------\n",
      "Train loss: 0.00161 | Train accuracy: 100.00%\n",
      "Test loss: 1.05731 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 238\n",
      "---------\n",
      "Train loss: 0.00157 | Train accuracy: 100.00%\n",
      "Test loss: 1.05751 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 239\n",
      "---------\n",
      "Train loss: 0.00156 | Train accuracy: 100.00%\n",
      "Test loss: 1.05767 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 240\n",
      "---------\n",
      "Train loss: 0.00155 | Train accuracy: 100.00%\n",
      "Test loss: 1.05785 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 241\n",
      "---------\n",
      "Train loss: 0.00156 | Train accuracy: 100.00%\n",
      "Test loss: 1.05802 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 242\n",
      "---------\n",
      "Train loss: 0.00154 | Train accuracy: 100.00%\n",
      "Test loss: 1.05820 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 243\n",
      "---------\n",
      "Train loss: 0.00156 | Train accuracy: 100.00%\n",
      "Test loss: 1.05838 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 244\n",
      "---------\n",
      "Train loss: 0.00157 | Train accuracy: 100.00%\n",
      "Test loss: 1.05860 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 245\n",
      "---------\n",
      "Train loss: 0.00153 | Train accuracy: 100.00%\n",
      "Test loss: 1.05877 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 246\n",
      "---------\n",
      "Train loss: 0.00156 | Train accuracy: 100.00%\n",
      "Test loss: 1.05894 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 247\n",
      "---------\n",
      "Train loss: 0.00156 | Train accuracy: 100.00%\n",
      "Test loss: 1.05914 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 248\n",
      "---------\n",
      "Train loss: 0.00153 | Train accuracy: 100.00%\n",
      "Test loss: 1.05932 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 249\n",
      "---------\n",
      "Train loss: 0.00157 | Train accuracy: 100.00%\n",
      "Test loss: 1.05945 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 250\n",
      "---------\n",
      "Train loss: 0.00155 | Train accuracy: 100.00%\n",
      "Test loss: 1.05961 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 251\n",
      "---------\n",
      "Train loss: 0.00154 | Train accuracy: 100.00%\n",
      "Test loss: 1.05978 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 252\n",
      "---------\n",
      "Train loss: 0.00152 | Train accuracy: 100.00%\n",
      "Test loss: 1.05993 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 253\n",
      "---------\n",
      "Train loss: 0.00151 | Train accuracy: 100.00%\n",
      "Test loss: 1.06011 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 254\n",
      "---------\n",
      "Train loss: 0.00152 | Train accuracy: 100.00%\n",
      "Test loss: 1.06029 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 255\n",
      "---------\n",
      "Train loss: 0.00151 | Train accuracy: 100.00%\n",
      "Test loss: 1.06046 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 256\n",
      "---------\n",
      "Train loss: 0.00151 | Train accuracy: 100.00%\n",
      "Test loss: 1.06063 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 257\n",
      "---------\n",
      "Train loss: 0.00154 | Train accuracy: 100.00%\n",
      "Test loss: 1.06080 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 258\n",
      "---------\n",
      "Train loss: 0.00153 | Train accuracy: 100.00%\n",
      "Test loss: 1.06101 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 259\n",
      "---------\n",
      "Train loss: 0.00153 | Train accuracy: 100.00%\n",
      "Test loss: 1.06120 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 260\n",
      "---------\n",
      "Train loss: 0.00153 | Train accuracy: 100.00%\n",
      "Test loss: 1.06138 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 261\n",
      "---------\n",
      "Train loss: 0.00150 | Train accuracy: 100.00%\n",
      "Test loss: 1.06157 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 262\n",
      "---------\n",
      "Train loss: 0.00151 | Train accuracy: 100.00%\n",
      "Test loss: 1.06172 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 263\n",
      "---------\n",
      "Train loss: 0.00151 | Train accuracy: 100.00%\n",
      "Test loss: 1.06189 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 264\n",
      "---------\n",
      "Train loss: 0.00150 | Train accuracy: 100.00%\n",
      "Test loss: 1.06207 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 265\n",
      "---------\n",
      "Train loss: 0.00151 | Train accuracy: 100.00%\n",
      "Test loss: 1.06226 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 266\n",
      "---------\n",
      "Train loss: 0.00147 | Train accuracy: 100.00%\n",
      "Test loss: 1.06243 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 267\n",
      "---------\n",
      "Train loss: 0.00147 | Train accuracy: 100.00%\n",
      "Test loss: 1.06260 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 268\n",
      "---------\n",
      "Train loss: 0.00148 | Train accuracy: 100.00%\n",
      "Test loss: 1.06278 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 269\n",
      "---------\n",
      "Train loss: 0.00148 | Train accuracy: 100.00%\n",
      "Test loss: 1.06295 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 270\n",
      "---------\n",
      "Train loss: 0.00146 | Train accuracy: 100.00%\n",
      "Test loss: 1.06312 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 271\n",
      "---------\n",
      "Train loss: 0.00147 | Train accuracy: 100.00%\n",
      "Test loss: 1.06328 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 272\n",
      "---------\n",
      "Train loss: 0.00147 | Train accuracy: 100.00%\n",
      "Test loss: 1.06344 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 273\n",
      "---------\n",
      "Train loss: 0.00146 | Train accuracy: 100.00%\n",
      "Test loss: 1.06360 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 274\n",
      "---------\n",
      "Train loss: 0.00148 | Train accuracy: 100.00%\n",
      "Test loss: 1.06378 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 275\n",
      "---------\n",
      "Train loss: 0.00145 | Train accuracy: 100.00%\n",
      "Test loss: 1.06395 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 276\n",
      "---------\n",
      "Train loss: 0.00149 | Train accuracy: 100.00%\n",
      "Test loss: 1.06413 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 277\n",
      "---------\n",
      "Train loss: 0.00145 | Train accuracy: 100.00%\n",
      "Test loss: 1.06432 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 278\n",
      "---------\n",
      "Train loss: 0.00144 | Train accuracy: 100.00%\n",
      "Test loss: 1.06447 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 279\n",
      "---------\n",
      "Train loss: 0.00151 | Train accuracy: 100.00%\n",
      "Test loss: 1.06461 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 280\n",
      "---------\n",
      "Train loss: 0.00145 | Train accuracy: 100.00%\n",
      "Test loss: 1.06479 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 281\n",
      "---------\n",
      "Train loss: 0.00145 | Train accuracy: 100.00%\n",
      "Test loss: 1.06498 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 282\n",
      "---------\n",
      "Train loss: 0.00145 | Train accuracy: 100.00%\n",
      "Test loss: 1.06518 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 283\n",
      "---------\n",
      "Train loss: 0.00143 | Train accuracy: 100.00%\n",
      "Test loss: 1.06534 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 284\n",
      "---------\n",
      "Train loss: 0.00144 | Train accuracy: 100.00%\n",
      "Test loss: 1.06554 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 285\n",
      "---------\n",
      "Train loss: 0.00144 | Train accuracy: 100.00%\n",
      "Test loss: 1.06570 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 286\n",
      "---------\n",
      "Train loss: 0.00142 | Train accuracy: 100.00%\n",
      "Test loss: 1.06587 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 287\n",
      "---------\n",
      "Train loss: 0.00142 | Train accuracy: 100.00%\n",
      "Test loss: 1.06604 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 288\n",
      "---------\n",
      "Train loss: 0.00142 | Train accuracy: 100.00%\n",
      "Test loss: 1.06622 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 289\n",
      "---------\n",
      "Train loss: 0.00141 | Train accuracy: 100.00%\n",
      "Test loss: 1.06639 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 290\n",
      "---------\n",
      "Train loss: 0.00141 | Train accuracy: 100.00%\n",
      "Test loss: 1.06656 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 291\n",
      "---------\n",
      "Train loss: 0.00142 | Train accuracy: 100.00%\n",
      "Test loss: 1.06675 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 292\n",
      "---------\n",
      "Train loss: 0.00141 | Train accuracy: 100.00%\n",
      "Test loss: 1.06694 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 293\n",
      "---------\n",
      "Train loss: 0.00144 | Train accuracy: 100.00%\n",
      "Test loss: 1.06712 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 294\n",
      "---------\n",
      "Train loss: 0.00141 | Train accuracy: 100.00%\n",
      "Test loss: 1.06731 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 295\n",
      "---------\n",
      "Train loss: 0.00145 | Train accuracy: 100.00%\n",
      "Test loss: 1.06749 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 296\n",
      "---------\n",
      "Train loss: 0.00143 | Train accuracy: 100.00%\n",
      "Test loss: 1.06768 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 297\n",
      "---------\n",
      "Train loss: 0.00138 | Train accuracy: 100.00%\n",
      "Test loss: 1.06784 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 298\n",
      "---------\n",
      "Train loss: 0.00139 | Train accuracy: 100.00%\n",
      "Test loss: 1.06801 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 299\n",
      "---------\n",
      "Train loss: 0.00142 | Train accuracy: 100.00%\n",
      "Test loss: 1.06817 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 300\n",
      "---------\n",
      "Train loss: 0.00138 | Train accuracy: 100.00%\n",
      "Test loss: 1.06833 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 301\n",
      "---------\n",
      "Train loss: 0.00140 | Train accuracy: 100.00%\n",
      "Test loss: 1.06851 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 302\n",
      "---------\n",
      "Train loss: 0.00140 | Train accuracy: 100.00%\n",
      "Test loss: 1.06869 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 303\n",
      "---------\n",
      "Train loss: 0.00138 | Train accuracy: 100.00%\n",
      "Test loss: 1.06887 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 304\n",
      "---------\n",
      "Train loss: 0.00137 | Train accuracy: 100.00%\n",
      "Test loss: 1.06903 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 305\n",
      "---------\n",
      "Train loss: 0.00137 | Train accuracy: 100.00%\n",
      "Test loss: 1.06920 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 306\n",
      "---------\n",
      "Train loss: 0.00138 | Train accuracy: 100.00%\n",
      "Test loss: 1.06935 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 307\n",
      "---------\n",
      "Train loss: 0.00138 | Train accuracy: 100.00%\n",
      "Test loss: 1.06949 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 308\n",
      "---------\n",
      "Train loss: 0.00140 | Train accuracy: 100.00%\n",
      "Test loss: 1.06962 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 309\n",
      "---------\n",
      "Train loss: 0.00138 | Train accuracy: 100.00%\n",
      "Test loss: 1.06981 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 310\n",
      "---------\n",
      "Train loss: 0.00135 | Train accuracy: 100.00%\n",
      "Test loss: 1.06996 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 311\n",
      "---------\n",
      "Train loss: 0.00139 | Train accuracy: 100.00%\n",
      "Test loss: 1.07016 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 312\n",
      "---------\n",
      "Train loss: 0.00136 | Train accuracy: 100.00%\n",
      "Test loss: 1.07033 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 313\n",
      "---------\n",
      "Train loss: 0.00136 | Train accuracy: 100.00%\n",
      "Test loss: 1.07048 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 314\n",
      "---------\n",
      "Train loss: 0.00138 | Train accuracy: 100.00%\n",
      "Test loss: 1.07062 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 315\n",
      "---------\n",
      "Train loss: 0.00135 | Train accuracy: 100.00%\n",
      "Test loss: 1.07080 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 316\n",
      "---------\n",
      "Train loss: 0.00135 | Train accuracy: 100.00%\n",
      "Test loss: 1.07095 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 317\n",
      "---------\n",
      "Train loss: 0.00136 | Train accuracy: 100.00%\n",
      "Test loss: 1.07114 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 318\n",
      "---------\n",
      "Train loss: 0.00137 | Train accuracy: 100.00%\n",
      "Test loss: 1.07120 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 319\n",
      "---------\n",
      "Train loss: 0.00134 | Train accuracy: 100.00%\n",
      "Test loss: 1.07137 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 320\n",
      "---------\n",
      "Train loss: 0.00133 | Train accuracy: 100.00%\n",
      "Test loss: 1.07154 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 321\n",
      "---------\n",
      "Train loss: 0.00133 | Train accuracy: 100.00%\n",
      "Test loss: 1.07171 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 322\n",
      "---------\n",
      "Train loss: 0.00133 | Train accuracy: 100.00%\n",
      "Test loss: 1.07188 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 323\n",
      "---------\n",
      "Train loss: 0.00135 | Train accuracy: 100.00%\n",
      "Test loss: 1.07205 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 324\n",
      "---------\n",
      "Train loss: 0.00133 | Train accuracy: 100.00%\n",
      "Test loss: 1.07221 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 325\n",
      "---------\n",
      "Train loss: 0.00133 | Train accuracy: 100.00%\n",
      "Test loss: 1.07238 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 326\n",
      "---------\n",
      "Train loss: 0.00136 | Train accuracy: 100.00%\n",
      "Test loss: 1.07255 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 327\n",
      "---------\n",
      "Train loss: 0.00132 | Train accuracy: 100.00%\n",
      "Test loss: 1.07272 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 328\n",
      "---------\n",
      "Train loss: 0.00133 | Train accuracy: 100.00%\n",
      "Test loss: 1.07286 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 329\n",
      "---------\n",
      "Train loss: 0.00132 | Train accuracy: 100.00%\n",
      "Test loss: 1.07301 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 330\n",
      "---------\n",
      "Train loss: 0.00138 | Train accuracy: 100.00%\n",
      "Test loss: 1.07318 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 331\n",
      "---------\n",
      "Train loss: 0.00132 | Train accuracy: 100.00%\n",
      "Test loss: 1.07333 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 332\n",
      "---------\n",
      "Train loss: 0.00132 | Train accuracy: 100.00%\n",
      "Test loss: 1.07349 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 333\n",
      "---------\n",
      "Train loss: 0.00134 | Train accuracy: 100.00%\n",
      "Test loss: 1.07364 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 334\n",
      "---------\n",
      "Train loss: 0.00130 | Train accuracy: 100.00%\n",
      "Test loss: 1.07381 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 335\n",
      "---------\n",
      "Train loss: 0.00131 | Train accuracy: 100.00%\n",
      "Test loss: 1.07398 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 336\n",
      "---------\n",
      "Train loss: 0.00130 | Train accuracy: 100.00%\n",
      "Test loss: 1.07414 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 337\n",
      "---------\n",
      "Train loss: 0.00130 | Train accuracy: 100.00%\n",
      "Test loss: 1.07430 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 338\n",
      "---------\n",
      "Train loss: 0.00130 | Train accuracy: 100.00%\n",
      "Test loss: 1.07451 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 339\n",
      "---------\n",
      "Train loss: 0.00129 | Train accuracy: 100.00%\n",
      "Test loss: 1.07466 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 340\n",
      "---------\n",
      "Train loss: 0.00129 | Train accuracy: 100.00%\n",
      "Test loss: 1.07481 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 341\n",
      "---------\n",
      "Train loss: 0.00130 | Train accuracy: 100.00%\n",
      "Test loss: 1.07496 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 342\n",
      "---------\n",
      "Train loss: 0.00129 | Train accuracy: 100.00%\n",
      "Test loss: 1.07510 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 343\n",
      "---------\n",
      "Train loss: 0.00127 | Train accuracy: 100.00%\n",
      "Test loss: 1.07526 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 344\n",
      "---------\n",
      "Train loss: 0.00128 | Train accuracy: 100.00%\n",
      "Test loss: 1.07542 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 345\n",
      "---------\n",
      "Train loss: 0.00129 | Train accuracy: 100.00%\n",
      "Test loss: 1.07559 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 346\n",
      "---------\n",
      "Train loss: 0.00128 | Train accuracy: 100.00%\n",
      "Test loss: 1.07575 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 347\n",
      "---------\n",
      "Train loss: 0.00128 | Train accuracy: 100.00%\n",
      "Test loss: 1.07588 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 348\n",
      "---------\n",
      "Train loss: 0.00127 | Train accuracy: 100.00%\n",
      "Test loss: 1.07604 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 349\n",
      "---------\n",
      "Train loss: 0.00130 | Train accuracy: 100.00%\n",
      "Test loss: 1.07618 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 350\n",
      "---------\n",
      "Train loss: 0.00130 | Train accuracy: 100.00%\n",
      "Test loss: 1.07631 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 351\n",
      "---------\n",
      "Train loss: 0.00130 | Train accuracy: 100.00%\n",
      "Test loss: 1.07647 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 352\n",
      "---------\n",
      "Train loss: 0.00126 | Train accuracy: 100.00%\n",
      "Test loss: 1.07662 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 353\n",
      "---------\n",
      "Train loss: 0.00127 | Train accuracy: 100.00%\n",
      "Test loss: 1.07677 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 354\n",
      "---------\n",
      "Train loss: 0.00126 | Train accuracy: 100.00%\n",
      "Test loss: 1.07691 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 355\n",
      "---------\n",
      "Train loss: 0.00126 | Train accuracy: 100.00%\n",
      "Test loss: 1.07707 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 356\n",
      "---------\n",
      "Train loss: 0.00126 | Train accuracy: 100.00%\n",
      "Test loss: 1.07720 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 357\n",
      "---------\n",
      "Train loss: 0.00126 | Train accuracy: 100.00%\n",
      "Test loss: 1.07737 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 358\n",
      "---------\n",
      "Train loss: 0.00125 | Train accuracy: 100.00%\n",
      "Test loss: 1.07752 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 359\n",
      "---------\n",
      "Train loss: 0.00124 | Train accuracy: 100.00%\n",
      "Test loss: 1.07768 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 360\n",
      "---------\n",
      "Train loss: 0.00125 | Train accuracy: 100.00%\n",
      "Test loss: 1.07783 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 361\n",
      "---------\n",
      "Train loss: 0.00124 | Train accuracy: 100.00%\n",
      "Test loss: 1.07798 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 362\n",
      "---------\n",
      "Train loss: 0.00123 | Train accuracy: 100.00%\n",
      "Test loss: 1.07813 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 363\n",
      "---------\n",
      "Train loss: 0.00124 | Train accuracy: 100.00%\n",
      "Test loss: 1.07829 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 364\n",
      "---------\n",
      "Train loss: 0.00126 | Train accuracy: 100.00%\n",
      "Test loss: 1.07846 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 365\n",
      "---------\n",
      "Train loss: 0.00124 | Train accuracy: 100.00%\n",
      "Test loss: 1.07862 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 366\n",
      "---------\n",
      "Train loss: 0.00123 | Train accuracy: 100.00%\n",
      "Test loss: 1.07877 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 367\n",
      "---------\n",
      "Train loss: 0.00124 | Train accuracy: 100.00%\n",
      "Test loss: 1.07892 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 368\n",
      "---------\n",
      "Train loss: 0.00122 | Train accuracy: 100.00%\n",
      "Test loss: 1.07907 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 369\n",
      "---------\n",
      "Train loss: 0.00123 | Train accuracy: 100.00%\n",
      "Test loss: 1.07923 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 370\n",
      "---------\n",
      "Train loss: 0.00122 | Train accuracy: 100.00%\n",
      "Test loss: 1.07937 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 371\n",
      "---------\n",
      "Train loss: 0.00123 | Train accuracy: 100.00%\n",
      "Test loss: 1.07951 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 372\n",
      "---------\n",
      "Train loss: 0.00122 | Train accuracy: 100.00%\n",
      "Test loss: 1.07966 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 373\n",
      "---------\n",
      "Train loss: 0.00124 | Train accuracy: 100.00%\n",
      "Test loss: 1.07981 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 374\n",
      "---------\n",
      "Train loss: 0.00122 | Train accuracy: 100.00%\n",
      "Test loss: 1.07997 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 375\n",
      "---------\n",
      "Train loss: 0.00122 | Train accuracy: 100.00%\n",
      "Test loss: 1.08015 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 376\n",
      "---------\n",
      "Train loss: 0.00121 | Train accuracy: 100.00%\n",
      "Test loss: 1.08030 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 377\n",
      "---------\n",
      "Train loss: 0.00122 | Train accuracy: 100.00%\n",
      "Test loss: 1.08043 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 378\n",
      "---------\n",
      "Train loss: 0.00125 | Train accuracy: 100.00%\n",
      "Test loss: 1.08056 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 379\n",
      "---------\n",
      "Train loss: 0.00120 | Train accuracy: 100.00%\n",
      "Test loss: 1.08070 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 380\n",
      "---------\n",
      "Train loss: 0.00120 | Train accuracy: 100.00%\n",
      "Test loss: 1.08085 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 381\n",
      "---------\n",
      "Train loss: 0.00121 | Train accuracy: 100.00%\n",
      "Test loss: 1.08100 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 382\n",
      "---------\n",
      "Train loss: 0.00119 | Train accuracy: 100.00%\n",
      "Test loss: 1.08114 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 383\n",
      "---------\n",
      "Train loss: 0.00122 | Train accuracy: 100.00%\n",
      "Test loss: 1.08129 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 384\n",
      "---------\n",
      "Train loss: 0.00121 | Train accuracy: 100.00%\n",
      "Test loss: 1.08147 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 385\n",
      "---------\n",
      "Train loss: 0.00120 | Train accuracy: 100.00%\n",
      "Test loss: 1.08162 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 386\n",
      "---------\n",
      "Train loss: 0.00119 | Train accuracy: 100.00%\n",
      "Test loss: 1.08176 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 387\n",
      "---------\n",
      "Train loss: 0.00120 | Train accuracy: 100.00%\n",
      "Test loss: 1.08191 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 388\n",
      "---------\n",
      "Train loss: 0.00119 | Train accuracy: 100.00%\n",
      "Test loss: 1.08205 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 389\n",
      "---------\n",
      "Train loss: 0.00119 | Train accuracy: 100.00%\n",
      "Test loss: 1.08219 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 390\n",
      "---------\n",
      "Train loss: 0.00118 | Train accuracy: 100.00%\n",
      "Test loss: 1.08233 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 391\n",
      "---------\n",
      "Train loss: 0.00118 | Train accuracy: 100.00%\n",
      "Test loss: 1.08247 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 392\n",
      "---------\n",
      "Train loss: 0.00118 | Train accuracy: 100.00%\n",
      "Test loss: 1.08261 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 393\n",
      "---------\n",
      "Train loss: 0.00120 | Train accuracy: 100.00%\n",
      "Test loss: 1.08274 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 394\n",
      "---------\n",
      "Train loss: 0.00119 | Train accuracy: 100.00%\n",
      "Test loss: 1.08291 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 395\n",
      "---------\n",
      "Train loss: 0.00117 | Train accuracy: 100.00%\n",
      "Test loss: 1.08304 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 396\n",
      "---------\n",
      "Train loss: 0.00118 | Train accuracy: 100.00%\n",
      "Test loss: 1.08320 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 397\n",
      "---------\n",
      "Train loss: 0.00119 | Train accuracy: 100.00%\n",
      "Test loss: 1.08333 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 398\n",
      "---------\n",
      "Train loss: 0.00117 | Train accuracy: 100.00%\n",
      "Test loss: 1.08347 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 399\n",
      "---------\n",
      "Train loss: 0.00117 | Train accuracy: 100.00%\n",
      "Test loss: 1.08361 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 400\n",
      "---------\n",
      "Train loss: 0.00118 | Train accuracy: 100.00%\n",
      "Test loss: 1.08379 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 401\n",
      "---------\n",
      "Train loss: 0.00116 | Train accuracy: 100.00%\n",
      "Test loss: 1.08394 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 402\n",
      "---------\n",
      "Train loss: 0.00116 | Train accuracy: 100.00%\n",
      "Test loss: 1.08407 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 403\n",
      "---------\n",
      "Train loss: 0.00115 | Train accuracy: 100.00%\n",
      "Test loss: 1.08421 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 404\n",
      "---------\n",
      "Train loss: 0.00116 | Train accuracy: 100.00%\n",
      "Test loss: 1.08434 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 405\n",
      "---------\n",
      "Train loss: 0.00116 | Train accuracy: 100.00%\n",
      "Test loss: 1.08449 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 406\n",
      "---------\n",
      "Train loss: 0.00118 | Train accuracy: 100.00%\n",
      "Test loss: 1.08463 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 407\n",
      "---------\n",
      "Train loss: 0.00116 | Train accuracy: 100.00%\n",
      "Test loss: 1.08477 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 408\n",
      "---------\n",
      "Train loss: 0.00115 | Train accuracy: 100.00%\n",
      "Test loss: 1.08492 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 409\n",
      "---------\n",
      "Train loss: 0.00115 | Train accuracy: 100.00%\n",
      "Test loss: 1.08506 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 410\n",
      "---------\n",
      "Train loss: 0.00118 | Train accuracy: 100.00%\n",
      "Test loss: 1.08523 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 411\n",
      "---------\n",
      "Train loss: 0.00115 | Train accuracy: 100.00%\n",
      "Test loss: 1.08537 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 412\n",
      "---------\n",
      "Train loss: 0.00115 | Train accuracy: 100.00%\n",
      "Test loss: 1.08551 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 413\n",
      "---------\n",
      "Train loss: 0.00114 | Train accuracy: 100.00%\n",
      "Test loss: 1.08564 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 414\n",
      "---------\n",
      "Train loss: 0.00114 | Train accuracy: 100.00%\n",
      "Test loss: 1.08579 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 415\n",
      "---------\n",
      "Train loss: 0.00114 | Train accuracy: 100.00%\n",
      "Test loss: 1.08594 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 416\n",
      "---------\n",
      "Train loss: 0.00114 | Train accuracy: 100.00%\n",
      "Test loss: 1.08607 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 417\n",
      "---------\n",
      "Train loss: 0.00116 | Train accuracy: 100.00%\n",
      "Test loss: 1.08623 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 418\n",
      "---------\n",
      "Train loss: 0.00113 | Train accuracy: 100.00%\n",
      "Test loss: 1.08636 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 419\n",
      "---------\n",
      "Train loss: 0.00113 | Train accuracy: 100.00%\n",
      "Test loss: 1.08650 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 420\n",
      "---------\n",
      "Train loss: 0.00113 | Train accuracy: 100.00%\n",
      "Test loss: 1.08663 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 421\n",
      "---------\n",
      "Train loss: 0.00113 | Train accuracy: 100.00%\n",
      "Test loss: 1.08677 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 422\n",
      "---------\n",
      "Train loss: 0.00112 | Train accuracy: 100.00%\n",
      "Test loss: 1.08692 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 423\n",
      "---------\n",
      "Train loss: 0.00113 | Train accuracy: 100.00%\n",
      "Test loss: 1.08706 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 424\n",
      "---------\n",
      "Train loss: 0.00115 | Train accuracy: 100.00%\n",
      "Test loss: 1.08722 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 425\n",
      "---------\n",
      "Train loss: 0.00111 | Train accuracy: 100.00%\n",
      "Test loss: 1.08736 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 426\n",
      "---------\n",
      "Train loss: 0.00114 | Train accuracy: 100.00%\n",
      "Test loss: 1.08749 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 427\n",
      "---------\n",
      "Train loss: 0.00113 | Train accuracy: 100.00%\n",
      "Test loss: 1.08764 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 428\n",
      "---------\n",
      "Train loss: 0.00111 | Train accuracy: 100.00%\n",
      "Test loss: 1.08777 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 429\n",
      "---------\n",
      "Train loss: 0.00112 | Train accuracy: 100.00%\n",
      "Test loss: 1.08791 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 430\n",
      "---------\n",
      "Train loss: 0.00111 | Train accuracy: 100.00%\n",
      "Test loss: 1.08804 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 431\n",
      "---------\n",
      "Train loss: 0.00111 | Train accuracy: 100.00%\n",
      "Test loss: 1.08818 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 432\n",
      "---------\n",
      "Train loss: 0.00111 | Train accuracy: 100.00%\n",
      "Test loss: 1.08832 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 433\n",
      "---------\n",
      "Train loss: 0.00111 | Train accuracy: 100.00%\n",
      "Test loss: 1.08848 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 434\n",
      "---------\n",
      "Train loss: 0.00110 | Train accuracy: 100.00%\n",
      "Test loss: 1.08863 | Test accuracy: 81.10%\n",
      "\n",
      "Epoch: 435\n",
      "---------\n",
      "Train loss: 0.00110 | Train accuracy: 100.00%\n",
      "Test loss: 1.08877 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 436\n",
      "---------\n",
      "Train loss: 0.00110 | Train accuracy: 100.00%\n",
      "Test loss: 1.08890 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 437\n",
      "---------\n",
      "Train loss: 0.00113 | Train accuracy: 100.00%\n",
      "Test loss: 1.08901 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 438\n",
      "---------\n",
      "Train loss: 0.00111 | Train accuracy: 100.00%\n",
      "Test loss: 1.08913 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 439\n",
      "---------\n",
      "Train loss: 0.00110 | Train accuracy: 100.00%\n",
      "Test loss: 1.08931 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 440\n",
      "---------\n",
      "Train loss: 0.00109 | Train accuracy: 100.00%\n",
      "Test loss: 1.08944 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 441\n",
      "---------\n",
      "Train loss: 0.00109 | Train accuracy: 100.00%\n",
      "Test loss: 1.08958 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 442\n",
      "---------\n",
      "Train loss: 0.00111 | Train accuracy: 100.00%\n",
      "Test loss: 1.08973 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 443\n",
      "---------\n",
      "Train loss: 0.00110 | Train accuracy: 100.00%\n",
      "Test loss: 1.08989 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 444\n",
      "---------\n",
      "Train loss: 0.00109 | Train accuracy: 100.00%\n",
      "Test loss: 1.09003 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 445\n",
      "---------\n",
      "Train loss: 0.00108 | Train accuracy: 100.00%\n",
      "Test loss: 1.09015 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 446\n",
      "---------\n",
      "Train loss: 0.00108 | Train accuracy: 100.00%\n",
      "Test loss: 1.09029 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 447\n",
      "---------\n",
      "Train loss: 0.00109 | Train accuracy: 100.00%\n",
      "Test loss: 1.09042 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 448\n",
      "---------\n",
      "Train loss: 0.00108 | Train accuracy: 100.00%\n",
      "Test loss: 1.09055 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 449\n",
      "---------\n",
      "Train loss: 0.00111 | Train accuracy: 100.00%\n",
      "Test loss: 1.09070 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 450\n",
      "---------\n",
      "Train loss: 0.00108 | Train accuracy: 100.00%\n",
      "Test loss: 1.09082 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 451\n",
      "---------\n",
      "Train loss: 0.00107 | Train accuracy: 100.00%\n",
      "Test loss: 1.09095 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 452\n",
      "---------\n",
      "Train loss: 0.00107 | Train accuracy: 100.00%\n",
      "Test loss: 1.09108 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 453\n",
      "---------\n",
      "Train loss: 0.00107 | Train accuracy: 100.00%\n",
      "Test loss: 1.09121 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 454\n",
      "---------\n",
      "Train loss: 0.00109 | Train accuracy: 100.00%\n",
      "Test loss: 1.09133 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 455\n",
      "---------\n",
      "Train loss: 0.00107 | Train accuracy: 100.00%\n",
      "Test loss: 1.09147 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 456\n",
      "---------\n",
      "Train loss: 0.00107 | Train accuracy: 100.00%\n",
      "Test loss: 1.09162 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 457\n",
      "---------\n",
      "Train loss: 0.00106 | Train accuracy: 100.00%\n",
      "Test loss: 1.09176 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 458\n",
      "---------\n",
      "Train loss: 0.00107 | Train accuracy: 100.00%\n",
      "Test loss: 1.09187 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 459\n",
      "---------\n",
      "Train loss: 0.00107 | Train accuracy: 100.00%\n",
      "Test loss: 1.09200 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 460\n",
      "---------\n",
      "Train loss: 0.00107 | Train accuracy: 100.00%\n",
      "Test loss: 1.09214 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 461\n",
      "---------\n",
      "Train loss: 0.00106 | Train accuracy: 100.00%\n",
      "Test loss: 1.09227 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 462\n",
      "---------\n",
      "Train loss: 0.00107 | Train accuracy: 100.00%\n",
      "Test loss: 1.09241 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 463\n",
      "---------\n",
      "Train loss: 0.00105 | Train accuracy: 100.00%\n",
      "Test loss: 1.09254 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 464\n",
      "---------\n",
      "Train loss: 0.00106 | Train accuracy: 100.00%\n",
      "Test loss: 1.09267 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 465\n",
      "---------\n",
      "Train loss: 0.00107 | Train accuracy: 100.00%\n",
      "Test loss: 1.09280 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 466\n",
      "---------\n",
      "Train loss: 0.00106 | Train accuracy: 100.00%\n",
      "Test loss: 1.09292 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 467\n",
      "---------\n",
      "Train loss: 0.00106 | Train accuracy: 100.00%\n",
      "Test loss: 1.09306 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 468\n",
      "---------\n",
      "Train loss: 0.00107 | Train accuracy: 100.00%\n",
      "Test loss: 1.09317 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 469\n",
      "---------\n",
      "Train loss: 0.00104 | Train accuracy: 100.00%\n",
      "Test loss: 1.09331 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 470\n",
      "---------\n",
      "Train loss: 0.00105 | Train accuracy: 100.00%\n",
      "Test loss: 1.09343 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 471\n",
      "---------\n",
      "Train loss: 0.00105 | Train accuracy: 100.00%\n",
      "Test loss: 1.09357 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 472\n",
      "---------\n",
      "Train loss: 0.00104 | Train accuracy: 100.00%\n",
      "Test loss: 1.09369 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 473\n",
      "---------\n",
      "Train loss: 0.00104 | Train accuracy: 100.00%\n",
      "Test loss: 1.09382 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 474\n",
      "---------\n",
      "Train loss: 0.00104 | Train accuracy: 100.00%\n",
      "Test loss: 1.09395 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 475\n",
      "---------\n",
      "Train loss: 0.00103 | Train accuracy: 100.00%\n",
      "Test loss: 1.09409 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 476\n",
      "---------\n",
      "Train loss: 0.00103 | Train accuracy: 100.00%\n",
      "Test loss: 1.09422 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 477\n",
      "---------\n",
      "Train loss: 0.00103 | Train accuracy: 100.00%\n",
      "Test loss: 1.09435 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 478\n",
      "---------\n",
      "Train loss: 0.00103 | Train accuracy: 100.00%\n",
      "Test loss: 1.09450 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 479\n",
      "---------\n",
      "Train loss: 0.00103 | Train accuracy: 100.00%\n",
      "Test loss: 1.09464 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 480\n",
      "---------\n",
      "Train loss: 0.00103 | Train accuracy: 100.00%\n",
      "Test loss: 1.09476 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 481\n",
      "---------\n",
      "Train loss: 0.00104 | Train accuracy: 100.00%\n",
      "Test loss: 1.09487 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 482\n",
      "---------\n",
      "Train loss: 0.00104 | Train accuracy: 100.00%\n",
      "Test loss: 1.09500 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 483\n",
      "---------\n",
      "Train loss: 0.00102 | Train accuracy: 100.00%\n",
      "Test loss: 1.09515 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 484\n",
      "---------\n",
      "Train loss: 0.00102 | Train accuracy: 100.00%\n",
      "Test loss: 1.09528 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 485\n",
      "---------\n",
      "Train loss: 0.00103 | Train accuracy: 100.00%\n",
      "Test loss: 1.09543 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 486\n",
      "---------\n",
      "Train loss: 0.00106 | Train accuracy: 100.00%\n",
      "Test loss: 1.09558 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 487\n",
      "---------\n",
      "Train loss: 0.00101 | Train accuracy: 100.00%\n",
      "Test loss: 1.09571 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 488\n",
      "---------\n",
      "Train loss: 0.00102 | Train accuracy: 100.00%\n",
      "Test loss: 1.09583 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 489\n",
      "---------\n",
      "Train loss: 0.00101 | Train accuracy: 100.00%\n",
      "Test loss: 1.09596 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 490\n",
      "---------\n",
      "Train loss: 0.00103 | Train accuracy: 100.00%\n",
      "Test loss: 1.09608 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 491\n",
      "---------\n",
      "Train loss: 0.00102 | Train accuracy: 100.00%\n",
      "Test loss: 1.09622 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 492\n",
      "---------\n",
      "Train loss: 0.00102 | Train accuracy: 100.00%\n",
      "Test loss: 1.09635 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 493\n",
      "---------\n",
      "Train loss: 0.00102 | Train accuracy: 100.00%\n",
      "Test loss: 1.09651 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 494\n",
      "---------\n",
      "Train loss: 0.00102 | Train accuracy: 100.00%\n",
      "Test loss: 1.09665 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 495\n",
      "---------\n",
      "Train loss: 0.00100 | Train accuracy: 100.00%\n",
      "Test loss: 1.09678 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 496\n",
      "---------\n",
      "Train loss: 0.00101 | Train accuracy: 100.00%\n",
      "Test loss: 1.09693 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 497\n",
      "---------\n",
      "Train loss: 0.00102 | Train accuracy: 100.00%\n",
      "Test loss: 1.09707 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 498\n",
      "---------\n",
      "Train loss: 0.00100 | Train accuracy: 100.00%\n",
      "Test loss: 1.09720 | Test accuracy: 81.21%\n",
      "\n",
      "Epoch: 499\n",
      "---------\n",
      "Train loss: 0.00100 | Train accuracy: 100.00%\n",
      "Test loss: 1.09733 | Test accuracy: 81.21%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "  print(f\"Epoch: {epoch}\\n---------\")\n",
    "  _ = train_step(data_loader=train_loader,\n",
    "                model=model,\n",
    "                loss_fn=loss_fn,\n",
    "                optimizer=optimizer,\n",
    "                accuracy_fn=accuracy_fn,\n",
    "                device=device,\n",
    "                verbose=True)\n",
    "  \n",
    "  _ = test_step(data_loader=test_loader,\n",
    "                model=model,\n",
    "                loss_fn=loss_fn,\n",
    "                accuracy_fn=accuracy_fn,\n",
    "                device=device,\n",
    "                verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>model_loss</th>\n",
       "      <th>model_acc (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>1.097330</td>\n",
       "      <td>81.211420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>validation</td>\n",
       "      <td>1.176991</td>\n",
       "      <td>80.758017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        split  model_loss  model_acc (%)\n",
       "0       train    0.001011     100.000000\n",
       "1        test    1.097330      81.211420\n",
       "2  validation    1.176991      80.758017"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_evals = []\n",
    "perf_eval_train = eval_model(data_loader=train_loader,\n",
    "                        model=model,\n",
    "                        loss_fn=loss_fn,\n",
    "                        accuracy_fn=accuracy_fn,\n",
    "                        device=device)\n",
    "perf_eval_train[\"split\"] = \"train\"\n",
    "perf_evals.append(perf_eval_train)\n",
    "perf_eval_test = eval_model(data_loader=test_loader,\n",
    "                        model=model,\n",
    "                        loss_fn=loss_fn,\n",
    "                        accuracy_fn=accuracy_fn,\n",
    "                        device=device)\n",
    "perf_eval_test[\"split\"] = \"test\"\n",
    "perf_evals.append(perf_eval_test)\n",
    "perf_eval_val = eval_model(data_loader=val_loader,\n",
    "                        model=model,\n",
    "                        loss_fn=loss_fn,\n",
    "                        accuracy_fn=accuracy_fn,\n",
    "                        device=device)\n",
    "perf_eval_val[\"split\"] = \"validation\"\n",
    "perf_evals.append(perf_eval_val)\n",
    "\n",
    "pd.DataFrame(perf_evals)[[\"split\",\"model_loss\",\"model_acc (%)\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_train, rows_test, rows_val = [], [], []\n",
    "for lang, split_dict in embed_and_cat_multilingual.items(): \n",
    "    X_train_lang = split_dict[\"train\"][\"embedding\"]\n",
    "    y_train_lang = split_dict[\"train\"][\"category\"]\n",
    "    X_test_lang = split_dict[\"test\"][\"embedding\"]\n",
    "    y_test_lang = split_dict[\"test\"][\"category\"]\n",
    "    X_val_lang = split_dict[\"validation\"][\"embedding\"]\n",
    "    y_val_lang = split_dict[\"validation\"][\"category\"]\n",
    "    X_train_lang, y_train_lang, X_test_lang, y_test_lang, X_val_lang, y_val_lang = np.array(X_train_lang), np.array(y_train_lang), np.array(X_test_lang), np.array(y_test_lang), np.array(X_val_lang), np.array(y_val_lang)\n",
    "\n",
    "    X_train_lang_scaled = scale_features(X_train_lang, scaler)\n",
    "    X_test_lang_scaled = scale_features(X_test_lang, scaler)\n",
    "    X_val_lang_scaled = scale_features(X_val_lang, scaler)\n",
    "\n",
    "    train_loader_lang = make_dataloader(X=X_train_lang_scaled, y=y_train_lang, batch_size=64, shuffle=True, seed=42)\n",
    "    test_loader_lang = make_dataloader(X=X_test_lang_scaled, y=y_test_lang, batch_size=1, shuffle=False, seed=42)\n",
    "    val_loader_lang = make_dataloader(X=X_val_lang_scaled, y=y_val_lang, batch_size=1, shuffle=False, seed=42)\n",
    "\n",
    "    perf_eval_train = eval_model(data_loader=train_loader_lang,\n",
    "                                model=model,\n",
    "                                loss_fn=loss_fn,\n",
    "                                accuracy_fn=accuracy_fn,\n",
    "                                device=device)\n",
    "    perf_eval_train[\"lang\"] = lang\n",
    "    rows_train.append(perf_eval_train)\n",
    "    perf_eval_test = eval_model(data_loader=test_loader_lang,\n",
    "                                model=model,\n",
    "                                loss_fn=loss_fn,\n",
    "                                accuracy_fn=accuracy_fn,\n",
    "                                device=device)\n",
    "    perf_eval_test[\"lang\"] = lang\n",
    "    rows_test.append(perf_eval_test)\n",
    "    perf_eval_val = eval_model(data_loader=val_loader_lang,\n",
    "                                model=model,\n",
    "                                loss_fn=loss_fn,\n",
    "                                accuracy_fn=accuracy_fn,\n",
    "                                device=device)\n",
    "    perf_eval_val[\"lang\"] = lang\n",
    "    rows_val.append(perf_eval_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tr-TR</td>\n",
       "      <td>100.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en-US</td>\n",
       "      <td>100.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>es-ES</td>\n",
       "      <td>100.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fr-FR</td>\n",
       "      <td>100.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>de-DE</td>\n",
       "      <td>100.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lang  train_accuracy  test_accuracy  val_accuracy\n",
       "0  tr-TR           100.0           80.0          82.0\n",
       "1  en-US           100.0           80.0          82.0\n",
       "2  es-ES           100.0           82.0          83.0\n",
       "3  fr-FR           100.0           79.0          74.0\n",
       "4  de-DE           100.0           83.0          83.0"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_perf_df = pd.DataFrame(rows_train)[[\"lang\",\"model_acc (%)\"]].rename(columns={\"model_acc (%)\": \"train_accuracy\"})\n",
    "test_perf_df = pd.DataFrame(rows_test)[[\"model_acc (%)\"]].rename(columns={\"model_acc (%)\": \"test_accuracy\"})\n",
    "val_perf_df = pd.DataFrame(rows_val)[[\"model_acc (%)\"]].rename(columns={\"model_acc (%)\": \"val_accuracy\"})\n",
    "\n",
    "pd.concat([train_perf_df, test_perf_df, val_perf_df], axis=1).round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 50, 250)\n",
    "    weight_init_name = trial.suggest_categorical(\"weight_init_name\", [\"kaiming_uniform_\", \"kaiming_normal_\", \"xavier_uniform_\", \"xavier_normal_\"])\n",
    "    weight_init = getattr(init, weight_init_name)\n",
    "    \n",
    "    layer_dims = trial.suggest_categorical(\"layer_dims\", [[50,100,50,15],\n",
    "                                                          [15,30,45,90,60,30,10]])\n",
    "    \n",
    "    act_name = trial.suggest_categorical(\"act_name\", [\"ReLU\", \"LeakyReLU\", \"RReLU\"])\n",
    "    layer_acts = [getattr(nn, act_name)() for _ in range(len(layer_dims))]\n",
    "\n",
    "    model = NNClassifier(input_dim_size=X_train.shape[1],\n",
    "                         output_dim_size=len(np.unique(y_train)),\n",
    "                         layer_dims=layer_dims,\n",
    "                         layer_acts=layer_acts,\n",
    "                         weight_init=weight_init)\n",
    "    \n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer_name\", [\"Adam\",\"RMSprop\",\"SGD\"])\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    test_accs = []\n",
    "    for epoch in range(num_epochs):\n",
    "        _ = train_step(data_loader=train_loader,\n",
    "                        model=model,\n",
    "                        loss_fn=loss_fn,\n",
    "                        optimizer=optimizer,\n",
    "                        accuracy_fn=accuracy_fn,\n",
    "                        device=device,\n",
    "                        verbose=False)\n",
    "        \n",
    "        _, test_acc = test_step(data_loader=test_loader,\n",
    "                                model=model,\n",
    "                                loss_fn=loss_fn,\n",
    "                                accuracy_fn=accuracy_fn,\n",
    "                                device=device,\n",
    "                                verbose=False)\n",
    "    #     if test_accs == []:\n",
    "    #         best_model = copy.deepcopy(model)\n",
    "    #     elif test_acc <= min(test_accs):\n",
    "    #         best_model = copy.deepcopy(model)\n",
    "    #     test_accs.append(test_acc)\n",
    "\n",
    "    # _, test_acc = test_step(data_loader=test_loader,\n",
    "    #                         model=best_model,\n",
    "    #                         loss_fn=loss_fn,\n",
    "    #                         accuracy_fn=accuracy_fn,\n",
    "    #                         device=device,\n",
    "    #                         verbose=False)\n",
    "\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 22:47:12,642] A new study created in memory with name: no-name-3ce7f291-a9ae-4a81-b1db-c3eed8424e2a\n",
      "[I 2025-03-07 22:47:15,505] Trial 0 finished with value: 73.59825102880659 and parameters: {'num_epochs': 59, 'weight_init_name': 'kaiming_uniform_', 'layer_dims': [15, 30, 45, 90, 60, 30, 10], 'act_name': 'ReLU', 'learning_rate': 0.0001119282546175383, 'optimizer_name': 'Adam'}. Best is trial 0 with value: 73.59825102880659.\n",
      "[I 2025-03-07 22:47:18,820] Trial 1 finished with value: 80.51697530864197 and parameters: {'num_epochs': 91, 'weight_init_name': 'xavier_normal_', 'layer_dims': [50, 100, 50, 15], 'act_name': 'RReLU', 'learning_rate': 0.0020540900304075525, 'optimizer_name': 'SGD'}. Best is trial 1 with value: 80.51697530864197.\n",
      "[I 2025-03-07 22:47:25,321] Trial 2 finished with value: 77.99639917695474 and parameters: {'num_epochs': 145, 'weight_init_name': 'xavier_uniform_', 'layer_dims': [15, 30, 45, 90, 60, 30, 10], 'act_name': 'ReLU', 'learning_rate': 0.0038257011339846506, 'optimizer_name': 'RMSprop'}. Best is trial 1 with value: 80.51697530864197.\n",
      "[I 2025-03-07 22:47:27,749] Trial 3 finished with value: 82.94753086419753 and parameters: {'num_epochs': 64, 'weight_init_name': 'kaiming_normal_', 'layer_dims': [50, 100, 50, 15], 'act_name': 'ReLU', 'learning_rate': 0.0007376662111107422, 'optimizer_name': 'RMSprop'}. Best is trial 3 with value: 82.94753086419753.\n",
      "[I 2025-03-07 22:47:36,655] Trial 4 finished with value: 82.02160493827161 and parameters: {'num_epochs': 228, 'weight_init_name': 'kaiming_uniform_', 'layer_dims': [50, 100, 50, 15], 'act_name': 'ReLU', 'learning_rate': 0.004995227643495581, 'optimizer_name': 'RMSprop'}. Best is trial 3 with value: 82.94753086419753.\n",
      "[I 2025-03-07 22:47:40,110] Trial 5 finished with value: 77.94495884773661 and parameters: {'num_epochs': 91, 'weight_init_name': 'kaiming_normal_', 'layer_dims': [15, 30, 45, 90, 60, 30, 10], 'act_name': 'LeakyReLU', 'learning_rate': 0.005825750417243755, 'optimizer_name': 'SGD'}. Best is trial 3 with value: 82.94753086419753.\n",
      "[I 2025-03-07 22:47:42,830] Trial 6 finished with value: 69.68878600823045 and parameters: {'num_epochs': 72, 'weight_init_name': 'kaiming_normal_', 'layer_dims': [15, 30, 45, 90, 60, 30, 10], 'act_name': 'LeakyReLU', 'learning_rate': 0.002693781632117344, 'optimizer_name': 'SGD'}. Best is trial 3 with value: 82.94753086419753.\n",
      "[I 2025-03-07 22:47:49,665] Trial 7 finished with value: 29.74537037037037 and parameters: {'num_epochs': 132, 'weight_init_name': 'kaiming_uniform_', 'layer_dims': [15, 30, 45, 90, 60, 30, 10], 'act_name': 'ReLU', 'learning_rate': 0.017873377353158044, 'optimizer_name': 'Adam'}. Best is trial 3 with value: 82.94753086419753.\n",
      "[I 2025-03-07 22:47:56,554] Trial 8 finished with value: 78.20216049382717 and parameters: {'num_epochs': 142, 'weight_init_name': 'kaiming_uniform_', 'layer_dims': [15, 30, 45, 90, 60, 30, 10], 'act_name': 'ReLU', 'learning_rate': 0.00011442495437380712, 'optimizer_name': 'RMSprop'}. Best is trial 3 with value: 82.94753086419753.\n",
      "[I 2025-03-07 22:48:00,649] Trial 9 finished with value: 78.4593621399177 and parameters: {'num_epochs': 91, 'weight_init_name': 'kaiming_uniform_', 'layer_dims': [15, 30, 45, 90, 60, 30, 10], 'act_name': 'RReLU', 'learning_rate': 0.004007076254809587, 'optimizer_name': 'SGD'}. Best is trial 3 with value: 82.94753086419753.\n",
      "[I 2025-03-07 22:48:07,809] Trial 10 finished with value: 81.64866255144032 and parameters: {'num_epochs': 198, 'weight_init_name': 'kaiming_normal_', 'layer_dims': [50, 100, 50, 15], 'act_name': 'LeakyReLU', 'learning_rate': 0.0006615258449442069, 'optimizer_name': 'RMSprop'}. Best is trial 3 with value: 82.94753086419753.\n",
      "[I 2025-03-07 22:48:16,836] Trial 11 finished with value: 15.393518518518519 and parameters: {'num_epochs': 240, 'weight_init_name': 'xavier_normal_', 'layer_dims': [50, 100, 50, 15], 'act_name': 'ReLU', 'learning_rate': 0.08363584006561303, 'optimizer_name': 'RMSprop'}. Best is trial 3 with value: 82.94753086419753.\n",
      "[I 2025-03-07 22:48:24,523] Trial 12 finished with value: 81.03137860082305 and parameters: {'num_epochs': 193, 'weight_init_name': 'xavier_uniform_', 'layer_dims': [50, 100, 50, 15], 'act_name': 'ReLU', 'learning_rate': 0.00047157466896311287, 'optimizer_name': 'RMSprop'}. Best is trial 3 with value: 82.94753086419753.\n",
      "[I 2025-03-07 22:48:34,136] Trial 13 finished with value: 80.05401234567901 and parameters: {'num_epochs': 229, 'weight_init_name': 'kaiming_normal_', 'layer_dims': [50, 100, 50, 15], 'act_name': 'ReLU', 'learning_rate': 0.0008234823202506155, 'optimizer_name': 'RMSprop'}. Best is trial 3 with value: 82.94753086419753.\n",
      "[I 2025-03-07 22:48:41,815] Trial 14 finished with value: 15.393518518518519 and parameters: {'num_epochs': 173, 'weight_init_name': 'kaiming_uniform_', 'layer_dims': [50, 100, 50, 15], 'act_name': 'ReLU', 'learning_rate': 0.017148825878750475, 'optimizer_name': 'RMSprop'}. Best is trial 3 with value: 82.94753086419753.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial no 3:\n",
      "  Objective Value: 82.94753086419753\n",
      "  Parameters:\n",
      "    num_epochs: 64\n",
      "    weight_init_name: kaiming_normal_\n",
      "    layer_dims: [50, 100, 50, 15]\n",
      "    act_name: ReLU\n",
      "    learning_rate: 0.0007376662111107422\n",
      "    optimizer_name: RMSprop\n"
     ]
    }
   ],
   "source": [
    "trial = study.best_trial\n",
    "best_params_optuna = trial.params\n",
    "\n",
    "print(f\"Best trial no {trial.number}:\")\n",
    "print(\"  Objective Value:\", trial.value)\n",
    "print(\"  Parameters:\")\n",
    "for key, value in best_params_optuna.items():\n",
    "    print(\"    {}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1    \u001b[36m76242.4584\u001b[0m       \u001b[32m0.1208\u001b[0m      \u001b[35m140.4924\u001b[0m  1.4709\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.9562\u001b[0m       \u001b[32m0.2354\u001b[0m        \u001b[35m1.9401\u001b[0m  1.5214\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1    \u001b[36m66324.0327\u001b[0m       \u001b[32m0.0979\u001b[0m      \u001b[35m113.2272\u001b[0m  1.5422\n",
      "      2       \u001b[36m91.1852\u001b[0m       \u001b[32m0.2188\u001b[0m       \u001b[35m54.0265\u001b[0m  0.1039\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.9451\u001b[0m       \u001b[32m0.2146\u001b[0m        \u001b[35m1.9246\u001b[0m  1.5841\n",
      "      2        \u001b[36m1.9332\u001b[0m       0.2354        \u001b[35m1.9244\u001b[0m  0.1022\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1    \u001b[36m84645.6703\u001b[0m       \u001b[32m0.1187\u001b[0m      \u001b[35m248.2301\u001b[0m  1.6295\n",
      "      2       \u001b[36m64.0979\u001b[0m       \u001b[32m0.2667\u001b[0m       \u001b[35m40.7546\u001b[0m  0.1074\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1     \u001b[36m5601.8590\u001b[0m       \u001b[32m0.2417\u001b[0m        \u001b[35m2.0642\u001b[0m  1.6658\n",
      "      2        \u001b[36m1.9143\u001b[0m       0.2146        \u001b[35m1.9042\u001b[0m  0.0955\n",
      "      3       \u001b[36m46.6704\u001b[0m       \u001b[32m0.2437\u001b[0m       \u001b[35m37.4258\u001b[0m  0.1022\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.9229\u001b[0m       \u001b[32m0.2354\u001b[0m        \u001b[35m1.9097\u001b[0m  1.6804\n",
      "      3        \u001b[36m1.9192\u001b[0m       0.2354        \u001b[35m1.9124\u001b[0m  0.0908\n",
      "      2      \u001b[36m103.1920\u001b[0m       \u001b[32m0.1583\u001b[0m       \u001b[35m34.4873\u001b[0m  0.1023\n",
      "      3       \u001b[36m40.4804\u001b[0m       0.2625       \u001b[35m28.7688\u001b[0m  0.0966\n",
      "      2        \u001b[36m2.1353\u001b[0m       0.2354        \u001b[35m1.8825\u001b[0m  0.0966\n",
      "      3        \u001b[36m1.8990\u001b[0m       0.2146        \u001b[35m1.8924\u001b[0m  0.0892\n",
      "      2        \u001b[36m1.9051\u001b[0m       0.2354        \u001b[35m1.8965\u001b[0m  0.0887\n",
      "      4       \u001b[36m37.1364\u001b[0m       \u001b[32m0.2562\u001b[0m       \u001b[35m29.1129\u001b[0m  0.0953\n",
      "      4        \u001b[36m1.9085\u001b[0m       0.2354        \u001b[35m1.9034\u001b[0m  0.0891\n",
      "      3       \u001b[36m32.7590\u001b[0m       0.1479       \u001b[35m21.0260\u001b[0m  0.0895\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1     \u001b[36m6126.6009\u001b[0m       \u001b[32m0.2354\u001b[0m        \u001b[35m1.8787\u001b[0m  1.8376\n",
      "      4       \u001b[36m31.9005\u001b[0m       \u001b[32m0.2729\u001b[0m       38.4693  0.0873\n",
      "      4        \u001b[36m1.8901\u001b[0m       0.2146        \u001b[35m1.8856\u001b[0m  0.0831\n",
      "      3        \u001b[36m1.8924\u001b[0m       0.2354        \u001b[35m1.8749\u001b[0m  0.0943\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m3.0423\u001b[0m       \u001b[32m0.2146\u001b[0m        \u001b[35m1.9240\u001b[0m  1.8685\n",
      "      3        \u001b[36m1.8945\u001b[0m       0.2354        \u001b[35m1.8886\u001b[0m  0.0856\n",
      "      5       \u001b[36m24.0034\u001b[0m       \u001b[32m0.2604\u001b[0m       \u001b[35m16.6109\u001b[0m  0.0918\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m2.1971\u001b[0m       \u001b[32m0.2833\u001b[0m        \u001b[35m1.7378\u001b[0m  1.8884\n",
      "      5        \u001b[36m1.9005\u001b[0m       0.2354        \u001b[35m1.8966\u001b[0m  0.0825\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m2.1466\u001b[0m       \u001b[32m0.4083\u001b[0m        \u001b[35m1.6791\u001b[0m  1.9121\n",
      "      4       \u001b[36m21.6700\u001b[0m       0.1333       \u001b[35m20.8662\u001b[0m  0.0940\n",
      "      2        \u001b[36m1.8879\u001b[0m       0.2354        \u001b[35m1.8753\u001b[0m  0.0906\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m2.8489\u001b[0m       \u001b[32m0.2146\u001b[0m        \u001b[35m1.8952\u001b[0m  1.9329\n",
      "      5       33.4027       \u001b[32m0.3521\u001b[0m       \u001b[35m24.9003\u001b[0m  0.0929\n",
      "      5        \u001b[36m1.8848\u001b[0m       0.2146        \u001b[35m1.8814\u001b[0m  0.0885\n",
      "      4        \u001b[36m1.8881\u001b[0m       0.2354        \u001b[35m1.8747\u001b[0m  0.0890\n",
      "      4        \u001b[36m1.8882\u001b[0m       0.2354        \u001b[35m1.8838\u001b[0m  0.0825\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1     \u001b[36m3194.3067\u001b[0m       \u001b[32m0.2354\u001b[0m        \u001b[35m1.9396\u001b[0m  1.9599\n",
      "      6       \u001b[36m11.4459\u001b[0m       \u001b[32m0.3229\u001b[0m       \u001b[35m10.0840\u001b[0m  0.0936\n",
      "      6        \u001b[36m1.8943\u001b[0m       0.2354        \u001b[35m1.8914\u001b[0m  0.0872\n",
      "      2        \u001b[36m1.5589\u001b[0m       \u001b[32m0.5979\u001b[0m        \u001b[35m1.3017\u001b[0m  0.1047\n",
      "      2        \u001b[36m1.8874\u001b[0m       \u001b[32m0.2354\u001b[0m        \u001b[35m1.8279\u001b[0m  0.1351\n",
      "      5       \u001b[36m10.3227\u001b[0m       \u001b[32m0.3146\u001b[0m        \u001b[35m7.7475\u001b[0m  0.0882\n",
      "      2        \u001b[36m1.5770\u001b[0m       \u001b[32m0.4167\u001b[0m        \u001b[35m1.4339\u001b[0m  0.1024\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m2.3094\u001b[0m       \u001b[32m0.4250\u001b[0m        \u001b[35m1.6666\u001b[0m  2.0148\n",
      "      3        \u001b[36m1.8868\u001b[0m       0.2354        1.8753  0.0877\n",
      "      6       \u001b[36m23.1217\u001b[0m       0.2021       \u001b[35m23.9901\u001b[0m  0.0896\n",
      "      6        \u001b[36m1.8814\u001b[0m       0.2146        \u001b[35m1.8787\u001b[0m  0.0858\n",
      "      5        \u001b[36m1.8843\u001b[0m       0.2354        \u001b[35m1.8807\u001b[0m  0.0865\n",
      "      5        \u001b[36m1.8871\u001b[0m       0.2354        1.8748  0.0924\n",
      "      2        \u001b[36m1.9064\u001b[0m       0.2354        \u001b[35m1.8797\u001b[0m  0.0929\n",
      "      7        \u001b[36m8.1861\u001b[0m       \u001b[32m0.3375\u001b[0m        \u001b[35m8.5337\u001b[0m  0.0917\n",
      "      7        \u001b[36m1.8897\u001b[0m       0.2354        \u001b[35m1.8875\u001b[0m  0.0875\n",
      "      2        \u001b[36m1.8970\u001b[0m       \u001b[32m0.2354\u001b[0m        \u001b[35m1.8800\u001b[0m  0.1468\n",
      "      3        \u001b[36m1.1893\u001b[0m       \u001b[32m0.7000\u001b[0m        \u001b[35m0.9651\u001b[0m  0.1075\n",
      "      6        \u001b[36m7.2201\u001b[0m       \u001b[32m0.3417\u001b[0m        \u001b[35m6.5315\u001b[0m  0.0943\n",
      "      4        \u001b[36m1.8866\u001b[0m       0.2354        \u001b[35m1.8753\u001b[0m  0.0970\n",
      "      3        \u001b[36m1.2151\u001b[0m       \u001b[32m0.5708\u001b[0m        \u001b[35m1.1529\u001b[0m  0.1054\n",
      "      7       28.1584       \u001b[32m0.4167\u001b[0m       \u001b[35m18.3936\u001b[0m  0.0950\n",
      "      2        \u001b[36m1.3776\u001b[0m       \u001b[32m0.5687\u001b[0m        \u001b[35m1.0857\u001b[0m  0.1096\n",
      "      7        \u001b[36m1.8793\u001b[0m       \u001b[32m0.2354\u001b[0m        \u001b[35m1.8769\u001b[0m  0.0879\n",
      "      6        \u001b[36m1.8818\u001b[0m       0.2354        \u001b[35m1.8787\u001b[0m  0.0887\n",
      "      3        \u001b[36m1.7884\u001b[0m       \u001b[32m0.2500\u001b[0m        1.8448  0.1375\n",
      "      6        \u001b[36m1.8866\u001b[0m       0.2354        1.8748  0.0957\n",
      "      3        \u001b[36m1.8844\u001b[0m       0.2354        \u001b[35m1.8749\u001b[0m  0.0954\n",
      "      8        \u001b[36m6.7788\u001b[0m       \u001b[32m0.3479\u001b[0m        \u001b[35m7.1919\u001b[0m  0.0918\n",
      "      8        \u001b[36m1.8862\u001b[0m       0.2354        \u001b[35m1.8845\u001b[0m  0.0794\n",
      "      4        \u001b[36m0.8249\u001b[0m       \u001b[32m0.7562\u001b[0m        \u001b[35m0.7922\u001b[0m  0.0989\n",
      "      7        \u001b[36m6.5326\u001b[0m       \u001b[32m0.3438\u001b[0m        6.5533  0.0920\n",
      "      5        \u001b[36m1.8864\u001b[0m       0.2354        \u001b[35m1.8752\u001b[0m  0.0875\n",
      "      8        \u001b[36m1.8778\u001b[0m       0.2354        \u001b[35m1.8756\u001b[0m  0.0804\n",
      "      3        \u001b[36m1.8799\u001b[0m       0.2354        \u001b[35m1.8752\u001b[0m  0.1313\n",
      "      8       \u001b[36m19.9587\u001b[0m       0.2271       19.9927  0.0914\n",
      "      7        \u001b[36m1.8800\u001b[0m       0.2354        \u001b[35m1.8774\u001b[0m  0.0821\n",
      "      4        \u001b[36m1.0282\u001b[0m       0.5708        \u001b[35m1.1520\u001b[0m  0.0996\n",
      "      3        \u001b[36m0.9532\u001b[0m       \u001b[32m0.6750\u001b[0m        \u001b[35m0.8473\u001b[0m  0.1000\n",
      "      7        \u001b[36m1.8862\u001b[0m       0.2354        1.8748  0.0887\n",
      "      4        \u001b[36m1.8826\u001b[0m       0.2354        \u001b[35m1.8746\u001b[0m  0.0850\n",
      "      9        \u001b[36m1.8835\u001b[0m       0.2354        \u001b[35m1.8823\u001b[0m  0.0799\n",
      "      9        \u001b[36m5.9383\u001b[0m       \u001b[32m0.3625\u001b[0m        \u001b[35m6.1316\u001b[0m  0.0910\n",
      "      4        \u001b[36m1.7653\u001b[0m       \u001b[32m0.4375\u001b[0m        \u001b[35m1.5882\u001b[0m  0.1372\n",
      "      8        \u001b[36m5.8336\u001b[0m       \u001b[32m0.3542\u001b[0m        \u001b[35m6.3026\u001b[0m  0.0893\n",
      "      9        \u001b[36m1.8768\u001b[0m       0.2354        \u001b[35m1.8748\u001b[0m  0.0846\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m3.1390\u001b[0m       \u001b[32m0.1396\u001b[0m        \u001b[35m1.9692\u001b[0m  2.3043\n",
      "      6        \u001b[36m1.8863\u001b[0m       0.2354        \u001b[35m1.8752\u001b[0m  0.0908\n",
      "      5        \u001b[36m0.7167\u001b[0m       \u001b[32m0.7937\u001b[0m        \u001b[35m0.7307\u001b[0m  0.0996\n",
      "      8        \u001b[36m1.8789\u001b[0m       0.2354        \u001b[35m1.8765\u001b[0m  0.0847\n",
      "      9       \u001b[36m19.7659\u001b[0m       0.2396       \u001b[35m17.2290\u001b[0m  0.0881\n",
      "      5        \u001b[36m0.8776\u001b[0m       \u001b[32m0.6729\u001b[0m        \u001b[35m0.9289\u001b[0m  0.1017\n",
      "      8        \u001b[36m1.8860\u001b[0m       0.2354        1.8748  0.0910\n",
      "     10        \u001b[36m1.8814\u001b[0m       0.2354        \u001b[35m1.8805\u001b[0m  0.0848\n",
      "      5        1.8827       0.2354        1.8747  0.0890\n",
      "      4        \u001b[36m0.7518\u001b[0m       \u001b[32m0.7125\u001b[0m        \u001b[35m0.7689\u001b[0m  0.1044\n",
      "     10        \u001b[36m5.2741\u001b[0m       0.3604        \u001b[35m5.3887\u001b[0m  0.0905\n",
      "      4        \u001b[36m1.8752\u001b[0m       0.2354        \u001b[35m1.8658\u001b[0m  0.1322\n",
      "     10        \u001b[36m1.8761\u001b[0m       0.2354        \u001b[35m1.8741\u001b[0m  0.0827\n",
      "      9        \u001b[36m5.4251\u001b[0m       \u001b[32m0.3688\u001b[0m        \u001b[35m5.9375\u001b[0m  0.0912\n",
      "      7        \u001b[36m1.8862\u001b[0m       0.2354        \u001b[35m1.8752\u001b[0m  0.0883\n",
      "      9        \u001b[36m1.8781\u001b[0m       0.2354        \u001b[35m1.8759\u001b[0m  0.0819\n",
      "     10       \u001b[36m19.6631\u001b[0m       0.2958       \u001b[35m15.6348\u001b[0m  0.0913\n",
      "      6        \u001b[36m0.6062\u001b[0m       \u001b[32m0.8542\u001b[0m        \u001b[35m0.4760\u001b[0m  0.0974\n",
      "     11        \u001b[36m1.8798\u001b[0m       0.2354        \u001b[35m1.8792\u001b[0m  0.0788\n",
      "      9        1.8863       0.2354        1.8748  0.0861\n",
      "      5        \u001b[36m1.5379\u001b[0m       \u001b[32m0.4750\u001b[0m        \u001b[35m1.4114\u001b[0m  0.1335\n",
      "      6        1.8829       0.2354        1.8747  0.0879\n",
      "      6        \u001b[36m0.6923\u001b[0m       0.5667        1.4838  0.0985\n",
      "      2        \u001b[36m1.8982\u001b[0m       \u001b[32m0.2146\u001b[0m        2.2763  0.1320\n",
      "      5        \u001b[36m0.6347\u001b[0m       \u001b[32m0.7146\u001b[0m        0.8645  0.0979\n",
      "     11        \u001b[36m5.2093\u001b[0m       0.3563        \u001b[35m4.7029\u001b[0m  0.0924\n",
      "     11        \u001b[36m1.8756\u001b[0m       0.2354        \u001b[35m1.8736\u001b[0m  0.0789\n",
      "     10        \u001b[36m1.8775\u001b[0m       0.2354        \u001b[35m1.8754\u001b[0m  0.0810\n",
      "      5        \u001b[36m1.8131\u001b[0m       \u001b[32m0.3000\u001b[0m        \u001b[35m1.7168\u001b[0m  0.1272\n",
      "      8        \u001b[36m1.8862\u001b[0m       0.2354        \u001b[35m1.8752\u001b[0m  0.0854\n",
      "     10        \u001b[36m5.2503\u001b[0m       \u001b[32m0.3833\u001b[0m        \u001b[35m5.6221\u001b[0m  0.0901\n",
      "     11       \u001b[36m15.9233\u001b[0m       0.1437       22.6468  0.0869\n",
      "     12        \u001b[36m1.8786\u001b[0m       0.2354        \u001b[35m1.8782\u001b[0m  0.0816\n",
      "      7        \u001b[36m0.4595\u001b[0m       \u001b[32m0.8583\u001b[0m        \u001b[35m0.4552\u001b[0m  0.0965\n",
      "     10        1.8864       0.2354        1.8748  0.0885\n",
      "      7        1.8831       0.2354        1.8747  0.0855\n",
      "      7        0.9068       \u001b[32m0.7063\u001b[0m        0.9496  0.0999\n",
      "     12        \u001b[36m5.0570\u001b[0m       0.3521        \u001b[35m4.3237\u001b[0m  0.0887\n",
      "      6        \u001b[36m0.5612\u001b[0m       \u001b[32m0.7417\u001b[0m        0.7845  0.1001\n",
      "     12        \u001b[36m1.8751\u001b[0m       0.2354        \u001b[35m1.8732\u001b[0m  0.0798\n",
      "      6        \u001b[36m1.4430\u001b[0m       \u001b[32m0.4833\u001b[0m        \u001b[35m1.3567\u001b[0m  0.1271\n",
      "     11        \u001b[36m1.8770\u001b[0m       0.2354        \u001b[35m1.8751\u001b[0m  0.0839\n",
      "      3        1.9362       \u001b[32m0.2542\u001b[0m        2.0399  0.1321\n",
      "      9        \u001b[36m1.8861\u001b[0m       0.2354        \u001b[35m1.8752\u001b[0m  0.0892\n",
      "     11        \u001b[36m4.7260\u001b[0m       \u001b[32m0.3854\u001b[0m        5.6344  0.0895\n",
      "     13        \u001b[36m1.8777\u001b[0m       0.2354        \u001b[35m1.8774\u001b[0m  0.0834\n",
      "     12       25.7844       0.2104       21.5605  0.0859\n",
      "     11        1.8866       0.2354        1.8748  0.0859\n",
      "      8        1.8833       0.2354        1.8747  0.0859\n",
      "      8        \u001b[36m0.2902\u001b[0m       \u001b[32m0.8917\u001b[0m        \u001b[35m0.4350\u001b[0m  0.0965\n",
      "      6        \u001b[36m1.6334\u001b[0m       \u001b[32m0.3229\u001b[0m        \u001b[35m1.6118\u001b[0m  0.1327\n",
      "     13        5.1027       0.2875        5.8113  0.0882\n",
      "     13        \u001b[36m1.8748\u001b[0m       0.2354        \u001b[35m1.8728\u001b[0m  0.0809\n",
      "      8        0.7924       0.6979        \u001b[35m0.7373\u001b[0m  0.1014\n",
      "      7        \u001b[36m0.5042\u001b[0m       \u001b[32m0.7958\u001b[0m        \u001b[35m0.5949\u001b[0m  0.0995\n",
      "     12        \u001b[36m1.8767\u001b[0m       0.2354        \u001b[35m1.8748\u001b[0m  0.0820\n",
      "     10        \u001b[36m1.8861\u001b[0m       0.2354        \u001b[35m1.8752\u001b[0m  0.0864\n",
      "     14        \u001b[36m1.8769\u001b[0m       0.2354        \u001b[35m1.8768\u001b[0m  0.0781\n",
      "     12        5.4052       \u001b[32m0.4208\u001b[0m        \u001b[35m5.5167\u001b[0m  0.0880\n",
      "     13       21.5754       0.2271       16.8488  0.0886\n",
      "      7        \u001b[36m1.4024\u001b[0m       0.4417        1.4795  0.1284\n",
      "     12        1.8867       0.2354        1.8748  0.0859\n",
      "      9        1.8835       0.2354        1.8748  0.0859\n",
      "      9        0.3735       0.8625        0.4464  0.0969\n",
      "      4        \u001b[36m1.8913\u001b[0m       0.2375        \u001b[35m1.8750\u001b[0m  0.1321\n",
      "     14        \u001b[36m1.8744\u001b[0m       0.2354        \u001b[35m1.8725\u001b[0m  0.0768\n",
      "     14        5.2577       0.3125        5.0552  0.0914\n",
      "     13        \u001b[36m1.8764\u001b[0m       0.2354        \u001b[35m1.8746\u001b[0m  0.0821\n",
      "      9        \u001b[36m0.6277\u001b[0m       \u001b[32m0.8063\u001b[0m        0.8835  0.1000\n",
      "      8        \u001b[36m0.4340\u001b[0m       0.7875        0.6383  0.0987\n",
      "      7       32.2965       0.2354        6.6906  0.1315\n",
      "     15        \u001b[36m1.8764\u001b[0m       0.2354        \u001b[35m1.8764\u001b[0m  0.0862\n",
      "     11        \u001b[36m1.8861\u001b[0m       0.2354        \u001b[35m1.8752\u001b[0m  0.0894\n",
      "     13        \u001b[36m4.3940\u001b[0m       0.4000        5.5634  0.0903\n",
      "     14       \u001b[36m15.3045\u001b[0m       0.2062       17.5434  0.0914\n",
      "     13        1.8868       0.2354        1.8748  0.0918\n",
      "     10        1.8836       0.2354        1.8748  0.0896\n",
      "     15        \u001b[36m1.8741\u001b[0m       0.2354        \u001b[35m1.8721\u001b[0m  0.0866\n",
      "     10        0.5321       0.8438        0.5557  0.1015\n",
      "     15        \u001b[36m4.8335\u001b[0m       0.3146        \u001b[35m4.2576\u001b[0m  0.0903\n",
      "      8        \u001b[36m1.3224\u001b[0m       \u001b[32m0.4854\u001b[0m        \u001b[35m1.2717\u001b[0m  0.1346\n",
      "     14        \u001b[36m1.8762\u001b[0m       0.2354        \u001b[35m1.8744\u001b[0m  0.0828\n",
      "     16        \u001b[36m1.8759\u001b[0m       0.2354        \u001b[35m1.8760\u001b[0m  0.1131\n",
      "      5        1.9242       0.2354        2.0819  0.1852\n",
      "     14        6.1554       \u001b[32m0.5083\u001b[0m        6.6951  0.1334\n",
      "     12        1.8861       0.2354        \u001b[35m1.8752\u001b[0m  0.1428\n",
      "     10        \u001b[36m0.6028\u001b[0m       \u001b[32m0.8167\u001b[0m        \u001b[35m0.5874\u001b[0m  0.1582\n",
      "      9        \u001b[36m0.3963\u001b[0m       0.7896        0.9476  0.1562\n",
      "     14        1.8870       0.2354        1.8749  0.1266\n",
      "     15       18.6095       0.2208       19.0649  0.1396\n",
      "     11        1.8838       0.2354        1.8748  0.1300\n",
      "     16        \u001b[36m1.8738\u001b[0m       0.2354        \u001b[35m1.8718\u001b[0m  0.1263\n",
      "      8        7.9160       0.1271        3.1556  0.1888\n",
      "     11        0.4107       \u001b[32m0.9250\u001b[0m        \u001b[35m0.3156\u001b[0m  0.1400\n",
      "     16        5.1742       0.3312        4.4671  0.1401\n",
      "     15        \u001b[36m1.8760\u001b[0m       0.2354        \u001b[35m1.8742\u001b[0m  0.1365\n",
      "     17        \u001b[36m1.8756\u001b[0m       0.2354        \u001b[35m1.8757\u001b[0m  0.0912\n",
      "      9        \u001b[36m1.2713\u001b[0m       0.4708        1.3117  0.1624\n",
      "     15        4.8852       0.3417        \u001b[35m4.8182\u001b[0m  0.0916\n",
      "     13        1.8861       0.2354        \u001b[35m1.8752\u001b[0m  0.0897\n",
      "     16       \u001b[36m13.7428\u001b[0m       0.2417       \u001b[35m14.0962\u001b[0m  0.0881\n",
      "     15        1.8871       0.2354        1.8749  0.0906\n",
      "     12        1.8840       0.2354        1.8748  0.0878\n",
      "     11        0.6923       0.6708        1.0588  0.1046\n",
      "     17        \u001b[36m1.8735\u001b[0m       0.2354        \u001b[35m1.8715\u001b[0m  0.0777\n",
      "     10        0.4364       0.7937        0.7111  0.1066\n",
      "      6        1.9491       0.2250        \u001b[35m1.8627\u001b[0m  0.1336\n",
      "     16        \u001b[36m1.8759\u001b[0m       0.2354        \u001b[35m1.8741\u001b[0m  0.0817\n",
      "     17        4.9844       0.2917        5.5475  0.0894\n",
      "     18        \u001b[36m1.8753\u001b[0m       0.2354        \u001b[35m1.8755\u001b[0m  0.0846\n",
      "     12        \u001b[36m0.2501\u001b[0m       0.9146        0.4158  0.0992\n",
      "      9       85.3895       0.0792       38.8401  0.1326\n",
      "     14        1.8862       0.2354        1.8752  0.0921\n",
      "     16        4.5709       0.3479       13.9160  0.0928\n",
      "     16        1.8872       0.2354        1.8749  0.0861\n",
      "     13        1.8841       0.2354        1.8749  0.0896\n",
      "     18        \u001b[36m1.8733\u001b[0m       0.2354        \u001b[35m1.8712\u001b[0m  0.0828\n",
      "     17       \u001b[36m12.7118\u001b[0m       0.2375       \u001b[35m11.7475\u001b[0m  0.0935\n",
      "     12        0.6144       0.7646        0.8595  0.1007\n",
      "     11        0.4423       \u001b[32m0.8313\u001b[0m        0.7607  0.1006\n",
      "     10        \u001b[36m1.2337\u001b[0m       0.4813        1.3570  0.1362\n",
      "     19        \u001b[36m1.8751\u001b[0m       0.2354        \u001b[35m1.8753\u001b[0m  0.0798\n",
      "     17        \u001b[36m1.8758\u001b[0m       0.2354        \u001b[35m1.8740\u001b[0m  0.0846\n",
      "     18        5.0006       0.2938        5.5571  0.0892\n",
      "     13        0.2910       0.9042        0.6653  0.1023\n",
      "      7        \u001b[36m1.8735\u001b[0m       0.2354        1.8741  0.1368\n",
      "     15        1.8862       0.2354        1.8752  0.0883\n",
      "     17       11.4232       0.4208        4.8562  0.0909\n",
      "     19        \u001b[36m1.8730\u001b[0m       0.2354        \u001b[35m1.8709\u001b[0m  0.0829\n",
      "     17        1.8873       0.2354        1.8749  0.0871\n",
      "     14        1.8842       0.2354        1.8749  0.0863\n",
      "     18       \u001b[36m10.8454\u001b[0m       0.2854       \u001b[35m10.2227\u001b[0m  0.0863\n",
      "     20        \u001b[36m1.8749\u001b[0m       0.2354        \u001b[35m1.8752\u001b[0m  0.0812\n",
      "     13        \u001b[36m0.5735\u001b[0m       \u001b[32m0.8396\u001b[0m        \u001b[35m0.5677\u001b[0m  0.1028\n",
      "     10       74.2212       0.1729       37.3747  0.1379\n",
      "     18        \u001b[36m1.8756\u001b[0m       0.2354        \u001b[35m1.8739\u001b[0m  0.0801\n",
      "     12        \u001b[36m0.2589\u001b[0m       \u001b[32m0.8896\u001b[0m        0.7497  0.1057\n",
      "     19        5.3221       0.3333        4.9277  0.0900\n",
      "     14        0.3868       0.9021        0.4281  0.1001\n",
      "     11        1.3038       0.4500        1.3963  0.1375\n",
      "     16        1.8862       0.2354        1.8752  0.0866\n",
      "     20        \u001b[36m1.8726\u001b[0m       0.2354        \u001b[35m1.8705\u001b[0m  0.0825\n",
      "     18        1.8874       0.2354        1.8749  0.0846\n",
      "     18        \u001b[36m4.2407\u001b[0m       0.5083        \u001b[35m4.5861\u001b[0m  0.0890\n",
      "     15        1.8844       0.2354        1.8749  0.0874\n",
      "     19       10.8978       0.2917        \u001b[35m9.1640\u001b[0m  0.0899\n",
      "     21        \u001b[36m1.8747\u001b[0m       0.2354        \u001b[35m1.8751\u001b[0m  0.0812\n",
      "     19        \u001b[36m1.8755\u001b[0m       0.2354        \u001b[35m1.8738\u001b[0m  0.0823\n",
      "      8        1.8760       0.2354        1.8740  0.1333\n",
      "     14        \u001b[36m0.3752\u001b[0m       \u001b[32m0.8583\u001b[0m        0.6330  0.1016\n",
      "     20        5.2509       0.3229        5.0614  0.0903\n",
      "     13        \u001b[36m0.1961\u001b[0m       \u001b[32m0.9021\u001b[0m        0.6880  0.1019\n",
      "     11      652.9565       0.0854     7780.8019  0.1313\n",
      "     21        \u001b[36m1.8722\u001b[0m       0.2354        \u001b[35m1.8701\u001b[0m  0.0809\n",
      "     17        1.8862       0.2354        1.8752  0.0875\n",
      "     19        1.8886       0.2354        1.8749  0.0878\n",
      "     15        \u001b[36m0.2487\u001b[0m       \u001b[32m0.9396\u001b[0m        0.3374  0.0996\n",
      "     19        \u001b[36m3.3549\u001b[0m       0.4062        5.9199  0.0886\n",
      "     16        1.8845       0.2354        1.8749  0.0862\n",
      "     20        \u001b[36m8.8354\u001b[0m       0.3458        \u001b[35m7.6662\u001b[0m  0.0907\n",
      "     22        \u001b[36m1.8746\u001b[0m       0.2354        \u001b[35m1.8750\u001b[0m  0.0798\n",
      "     20        \u001b[36m1.8754\u001b[0m       0.2354        \u001b[35m1.8737\u001b[0m  0.0816\n",
      "     12        1.3247       0.4854        \u001b[35m1.2517\u001b[0m  0.1280\n",
      "     21        \u001b[36m4.2182\u001b[0m       0.3208        4.8513  0.0866\n",
      "     15        \u001b[36m0.2954\u001b[0m       \u001b[32m0.8604\u001b[0m        0.5977  0.0997\n",
      "     14        0.2241       0.8917        0.7200  0.1026\n",
      "     22        \u001b[36m1.8718\u001b[0m       0.2354        \u001b[35m1.8696\u001b[0m  0.0817\n",
      "      9        1.8759       0.2354        1.8740  0.1331\n",
      "     18        1.8862       0.2354        1.8752  0.0888\n",
      "     20        1.8877       0.2354        1.8749  0.0919\n",
      "     20        4.6074       0.4104        7.7767  0.0936\n",
      "     17        1.8846       0.2354        1.8749  0.0910\n",
      "     21       11.6248       0.2854        8.5942  0.0888\n",
      "     16        \u001b[36m0.1583\u001b[0m       0.9313        \u001b[35m0.2761\u001b[0m  0.1010\n",
      "     23        \u001b[36m1.8745\u001b[0m       0.2354        \u001b[35m1.8749\u001b[0m  0.0859\n",
      "     21        \u001b[36m1.8753\u001b[0m       0.2354        \u001b[35m1.8737\u001b[0m  0.0867\n",
      "     12     2247.5117       0.1229     3449.9914  0.1343\n",
      "     22        5.5455       0.3000        \u001b[35m4.2121\u001b[0m  0.0943\n",
      "     23        \u001b[36m1.8713\u001b[0m       0.2354        \u001b[35m1.8691\u001b[0m  0.0824\n",
      "     16        \u001b[36m0.2327\u001b[0m       0.8396        0.6095  0.0995\n",
      "     13        \u001b[36m1.1919\u001b[0m       \u001b[32m0.5167\u001b[0m        \u001b[35m1.2168\u001b[0m  0.1351\n",
      "     19        1.8862       0.2354        1.8752  0.0898\n",
      "     15        0.2390       0.9021        \u001b[35m0.5519\u001b[0m  0.1053\n",
      "     21        1.8877       0.2354        1.8749  0.0906\n",
      "     21        3.5117       0.4125        4.8709  0.0862\n",
      "     18        1.8847       0.2354        1.8749  0.0884\n",
      "     24        \u001b[36m1.8745\u001b[0m       0.2354        \u001b[35m1.8748\u001b[0m  0.0795\n",
      "     22       12.4710       \u001b[32m0.4313\u001b[0m       15.0225  0.0879\n",
      "     22        \u001b[36m1.8752\u001b[0m       0.2354        \u001b[35m1.8736\u001b[0m  0.0785\n",
      "     17        \u001b[36m0.1222\u001b[0m       \u001b[32m0.9563\u001b[0m        \u001b[35m0.2137\u001b[0m  0.1012\n",
      "     10        1.8760       0.2354        1.8740  0.1272\n",
      "     24        \u001b[36m1.8708\u001b[0m       0.2354        \u001b[35m1.8685\u001b[0m  0.0778\n",
      "     23        4.4851       0.3312        4.2522  0.0901\n",
      "     13     9807.6538       0.2167     7567.5843  0.1310\n",
      "     17        0.2410       0.8458        0.8274  0.1025\n",
      "     20        1.8862       0.2354        1.8752  0.0879\n",
      "     22        4.4881       0.3708        7.3548  0.0856\n",
      "     25        \u001b[36m1.8744\u001b[0m       0.2354        \u001b[35m1.8748\u001b[0m  0.0815\n",
      "     19        1.8848       0.2354        1.8750  0.0860\n",
      "     22        1.8878       0.2354        1.8749  0.0896\n",
      "     23        \u001b[36m1.8752\u001b[0m       0.2354        \u001b[35m1.8735\u001b[0m  0.0829\n",
      "     16        0.2851       0.8833        0.8167  0.0979\n",
      "     23        9.5445       0.3812        \u001b[35m6.6501\u001b[0m  0.0856\n",
      "     14        \u001b[36m1.1363\u001b[0m       \u001b[32m0.5979\u001b[0m        \u001b[35m1.1220\u001b[0m  0.1323\n",
      "     18        \u001b[36m0.0890\u001b[0m       \u001b[32m0.9688\u001b[0m        \u001b[35m0.1697\u001b[0m  0.0997\n",
      "     25        \u001b[36m1.8702\u001b[0m       0.2354        \u001b[35m1.8677\u001b[0m  0.0785\n",
      "     24        4.7149       0.2979        4.8065  0.0908\n",
      "     11        1.8759       0.2354        1.8739  0.1281\n",
      "     21        1.8862       0.2354        1.8752  0.0870\n",
      "     26        \u001b[36m1.8743\u001b[0m       0.2354        \u001b[35m1.8748\u001b[0m  0.0826\n",
      "     23        3.5325       0.4292        4.6551  0.0864\n",
      "     24        \u001b[36m1.8751\u001b[0m       0.2354        \u001b[35m1.8734\u001b[0m  0.0808\n",
      "     20        1.8848       0.2146        1.8750  0.0877\n",
      "     23        1.8879       0.2354        1.8749  0.0862\n",
      "     18        0.3126       \u001b[32m0.8938\u001b[0m        \u001b[35m0.5569\u001b[0m  0.0975\n",
      "     24        \u001b[36m6.7545\u001b[0m       0.2979       10.0585  0.0852\n",
      "     17        \u001b[36m0.1959\u001b[0m       0.9021        0.7413  0.1006\n",
      "     14     6999.0844       0.1792     1989.9959  0.1285\n",
      "     26        \u001b[36m1.8693\u001b[0m       0.2354        \u001b[35m1.8667\u001b[0m  0.0792\n",
      "     19        \u001b[36m0.0704\u001b[0m       0.9417        0.3567  0.0992\n",
      "     25        \u001b[36m3.4678\u001b[0m       \u001b[32m0.3792\u001b[0m        \u001b[35m3.1685\u001b[0m  0.0884\n",
      "     15        \u001b[36m1.0534\u001b[0m       0.5625        1.1572  0.1327\n",
      "     27        \u001b[36m1.8743\u001b[0m       0.2354        \u001b[35m1.8747\u001b[0m  0.0796\n",
      "     25        \u001b[36m1.8750\u001b[0m       0.2354        \u001b[35m1.8733\u001b[0m  0.0812\n",
      "     22        1.8862       0.2354        1.8752  0.0887\n",
      "     21        1.8849       0.2146        1.8750  0.0843\n",
      "     24        4.4322       0.3646       17.4158  0.0880\n",
      "     24        1.8897       0.2354        1.8749  0.0887\n",
      "     25        9.2399       0.4000        \u001b[35m6.2572\u001b[0m  0.0890\n",
      "     19        \u001b[36m0.2174\u001b[0m       0.8792        0.6286  0.1031\n",
      "     12        1.8759       0.2354        1.8739  0.1333\n",
      "     27        \u001b[36m1.8683\u001b[0m       0.2354        \u001b[35m1.8656\u001b[0m  0.0802\n",
      "     18        0.2314       0.8583        1.0980  0.1004\n",
      "     26        3.7211       0.3375        \u001b[35m3.0335\u001b[0m  0.0852\n",
      "     20        \u001b[36m0.0647\u001b[0m       0.9646        0.2158  0.1023\n",
      "     15     3271.5754       0.2146     2944.7028  0.1340\n",
      "     28        \u001b[36m1.8743\u001b[0m       0.2354        \u001b[35m1.8747\u001b[0m  0.0842\n",
      "     26        \u001b[36m1.8749\u001b[0m       0.2354        \u001b[35m1.8732\u001b[0m  0.0851\n",
      "     23        1.8863       0.2354        1.8752  0.0897\n",
      "     22        1.8850       0.2146        1.8750  0.0884\n",
      "     25        9.8819       \u001b[32m0.6062\u001b[0m        \u001b[35m2.5622\u001b[0m  0.0931\n",
      "     25        1.8880       0.2354        1.8749  0.0933\n",
      "     26        7.9969       0.3125       30.4629  0.0946\n",
      "     28        \u001b[36m1.8671\u001b[0m       0.2354        \u001b[35m1.8641\u001b[0m  0.0854\n",
      "     16        1.0960       0.5833        \u001b[35m1.0455\u001b[0m  0.1349\n",
      "     20        \u001b[36m0.1374\u001b[0m       \u001b[32m0.9000\u001b[0m        \u001b[35m0.4316\u001b[0m  0.1068\n",
      "     19        0.4759       0.8792        0.6411  0.1033\n",
      "     27        4.0180       0.3646        3.8888  0.0881\n",
      "     29        \u001b[36m1.8742\u001b[0m       0.2354        \u001b[35m1.8747\u001b[0m  0.0829\n",
      "     13        1.8759       0.2354        1.8739  0.1392\n",
      "     27        \u001b[36m1.8748\u001b[0m       0.2354        \u001b[35m1.8731\u001b[0m  0.0824\n",
      "     21        \u001b[36m0.0394\u001b[0m       0.9604        0.2107  0.1094\n",
      "     24        1.8863       0.2354        1.8752  0.0906\n",
      "     23        1.8850       0.2146        1.8750  0.0941\n",
      "     26        \u001b[36m2.6236\u001b[0m       0.4167        7.1310  0.0911\n",
      "     26        1.8897       0.2354        1.8749  0.0909\n",
      "     27       18.6754       0.2729       10.1611  0.0951\n",
      "     29        \u001b[36m1.8655\u001b[0m       0.2354        \u001b[35m1.8621\u001b[0m  0.0810\n",
      "     16     3792.9056       0.2146     1980.4416  0.1343\n",
      "     21        \u001b[36m0.1285\u001b[0m       0.8979        \u001b[35m0.3865\u001b[0m  0.1034\n",
      "     28        3.6708       0.3604        3.8965  0.0887\n",
      "     20        0.6453       0.8063        0.9748  0.1031\n",
      "     30        \u001b[36m1.8742\u001b[0m       0.2354        \u001b[35m1.8747\u001b[0m  0.0833\n",
      "     28        \u001b[36m1.8747\u001b[0m       0.2354        \u001b[35m1.8730\u001b[0m  0.0851\n",
      "     17        1.0962       0.5563        1.0621  0.1369\n",
      "     25        1.8863       0.2354        1.8752  0.0892\n",
      "     24        1.8851       0.2146        1.8750  0.0895\n",
      "     22        0.2482       0.9437        0.4529  0.1011\n",
      "     27        2.7939       0.4938        4.0336  0.0929\n",
      "     30        \u001b[36m1.8634\u001b[0m       0.2354        \u001b[35m1.8596\u001b[0m  0.0808\n",
      "     27        1.8881       0.2354        1.8749  0.0907\n",
      "     28        6.9216       0.3937       10.1038  0.0917\n",
      "     14        1.8759       0.2354        1.8739  0.1363\n",
      "     29        3.7428       0.3708        4.3180  0.0896\n",
      "     31        \u001b[36m1.8742\u001b[0m       0.2354        \u001b[35m1.8747\u001b[0m  0.0898\n",
      "     22        \u001b[36m0.0891\u001b[0m       0.8896        0.5860  0.1018\n",
      "     17     1536.4123       0.2188      742.3702  0.1319\n",
      "     29        \u001b[36m1.8746\u001b[0m       0.2354        \u001b[35m1.8729\u001b[0m  0.0860\n",
      "     21        0.5428       0.8271        1.1652  0.1009\n",
      "     26        1.8863       0.2354        1.8752  0.0937\n",
      "     31        \u001b[36m1.8605\u001b[0m       0.2354        \u001b[35m1.8555\u001b[0m  0.0830\n",
      "     25        1.8851       0.2146        1.8750  0.0952\n",
      "     28        3.0530       0.6042        \u001b[35m2.1565\u001b[0m  0.0884\n",
      "     23        0.1011       0.9187        0.4257  0.1027\n",
      "     28        1.8881       0.2354        1.8749  0.0955\n",
      "     29        7.4852       0.3542        \u001b[35m5.8325\u001b[0m  0.0888\n",
      "     18        \u001b[36m1.0421\u001b[0m       0.5250        1.5584  0.1428\n",
      "     32        \u001b[36m1.8742\u001b[0m       0.2354        \u001b[35m1.8746\u001b[0m  0.0793\n",
      "     30        \u001b[36m1.8744\u001b[0m       0.2354        \u001b[35m1.8728\u001b[0m  0.0832\n",
      "     30        \u001b[36m3.3558\u001b[0m       0.3646        5.2089  0.0910\n",
      "     15        1.8759       0.2354        1.8739  0.1293\n",
      "     23        \u001b[36m0.0609\u001b[0m       0.8500        0.7957  0.1020\n",
      "     22        0.4221       0.7479        1.7850  0.1059\n",
      "     32        \u001b[36m1.8562\u001b[0m       0.2354        \u001b[35m1.8504\u001b[0m  0.0824\n",
      "     27        1.8863       0.2354        1.8752  0.0883\n",
      "     29        2.8916       0.3708       13.7465  0.0876\n",
      "     26        1.8852       0.2146        1.8750  0.0882\n",
      "     30        \u001b[36m4.6730\u001b[0m       0.3438        \u001b[35m4.9796\u001b[0m  0.0858\n",
      "     29        1.8881       0.2354        1.8749  0.0891\n",
      "     18      458.3461       0.0708      251.4225  0.1337\n",
      "     24        0.1168       0.9458        0.3082  0.1034\n",
      "     33        \u001b[36m1.8742\u001b[0m       0.2354        \u001b[35m1.8746\u001b[0m  0.0829\n",
      "     31        \u001b[36m1.8743\u001b[0m       0.2354        \u001b[35m1.8726\u001b[0m  0.0830\n",
      "     31        3.3739       0.3583        5.2162  0.0942\n",
      "     19        1.2321       0.4729        5.9334  0.1333\n",
      "     33        \u001b[36m1.8507\u001b[0m       0.2354        \u001b[35m1.8434\u001b[0m  0.0844\n",
      "     24        0.1082       0.8396        1.2115  0.1020\n",
      "     28        1.8863       0.2354        1.8752  0.0898\n",
      "     23        0.6261       0.7896        1.0831  0.1005\n",
      "     27        1.8852       0.2146        1.8750  0.0912\n",
      "     30        3.9270       0.5417        2.9339  0.0923\n",
      "     16        1.8759       0.2354        1.8739  0.1333\n",
      "     31        \u001b[36m4.5101\u001b[0m       0.3771        \u001b[35m4.8243\u001b[0m  0.0939\n",
      "     30        1.8881       0.2354        1.8749  0.0947\n",
      "     34        \u001b[36m1.8741\u001b[0m       0.2354        \u001b[35m1.8746\u001b[0m  0.0855\n",
      "     25        0.1433       \u001b[32m0.9729\u001b[0m        0.2026  0.1022\n",
      "     32        \u001b[36m1.8742\u001b[0m       0.2354        \u001b[35m1.8725\u001b[0m  0.0892\n",
      "     19      138.9439       0.1083       96.4694  0.1379\n",
      "     32        \u001b[36m3.1904\u001b[0m       0.2771        7.9551  0.0895\n",
      "     34        \u001b[36m1.8431\u001b[0m       0.2354        \u001b[35m1.8332\u001b[0m  0.0820\n",
      "     29        1.8863       0.2354        1.8752  0.0919\n",
      "     25        0.2616       0.8708        0.9628  0.0974\n",
      "     28        1.8853       0.2146        1.8751  0.0907\n",
      "     31        2.6941       \u001b[32m0.6562\u001b[0m        \u001b[35m1.8137\u001b[0m  0.0950\n",
      "     24        0.5853       0.8063        0.8371  0.1010\n",
      "     31        1.8896       0.2354        1.8749  0.0899\n",
      "     32        \u001b[36m3.9816\u001b[0m       0.3563        \u001b[35m3.9872\u001b[0m  0.0931\n",
      "     35        \u001b[36m1.8741\u001b[0m       0.2354        \u001b[35m1.8746\u001b[0m  0.0780\n",
      "     20        2.0718       0.3583        1.6412  0.1356\n",
      "     33        \u001b[36m1.8740\u001b[0m       0.2354        \u001b[35m1.8723\u001b[0m  0.0835\n",
      "     26        0.1132       0.9229        0.3420  0.0997\n",
      "     17        1.8759       0.2354        1.8739  0.1311\n",
      "     33        3.7731       \u001b[32m0.3958\u001b[0m        4.5018  0.0909\n",
      "     35        \u001b[36m1.8320\u001b[0m       0.2333        \u001b[35m1.8187\u001b[0m  0.0819\n",
      "     30        1.8863       0.2354        1.8752  0.0883\n",
      "     29        1.8853       0.2146        1.8751  0.0884\n",
      "     32        2.7060       0.3854       11.9873  0.0872\n",
      "     20       50.7922       0.1229       46.1898  0.1316\n",
      "     26        0.6705       0.8417        1.1557  0.1015\n",
      "     32        1.8882       0.2354        1.8749  0.0866\n",
      "     36        \u001b[36m1.8741\u001b[0m       0.2354        \u001b[35m1.8746\u001b[0m  0.0814\n",
      "     33        4.2040       0.4042        4.4572  0.0893\n",
      "     25        0.4147       0.8917        0.7690  0.1005\n",
      "     34        \u001b[36m1.8739\u001b[0m       0.2354        \u001b[35m1.8722\u001b[0m  0.0878\n",
      "     36        \u001b[36m1.8161\u001b[0m       \u001b[32m0.2396\u001b[0m        \u001b[35m1.8004\u001b[0m  0.0837\n",
      "     27        0.1568       0.9437        0.3443  0.1017\n",
      "     34        \u001b[36m2.1796\u001b[0m       0.3875        4.2248  0.0898\n",
      "     21        1.6391       0.4042        1.4936  0.1390\n",
      "     31        1.8863       0.2354        1.8752  0.0898\n",
      "     30        1.8853       0.2146        1.8751  0.0899\n",
      "     18        1.8759       0.2354        1.8739  0.1324\n",
      "     33        2.7684       0.5375        3.4916  0.0914\n",
      "     37        \u001b[36m1.8741\u001b[0m       0.2354        \u001b[35m1.8746\u001b[0m  0.0844\n",
      "     33        1.8882       0.2354        1.8749  0.0915\n",
      "     34       18.0865       0.3458        7.6267  0.0917\n",
      "     27        0.6409       0.7771        0.8924  0.1017\n",
      "     35        \u001b[36m1.8737\u001b[0m       0.2354        \u001b[35m1.8720\u001b[0m  0.0813\n",
      "     26        0.4132       0.8500        1.0033  0.1012\n",
      "     37        \u001b[36m1.7957\u001b[0m       \u001b[32m0.2562\u001b[0m        \u001b[35m1.7825\u001b[0m  0.0838\n",
      "     21       31.1166       0.2479       22.0456  0.1324\n",
      "     35        2.5958       \u001b[32m0.4083\u001b[0m        4.2630  0.0890\n",
      "     28        0.1284       0.9563        0.2591  0.1046\n",
      "     32        1.8863       0.2354        1.8752  0.0954\n",
      "     31        1.8853       0.2146        1.8751  0.0898\n",
      "     38        \u001b[36m1.8741\u001b[0m       0.2354        \u001b[35m1.8746\u001b[0m  0.0827\n",
      "     34        \u001b[36m2.1088\u001b[0m       0.4417        5.6297  0.0882\n",
      "     34        1.8885       0.2354        1.8749  0.0883\n",
      "     35        4.0158       \u001b[32m0.4354\u001b[0m        \u001b[35m3.5172\u001b[0m  0.0904\n",
      "     36        \u001b[36m1.8736\u001b[0m       0.2354        \u001b[35m1.8718\u001b[0m  0.0813\n",
      "     28        0.7306       0.6312       26.7877  0.0996\n",
      "     22        7.0223       0.1500      223.3866  0.1352\n",
      "     38        \u001b[36m1.7732\u001b[0m       \u001b[32m0.3417\u001b[0m        \u001b[35m1.7604\u001b[0m  0.0785\n",
      "     19        1.8759       0.2354        1.8739  0.1313\n",
      "     27        0.2811       0.8708        1.1786  0.1055\n",
      "     36        2.2653       \u001b[32m0.4125\u001b[0m        3.9551  0.0882\n",
      "     29        0.1782       0.9500        0.4325  0.1050\n",
      "     39        \u001b[36m1.8741\u001b[0m       0.2354        \u001b[35m1.8746\u001b[0m  0.0827\n",
      "     33        1.8863       0.2354        1.8752  0.0896\n",
      "     32        1.8854       0.2146        1.8751  0.0877\n",
      "     35        2.4876       0.5979        2.7196  0.0877\n",
      "     22       17.5621       0.2250       12.2683  0.1370\n",
      "     35        1.8885       0.2354        1.8749  0.0885\n",
      "     37        \u001b[36m1.8734\u001b[0m       0.2354        \u001b[35m1.8716\u001b[0m  0.0785\n",
      "     36        \u001b[36m2.7415\u001b[0m       \u001b[32m0.4417\u001b[0m        \u001b[35m3.0152\u001b[0m  0.0908\n",
      "     39        \u001b[36m1.7534\u001b[0m       \u001b[32m0.3729\u001b[0m        \u001b[35m1.6950\u001b[0m  0.0831\n",
      "     29        8.7848       0.5979        2.9353  0.1047\n",
      "     37        2.1863       0.4125        4.0438  0.0880\n",
      "     28        0.2711       0.8479        0.9146  0.1029\n",
      "     23      479.2204       0.1479      138.8603  0.1364\n",
      "     40        \u001b[36m1.8741\u001b[0m       0.2354        \u001b[35m1.8746\u001b[0m  0.0826\n",
      "     34        1.8863       0.2354        1.8752  0.0890\n",
      "     33        1.8854       0.2146        1.8751  0.0890\n",
      "     20        1.8759       0.2354        1.8739  0.1379\n",
      "     38        \u001b[36m1.8732\u001b[0m       0.2354        \u001b[35m1.8714\u001b[0m  0.0817\n",
      "     36        2.3790       0.4229       11.1294  0.0941\n",
      "     30        0.2314       0.8625        0.5382  0.1022\n",
      "     36        1.8885       0.2354        1.8749  0.0928\n",
      "     37        \u001b[36m2.4352\u001b[0m       \u001b[32m0.4458\u001b[0m        3.1762  0.0900\n",
      "     40        \u001b[36m1.7350\u001b[0m       0.3146        \u001b[35m1.6676\u001b[0m  0.0861\n",
      "     23       11.2917       \u001b[32m0.3458\u001b[0m        8.9880  0.1404\n",
      "     30        1.8762       0.7021        1.0912  0.1054\n",
      "     38        \u001b[36m2.0303\u001b[0m       \u001b[32m0.4250\u001b[0m        3.8723  0.0931\n",
      "     41        \u001b[36m1.8741\u001b[0m       0.2354        \u001b[35m1.8746\u001b[0m  0.0793\n",
      "     29        0.2553       0.8625        0.7517  0.1014\n",
      "     39        \u001b[36m1.8730\u001b[0m       0.2354        \u001b[35m1.8712\u001b[0m  0.0797\n",
      "     35        1.8863       0.2354        1.8752  0.0932\n",
      "     34        1.8854       0.2146        1.8751  0.0930\n",
      "     37        2.3946       0.5312        4.8455  0.0905\n",
      "     41        \u001b[36m1.7191\u001b[0m       0.3604        \u001b[35m1.6264\u001b[0m  0.0740\n",
      "     37        1.8885       0.2354        1.8749  0.0912\n",
      "     31        0.2694       0.9396        0.4096  0.1070\n",
      "     38        \u001b[36m2.2632\u001b[0m       0.4458        \u001b[35m3.0127\u001b[0m  0.0957\n",
      "     24      714.3245       0.1167      537.4492  0.1381\n",
      "     21        1.8759       0.2354        1.8739  0.1427\n",
      "     42        \u001b[36m1.8741\u001b[0m       0.2354        \u001b[35m1.8746\u001b[0m  0.0800\n",
      "     39        2.0462       \u001b[32m0.4458\u001b[0m        3.7376  0.0918\n",
      "     40        \u001b[36m1.8727\u001b[0m       0.2354        \u001b[35m1.8709\u001b[0m  0.0812\n",
      "     31        0.8548       0.7625        0.7369  0.1061\n",
      "     36        1.8863       0.2354        1.8752  0.0894\n",
      "     42        \u001b[36m1.7146\u001b[0m       0.3125        1.6427  0.0788\n",
      "     30        \u001b[36m0.1829\u001b[0m       \u001b[32m0.9083\u001b[0m        0.5643  0.1102\n",
      "     35        1.8854       0.2146        1.8751  0.0932\n",
      "     38        \u001b[36m1.5287\u001b[0m       0.6271        2.0718  0.0921\n",
      "     24        7.6480       0.2854        6.6630  0.1385\n",
      "     38        1.8885       0.2354        1.8749  0.0916\n",
      "     39        2.4185       \u001b[32m0.4854\u001b[0m        \u001b[35m2.6087\u001b[0m  0.0925\n",
      "     32        0.2663       0.9167        0.3743  0.1047\n",
      "     43        \u001b[36m1.8741\u001b[0m       0.2354        \u001b[35m1.8746\u001b[0m  0.0870\n",
      "     40        \u001b[36m1.6838\u001b[0m       0.4375        3.5911  0.0885\n",
      "     41        \u001b[36m1.8725\u001b[0m       0.2354        \u001b[35m1.8706\u001b[0m  0.0842\n",
      "     25      235.1973       0.1271      505.9118  0.1363\n",
      "     43        \u001b[36m1.7112\u001b[0m       0.1979        1.8428  0.0846\n",
      "     32        0.6108       0.8083        0.5474  0.1059\n",
      "     37        1.8863       0.2354        1.8752  0.0930\n",
      "     36        1.8854       0.2146        1.8751  0.0915\n",
      "     22        1.8759       0.2354        1.8739  0.1362\n",
      "     39        3.0499       0.3292       38.7892  0.0898\n",
      "     31        \u001b[36m0.1556\u001b[0m       \u001b[32m0.9146\u001b[0m        0.8149  0.1030\n",
      "     39        1.8885       0.2354        1.8749  0.0932\n",
      "     40        2.3578       \u001b[32m0.5229\u001b[0m        \u001b[35m2.6021\u001b[0m  0.0936\n",
      "     33        0.2142       0.8812        0.5459  0.0994\n",
      "     44        \u001b[36m1.8741\u001b[0m       0.2354        \u001b[35m1.8746\u001b[0m  0.0833\n",
      "     25        6.0791       0.2542        5.4526  0.1370\n",
      "     41        1.7515       \u001b[32m0.4521\u001b[0m        3.6887  0.0911\n",
      "     42        \u001b[36m1.8722\u001b[0m       0.2354        \u001b[35m1.8703\u001b[0m  0.0823\n",
      "     44        \u001b[36m1.6719\u001b[0m       0.3042        1.7109  0.0805\n",
      "     38        1.8863       0.2354        1.8752  0.0874\n",
      "     37        1.8854       0.2146        1.8751  0.0873\n",
      "     40       32.5932       0.4188       11.8238  0.0897\n",
      "     33        0.3963       0.8313        0.4996  0.0999\n",
      "     40        1.8885       0.2354        1.8749  0.0911\n",
      "     32        \u001b[36m0.0849\u001b[0m       0.9104        0.6930  0.1021\n",
      "     26      135.7812       0.2354       52.0639  0.1390\n",
      "     41        2.6486       0.4958        2.8057  0.0944\n",
      "     45        \u001b[36m1.8741\u001b[0m       0.2354        \u001b[35m1.8746\u001b[0m  0.0794\n",
      "     23        1.8759       0.2354        1.8739  0.1378\n",
      "     43        \u001b[36m1.8719\u001b[0m       0.2354        \u001b[35m1.8700\u001b[0m  0.0851\n",
      "     34        0.4710       0.9104        0.5401  0.1033\n",
      "     42        \u001b[36m1.5993\u001b[0m       \u001b[32m0.4625\u001b[0m        3.7488  0.0908\n",
      "     45        1.7013       0.3521        \u001b[35m1.5803\u001b[0m  0.0844\n",
      "     38        1.8854       0.2146        1.8751  0.0882\n",
      "     39        1.8863       0.2354        1.8752  0.0950\n",
      "     41        7.9150       0.5667        5.0467  0.0910\n",
      "     26        5.3311       0.2687        4.8420  0.1343\n",
      "     41        1.8885       0.2354        1.8749  0.0887\n",
      "     34        0.2893       0.8688        0.4252  0.1007\n",
      "     46        \u001b[36m1.8741\u001b[0m       0.2354        \u001b[35m1.8746\u001b[0m  0.0817\n",
      "     42       44.9360       0.1229      352.5045  0.0895\n",
      "     33        \u001b[36m0.0798\u001b[0m       0.9021        0.6483  0.1037\n",
      "     44        \u001b[36m1.8715\u001b[0m       0.2354        \u001b[35m1.8696\u001b[0m  0.0831\n",
      "     43        \u001b[36m1.5778\u001b[0m       \u001b[32m0.4688\u001b[0m        3.8089  0.0904\n",
      "     46        \u001b[36m1.6283\u001b[0m       0.3458        1.6177  0.0863\n",
      "     35        0.3058       0.8875        0.8351  0.1053\n",
      "     27      152.1520       0.1167      102.7855  0.1384\n",
      "     39        1.8854       0.2146        1.8751  0.0854\n",
      "     24        1.8759       0.2354        1.8739  0.1346\n",
      "     42        2.9301       \u001b[32m0.6604\u001b[0m        \u001b[35m1.6798\u001b[0m  0.0867\n",
      "     40        1.8863       0.2354        1.8752  0.0949\n",
      "     42        1.8885       0.2354        1.8749  0.0907\n",
      "     47        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0839\n",
      "     35        0.2347       0.8479        0.5009  0.1055\n",
      "     43      652.8738       0.1146      186.4090  0.0946\n",
      "     45        \u001b[36m1.8712\u001b[0m       0.2354        \u001b[35m1.8692\u001b[0m  0.0816\n",
      "     34        0.1190       0.8750        0.7244  0.1020\n",
      "     27        4.8169       0.2687        4.2725  0.1375\n",
      "     47        1.6603       \u001b[32m0.3833\u001b[0m        \u001b[35m1.5404\u001b[0m  0.0842\n",
      "     44        \u001b[36m1.5494\u001b[0m       0.4667        3.9953  0.0921\n",
      "     40        1.8854       0.2146        1.8751  0.0906\n",
      "     36        0.2676       0.9229        0.5758  0.1036\n",
      "     43        1.6805       0.6604        \u001b[35m1.3847\u001b[0m  0.0919\n",
      "     41        1.8863       0.2354        1.8752  0.0933\n",
      "     48        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0828\n",
      "     43        1.8885       0.2354        1.8749  0.0933\n",
      "     46        \u001b[36m1.8707\u001b[0m       0.2354        \u001b[35m1.8687\u001b[0m  0.0795\n",
      "     44      122.9401       0.2125       30.9617  0.0900\n",
      "     28      216.6236       0.1375      146.6034  0.1430\n",
      "     48        \u001b[36m1.6009\u001b[0m       0.3688        1.5728  0.0770\n",
      "     36        0.2297       0.8625        0.4561  0.1073\n",
      "     25        1.8759       0.2354        1.8739  0.1405\n",
      "     35        0.1170       0.8583        1.1497  0.1093\n",
      "     45        \u001b[36m1.5336\u001b[0m       0.3312        4.3754  0.0919\n",
      "     49        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0740\n",
      "     41        1.8854       0.2146        1.8751  0.0953\n",
      "     28        4.4369       0.2729        4.0408  0.1366\n",
      "     44        \u001b[36m1.3483\u001b[0m       \u001b[32m0.6750\u001b[0m        1.3912  0.0929\n",
      "     47        \u001b[36m1.8702\u001b[0m       0.2354        \u001b[35m1.8681\u001b[0m  0.0736\n",
      "     37        0.4581       0.9208        0.6548  0.1101\n",
      "     42        1.8863       0.2354        1.8752  0.0964\n",
      "     44        1.8885       0.2354        1.8749  0.0907\n",
      "     49        1.6558       \u001b[32m0.3875\u001b[0m        \u001b[35m1.5189\u001b[0m  0.0746\n",
      "     45       22.2594       0.2354       16.4133  0.0917\n",
      "     37        0.1716       0.8771        0.4564  0.1056\n",
      "     46        1.5820       0.4604        4.1416  0.0953\n",
      "     36        0.1652       0.8875        0.7280  0.1021\n",
      "     50        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0833\n",
      "     42        1.8854       0.2146        1.8751  0.0848\n",
      "     29      595.3817       0.1250      821.2621  0.1339\n",
      "     26        1.8759       0.2354        1.8739  0.1354\n",
      "     48        \u001b[36m1.8697\u001b[0m       0.2354        \u001b[35m1.8675\u001b[0m  0.0836\n",
      "     45        \u001b[36m1.2748\u001b[0m       \u001b[32m0.6917\u001b[0m        \u001b[35m1.2276\u001b[0m  0.0921\n",
      "     43        1.8863       0.2354        1.8752  0.0936\n",
      "     50        \u001b[36m1.5686\u001b[0m       0.3667        1.5582  0.0774\n",
      "     38        0.2635       0.9292        0.4002  0.0994\n",
      "     45        1.8885       0.2354        1.8749  0.0916\n",
      "     46       11.8976       0.2083        6.8131  0.0922\n",
      "     29        4.1899       0.2812        3.9004  0.1363\n",
      "     51        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.1099\n",
      "     47        2.2861       0.3854        \u001b[35m1.8236\u001b[0m  0.1261\n",
      "     43        1.8855       0.2146        1.8751  0.1179\n",
      "     38        0.1454       0.8604        0.5651  0.1458\n",
      "     49        \u001b[36m1.8691\u001b[0m       0.2354        \u001b[35m1.8668\u001b[0m  0.1178\n",
      "     37        0.1113       \u001b[32m0.9208\u001b[0m        0.7193  0.1424\n",
      "     51        1.6153       \u001b[32m0.3958\u001b[0m        \u001b[35m1.5041\u001b[0m  0.1123\n",
      "     44        1.8863       0.2354        1.8752  0.1151\n",
      "     46        1.8885       0.2354        1.8749  0.1223\n",
      "     46        \u001b[36m1.1109\u001b[0m       \u001b[32m0.7000\u001b[0m        \u001b[35m1.1916\u001b[0m  0.1370\n",
      "     39        0.4843       0.7833        1.0459  0.1361\n",
      "     30      299.7521       0.1500       40.6263  0.1778\n",
      "     27        1.8759       0.2354        1.8739  0.1724\n",
      "     47        8.6081       0.1208        8.1466  0.1366\n",
      "     52        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0810\n",
      "     48        1.8602       0.4229        \u001b[35m1.8068\u001b[0m  0.0915\n",
      "     44        1.8855       0.2146        1.8751  0.0887\n",
      "     50        \u001b[36m1.8683\u001b[0m       0.2354        \u001b[35m1.8659\u001b[0m  0.0845\n",
      "     52        \u001b[36m1.5124\u001b[0m       0.3417        1.5824  0.0826\n",
      "     39        0.1324       0.8271        0.7581  0.1016\n",
      "     45        1.8863       0.2354        1.8752  0.0892\n",
      "     38        \u001b[36m0.0593\u001b[0m       \u001b[32m0.9229\u001b[0m        0.8764  0.1006\n",
      "     30        3.9917       0.2917        3.7734  0.1841\n",
      "     47        \u001b[36m0.9994\u001b[0m       0.6979        \u001b[35m1.1051\u001b[0m  0.0900\n",
      "     47        1.8885       0.2354        1.8749  0.0913\n",
      "     40        0.3486       0.9229        1.3637  0.1022\n",
      "     48        6.6236       0.1042        4.5076  0.0907\n",
      "     53        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0813\n",
      "     45        1.8855       0.2146        1.8751  0.0859\n",
      "     49        1.7236       0.4396        \u001b[35m1.6276\u001b[0m  0.0907\n",
      "     51        \u001b[36m1.8675\u001b[0m       0.2354        \u001b[35m1.8648\u001b[0m  0.0816\n",
      "     53        \u001b[36m1.5078\u001b[0m       0.3917        \u001b[35m1.4900\u001b[0m  0.0820\n",
      "     31       17.4037       0.1375        2.9986  0.1344\n",
      "     28        1.8759       0.2354        1.8739  0.1309\n",
      "     46        1.8863       0.2354        1.8752  0.0881\n",
      "     40        0.1852       0.8354        0.7308  0.1006\n",
      "     48        \u001b[36m0.9612\u001b[0m       \u001b[32m0.7104\u001b[0m        \u001b[35m1.0781\u001b[0m  0.0891\n",
      "     48        1.8885       0.2354        1.8749  0.0917\n",
      "     39        \u001b[36m0.0388\u001b[0m       \u001b[32m0.9333\u001b[0m        0.8452  0.1047\n",
      "     54        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0810\n",
      "     49        5.9546       0.1500        3.9059  0.0860\n",
      "     41        0.9998       0.8063        2.4417  0.1014\n",
      "     31        3.8001       0.3063        3.5885  0.1369\n",
      "     52        \u001b[36m1.8664\u001b[0m       0.2354        \u001b[35m1.8636\u001b[0m  0.0787\n",
      "     46        1.8855       0.2146        1.8751  0.0881\n",
      "     54        1.5228       0.3354        1.5729  0.0812\n",
      "     50        \u001b[36m1.5022\u001b[0m       \u001b[32m0.4729\u001b[0m        1.7915  0.0879\n",
      "     47        1.8863       0.2354        1.8752  0.0861\n",
      "     49        \u001b[36m0.8817\u001b[0m       0.7083        \u001b[35m0.9766\u001b[0m  0.0901\n",
      "     49        1.8885       0.2354        1.8749  0.0910\n",
      "     41        0.1875       0.8417        0.8022  0.1009\n",
      "     55        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0796\n",
      "     29        1.8759       0.2354        1.8739  0.1303\n",
      "     32       45.4018       0.2146       20.4481  0.1330\n",
      "     40        \u001b[36m0.0324\u001b[0m       0.9271        0.9262  0.1019\n",
      "     50        4.3933       0.1479        \u001b[35m2.4320\u001b[0m  0.0878\n",
      "     53        \u001b[36m1.8652\u001b[0m       0.2354        \u001b[35m1.8622\u001b[0m  0.0825\n",
      "     55        1.5748       0.3563        1.5435  0.0837\n",
      "     42        2.6238       0.6333        1.8341  0.1027\n",
      "     47        1.8855       0.2146        1.8751  0.0901\n",
      "     48        1.8863       0.2354        1.8752  0.0928\n",
      "     32        3.6581       0.2896        3.5681  0.1388\n",
      "     50        \u001b[36m0.8300\u001b[0m       \u001b[32m0.7188\u001b[0m        \u001b[35m0.9649\u001b[0m  0.0903\n",
      "     50        1.8885       0.2354        1.8749  0.0932\n",
      "     56        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0875\n",
      "     42        0.2264       0.8000        1.0961  0.1059\n",
      "     41        0.0376       0.9187        0.8516  0.1044\n",
      "     54        \u001b[36m1.8638\u001b[0m       0.2354        \u001b[35m1.8605\u001b[0m  0.0898\n",
      "     56        1.5218       \u001b[32m0.4000\u001b[0m        1.4917  0.0853\n",
      "     48        1.8855       0.2146        1.8751  0.0889\n",
      "     43        1.4193       0.6875        1.1165  0.1019\n",
      "     30        1.8759       0.2354        1.8739  0.1372\n",
      "     33       26.4209       0.3125       19.8668  0.1392\n",
      "     49        1.8863       0.2354        1.8752  0.0882\n",
      "     57        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0833\n",
      "     51        1.8885       0.2354        1.8749  0.0911\n",
      "     55        \u001b[36m1.8621\u001b[0m       0.2354        \u001b[35m1.8584\u001b[0m  0.0764\n",
      "     57        1.5774       \u001b[32m0.4208\u001b[0m        \u001b[35m1.4596\u001b[0m  0.0753\n",
      "     43        0.2552       0.8229        0.8788  0.0973\n",
      "     33        3.4888       0.3250        3.4859  0.1277\n",
      "     49        1.8855       0.2146        1.8751  0.0818\n",
      "     42        0.0327       0.9292        0.8926  0.0988\n",
      "     50        1.8863       0.2354        1.8752  0.0811\n",
      "     44        1.0270       0.6792        1.0128  0.0953\n",
      "     58        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0735\n",
      "     52        1.8885       0.2354        1.8749  0.0807\n",
      "     56        \u001b[36m1.8599\u001b[0m       0.2354        \u001b[35m1.8558\u001b[0m  0.0707\n",
      "     31        1.8757       0.2354        1.8739  0.1217\n",
      "     58        1.6068       0.3271        1.6591  0.0703\n",
      "     34       14.7930       0.1479        8.1322  0.1247\n",
      "     44        0.2921       0.8562        0.6545  0.0867\n",
      "     50        1.8855       0.2146        1.8751  0.0783\n",
      "     43        \u001b[36m0.0268\u001b[0m       0.9187        0.9233  0.0892\n",
      "     51        1.8863       0.2354        1.8752  0.0740\n",
      "     59        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0685\n",
      "     45        0.7963       0.7646        0.8016  0.0869\n",
      "     34        3.3430       0.3250        3.3018  0.1190\n",
      "     57        \u001b[36m1.8573\u001b[0m       0.2354        \u001b[35m1.8526\u001b[0m  0.0680\n",
      "     53        1.8885       0.2354        1.8749  0.0757\n",
      "     59        1.5212       0.3646        1.5083  0.0672\n",
      "     51        1.8855       0.2146        1.8751  0.0771\n",
      "     45        0.1896       0.9000        0.9716  0.0901\n",
      "     32        1.8834       0.2375        1.8708  0.1167\n",
      "     52        1.8863       0.2354        1.8752  0.0755\n",
      "     60        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0681\n",
      "     35        9.7096       0.0979       11.3953  0.1179\n",
      "     44        \u001b[36m0.0248\u001b[0m       0.9187        0.8858  0.0885\n",
      "     58        \u001b[36m1.8539\u001b[0m       0.2354        \u001b[35m1.8484\u001b[0m  0.0701\n",
      "     60        \u001b[36m1.4446\u001b[0m       0.3667        1.5089  0.0679\n",
      "     46        0.5088       0.8250        0.7060  0.0871\n",
      "     54        1.8885       0.2354        1.8749  0.0772\n",
      "     52        1.8855       0.2146        1.8751  0.0747\n",
      "     35        3.2039       0.3187        3.2911  0.1210\n",
      "     61        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0685\n",
      "     53        1.8863       0.2354        1.8752  0.0762\n",
      "     46        0.1726       \u001b[32m0.9250\u001b[0m        0.8346  0.0887\n",
      "     61        1.4541       0.3417        1.6484  0.0687\n",
      "     59        \u001b[36m1.8495\u001b[0m       0.2354        \u001b[35m1.8429\u001b[0m  0.0703\n",
      "     45        \u001b[36m0.0165\u001b[0m       0.9208        0.8594  0.0873\n",
      "     55        1.8885       0.2354        1.8749  0.0759\n",
      "     47        0.4590       0.8729        0.4986  0.0905\n",
      "     33        1.8759       0.2354        1.8739  0.1203\n",
      "     36        7.9997       0.1375        5.9225  0.1188\n",
      "     53        1.8855       0.2146        1.8751  0.0754\n",
      "     62        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0672\n",
      "     54        1.8863       0.2354        1.8752  0.0743\n",
      "     62        1.5438       0.3833        1.4989  0.0648\n",
      "     60        \u001b[36m1.8437\u001b[0m       \u001b[32m0.2938\u001b[0m        \u001b[35m1.8353\u001b[0m  0.0676\n",
      "     47        0.2336       0.8938        0.6716  0.0868\n",
      "     36        3.1273       0.3271        3.2685  0.1180\n",
      "     56        1.8885       0.2354        1.8749  0.0764\n",
      "     46        \u001b[36m0.0139\u001b[0m       0.9229        0.8577  0.0869\n",
      "     48        0.2977       0.8938        0.4050  0.0865\n",
      "     54        1.8855       0.2146        1.8751  0.0738\n",
      "     63        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0686\n",
      "     63        \u001b[36m1.3967\u001b[0m       0.3875        1.5057  0.0679\n",
      "     55        1.8863       0.2354        1.8752  0.0774\n",
      "     61        \u001b[36m1.8358\u001b[0m       \u001b[32m0.3271\u001b[0m        \u001b[35m1.8249\u001b[0m  0.0696\n",
      "     34        1.8759       0.2354        1.8739  0.1166\n",
      "     37        5.2871       0.0958        4.4043  0.1178\n",
      "     48        0.2507       0.8313        1.3383  0.0876\n",
      "     57        1.8885       0.2354        1.8749  0.0766\n",
      "     47        \u001b[36m0.0127\u001b[0m       0.9208        0.8740  0.0892\n",
      "     64        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0721\n",
      "     55        1.8855       0.2146        1.8751  0.0793\n",
      "     49        0.2452       0.8958        0.4542  0.0887\n",
      "     64        1.4416       \u001b[32m0.4458\u001b[0m        \u001b[35m1.4059\u001b[0m  0.0690\n",
      "     37        3.0593       0.3292        3.2198  0.1193\n",
      "     62        \u001b[36m1.8252\u001b[0m       \u001b[32m0.3417\u001b[0m        \u001b[35m1.8105\u001b[0m  0.0702\n",
      "     56        1.8863       0.2354        1.8752  0.0786\n",
      "     49        0.5868       0.7917        0.8080  0.0923\n",
      "     65        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0703\n",
      "     35        1.8759       0.2354        1.8739  0.1184\n",
      "     38        3.1083       0.2771        2.6264  0.1204\n",
      "     48        \u001b[36m0.0125\u001b[0m       0.9229        0.8907  0.0900\n",
      "     65        1.5975       0.4021        1.5364  0.0723\n",
      "     56        1.8855       0.2146        1.8751  0.0798\n",
      "     63        \u001b[36m1.8114\u001b[0m       \u001b[32m0.3708\u001b[0m        \u001b[35m1.7914\u001b[0m  0.0717\n",
      "     50        0.2371       0.9083        0.3251  0.0894\n",
      "     57        1.8863       0.2354        1.8752  0.0768\n",
      "     66        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0662\n",
      "     38        2.9822       0.3354        3.2054  0.1209\n",
      "     66        1.4704       0.4000        1.4648  0.0657\n",
      "     50        0.2136       0.8854        0.6393  0.0858\n",
      "     57        1.8855       0.2146        1.8751  0.0720\n",
      "     64        \u001b[36m1.7938\u001b[0m       \u001b[32m0.3750\u001b[0m        \u001b[35m1.7664\u001b[0m  0.0667\n",
      "     49        \u001b[36m0.0114\u001b[0m       0.9229        0.8955  0.0862\n",
      "     51        0.1628       0.9375        0.3475  0.0861\n",
      "     36        1.8759       0.2354        1.8739  0.1173\n",
      "     39        2.2665       0.2854        2.1243  0.1176\n",
      "     67        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0691\n",
      "     67        \u001b[36m1.3579\u001b[0m       0.3812        1.5654  0.0663\n",
      "     65        \u001b[36m1.7737\u001b[0m       0.3646        \u001b[35m1.7368\u001b[0m  0.0676\n",
      "     51        0.1661       0.8917        0.4589  0.0862\n",
      "     39        2.9211       0.3396        3.1751  0.1148\n",
      "     50        \u001b[36m0.0109\u001b[0m       0.9250        0.9195  0.0837\n",
      "     52        0.1208       0.9354        0.3030  0.0817\n",
      "     68        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0629\n",
      "     68        1.4616       \u001b[32m0.4625\u001b[0m        \u001b[35m1.3339\u001b[0m  0.0621\n",
      "     66        \u001b[36m1.7520\u001b[0m       0.3688        \u001b[35m1.7029\u001b[0m  0.0622\n",
      "     37        1.8759       0.2354        1.8739  0.1106\n",
      "     40        2.4683       0.3458        3.6104  0.1105\n",
      "     52        0.0808       0.9083        0.4061  0.0795\n",
      "     51        \u001b[36m0.0103\u001b[0m       0.9250        0.9406  0.0780\n",
      "     69        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0631\n",
      "     69        \u001b[36m1.3190\u001b[0m       0.4229        1.4485  0.0594\n",
      "     53        0.0864       0.9354        0.2951  0.0782\n",
      "     67        \u001b[36m1.7440\u001b[0m       0.3458        \u001b[35m1.6758\u001b[0m  0.0616\n",
      "     40        2.8606       0.3333        3.1759  0.1090\n",
      "     70        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0621\n",
      "     53        \u001b[36m0.0551\u001b[0m       0.9187        \u001b[35m0.3777\u001b[0m  0.0784\n",
      "     70        1.3860       0.3667        1.6254  0.0606\n",
      "     38        1.8759       0.2354        1.8739  0.1081\n",
      "     52        \u001b[36m0.0090\u001b[0m       0.9271        0.9438  0.0791\n",
      "     41        3.3079       0.3521        2.6052  0.1092\n",
      "     68        \u001b[36m1.7151\u001b[0m       \u001b[32m0.3833\u001b[0m        \u001b[35m1.6429\u001b[0m  0.0614\n",
      "     54        0.0698       0.9479        0.2498  0.0783\n",
      "     71        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0613\n",
      "     71        1.4318       0.4542        1.3758  0.0602\n",
      "     41        2.8059       0.3312        3.1221  0.1100\n",
      "     69        1.7214       0.3542        \u001b[35m1.6187\u001b[0m  0.0608\n",
      "     54        \u001b[36m0.0346\u001b[0m       0.9187        \u001b[35m0.3723\u001b[0m  0.0781\n",
      "     53        \u001b[36m0.0083\u001b[0m       0.9271        0.9653  0.0792\n",
      "     55        0.0506       0.9396        0.2799  0.0796\n",
      "     39        1.8759       0.2354        1.8739  0.1085\n",
      "     72        1.8741       0.2354        1.8746  0.0622\n",
      "     42        2.4700       0.2188        2.5791  0.1095\n",
      "     72        \u001b[36m1.2588\u001b[0m       \u001b[32m0.4771\u001b[0m        \u001b[35m1.3107\u001b[0m  0.0606\n",
      "     70        \u001b[36m1.6620\u001b[0m       0.3479        1.6199  0.0616\n",
      "     55        \u001b[36m0.0258\u001b[0m       0.9125        0.3873  0.0773\n",
      "     54        \u001b[36m0.0076\u001b[0m       0.9250        0.9875  0.0791\n",
      "     42        2.7535       0.3187        3.1138  0.1100\n",
      "     73        1.8741       0.2354        1.8746  0.0617\n",
      "     56        0.0459       0.9375        0.2864  0.0794\n",
      "     73        1.3073       0.3750        1.6157  0.0611\n",
      "     71        1.6640       \u001b[32m0.3896\u001b[0m        \u001b[35m1.6116\u001b[0m  0.0616\n",
      "     40        1.8759       0.2354        1.8739  0.1079\n",
      "     56        \u001b[36m0.0230\u001b[0m       0.9146        0.3873  0.0786\n",
      "     43        2.1852       0.2333        1.8235  0.1105\n",
      "     74        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0603\n",
      "     74        1.3879       \u001b[32m0.4896\u001b[0m        \u001b[35m1.3052\u001b[0m  0.0604\n",
      "     55        \u001b[36m0.0071\u001b[0m       0.9250        1.0331  0.0785\n",
      "     72        1.7098       0.3667        \u001b[35m1.5819\u001b[0m  0.0609\n",
      "     57        0.0422       0.9396        0.3253  0.0780\n",
      "     43        2.7225       0.3042        3.1144  0.1084\n",
      "     75        1.8741       0.2354        1.8746  0.0613\n",
      "     75        \u001b[36m1.2398\u001b[0m       \u001b[32m0.4917\u001b[0m        1.3165  0.0614\n",
      "     57        \u001b[36m0.0198\u001b[0m       0.9167        0.4012  0.0789\n",
      "     73        \u001b[36m1.5887\u001b[0m       0.3667        \u001b[35m1.5790\u001b[0m  0.0615\n",
      "     56        \u001b[36m0.0067\u001b[0m       0.9250        1.0485  0.0787\n",
      "     41        1.8759       0.2354        1.8739  0.1098\n",
      "     58        0.0501       0.9479        0.2858  0.0780\n",
      "     44        1.9313       0.2708        2.1370  0.1081\n",
      "     76        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0605\n",
      "     76        \u001b[36m1.2155\u001b[0m       \u001b[32m0.5229\u001b[0m        \u001b[35m1.2156\u001b[0m  0.0595\n",
      "     74        \u001b[36m1.5741\u001b[0m       0.3625        1.5948  0.0612\n",
      "     58        \u001b[36m0.0170\u001b[0m       0.9167        0.4195  0.0785\n",
      "     44        2.6902       0.3000        3.1196  0.1085\n",
      "     57        \u001b[36m0.0063\u001b[0m       0.9250        1.0578  0.0808\n",
      "     59        0.0587       0.9542        0.6227  0.0791\n",
      "     77        1.2986       0.5167        \u001b[35m1.2154\u001b[0m  0.0616\n",
      "     77        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0624\n",
      "     42        1.8759       0.2354        1.8739  0.1085\n",
      "     75        \u001b[36m1.5666\u001b[0m       0.3708        \u001b[35m1.5627\u001b[0m  0.0602\n",
      "     45        2.2435       0.2667        2.3676  0.1096\n",
      "     59        \u001b[36m0.0146\u001b[0m       0.9187        0.4305  0.0792\n",
      "     58        \u001b[36m0.0061\u001b[0m       0.9250        1.0818  0.0799\n",
      "     78        1.2198       0.4625        1.3846  0.0610\n",
      "     78        1.8741       0.2354        1.8746  0.0614\n",
      "     60        0.0630       0.9521        0.8342  0.0807\n",
      "     45        2.6578       0.2938        3.1002  0.1099\n",
      "     76        \u001b[36m1.5517\u001b[0m       0.3750        1.5702  0.0616\n",
      "     43        1.8759       0.2354        1.8739  0.1164\n",
      "     79        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0636\n",
      "     60        \u001b[36m0.0128\u001b[0m       0.9208        0.4427  0.0884\n",
      "     79        1.3171       \u001b[32m0.5292\u001b[0m        \u001b[35m1.1869\u001b[0m  0.0653\n",
      "     46        2.2549       0.2708        2.3768  0.1162\n",
      "     59        \u001b[36m0.0060\u001b[0m       0.9250        1.0961  0.0875\n",
      "     77        \u001b[36m1.5276\u001b[0m       0.3729        1.5708  0.0687\n",
      "     61        0.0876       0.9479        0.4693  0.0874\n",
      "     80        1.8741       0.2354        1.8746  0.0610\n",
      "     80        \u001b[36m1.1550\u001b[0m       0.5250        \u001b[35m1.1585\u001b[0m  0.0612\n",
      "     46        2.6181       0.2875        3.0678  0.1163\n",
      "     61        \u001b[36m0.0114\u001b[0m       0.9208        0.4581  0.0789\n",
      "     78        \u001b[36m1.5080\u001b[0m       0.3771        1.5704  0.0604\n",
      "     60        \u001b[36m0.0059\u001b[0m       0.9250        1.0962  0.0794\n",
      "     44        1.8759       0.2354        1.8739  0.1089\n",
      "     62        0.0454       0.9417        0.4265  0.0776\n",
      "     47        2.0861       0.2687        2.1381  0.1088\n",
      "     81        \u001b[36m1.1410\u001b[0m       \u001b[32m0.5396\u001b[0m        \u001b[35m1.1473\u001b[0m  0.0598\n",
      "     81        1.8741       0.2354        1.8746  0.0619\n",
      "     79        \u001b[36m1.4893\u001b[0m       0.3812        \u001b[35m1.5528\u001b[0m  0.0611\n",
      "     62        \u001b[36m0.0102\u001b[0m       0.9167        0.4691  0.0793\n",
      "     47        2.5824       0.2792        3.0463  0.1083\n",
      "     61        \u001b[36m0.0055\u001b[0m       0.9250        1.1301  0.0781\n",
      "     82        1.1990       0.5292        1.1866  0.0587\n",
      "     82        1.8741       0.2354        1.8746  0.0606\n",
      "     63        0.0405       0.9437        0.3371  0.0781\n",
      "     80        1.5270       \u001b[32m0.4229\u001b[0m        \u001b[35m1.4993\u001b[0m  0.0610\n",
      "     45        1.8759       0.2354        1.8739  0.1086\n",
      "     48        2.2141       0.2396        2.2547  0.1098\n",
      "     63        \u001b[36m0.0097\u001b[0m       0.9167        0.4741  0.0792\n",
      "     83        1.1765       0.5396        \u001b[35m1.1177\u001b[0m  0.0599\n",
      "     83        1.8741       0.2354        1.8746  0.0607\n",
      "     62        \u001b[36m0.0052\u001b[0m       0.9271        1.1468  0.0789\n",
      "     64        0.0433       0.9563        0.3315  0.0783\n",
      "     81        1.5564       0.4104        \u001b[35m1.3970\u001b[0m  0.0606\n",
      "     48        2.5477       0.2687        3.0155  0.1092\n",
      "     84        1.1495       0.5229        1.1953  0.0605\n",
      "     84        \u001b[36m1.8741\u001b[0m       0.2354        1.8746  0.0624\n",
      "     64        \u001b[36m0.0087\u001b[0m       0.9187        0.5074  0.0804\n",
      "     46        1.8759       0.2354        1.8739  0.1091\n",
      "     82        1.4987       0.4167        \u001b[35m1.3783\u001b[0m  0.0623\n",
      "     49        2.0301       0.2875        1.9938  0.1102\n",
      "     63        \u001b[36m0.0049\u001b[0m       0.9250        1.2103  0.0802\n",
      "     65        0.0517       0.9479        0.3679  0.0789\n",
      "     85        1.1547       \u001b[32m0.5479\u001b[0m        1.1263  0.0609\n",
      "     85        1.8741       0.2354        1.8746  0.0612\n",
      "     49        2.5159       0.2729        3.0209  0.1094\n",
      "     83        \u001b[36m1.4649\u001b[0m       \u001b[32m0.4250\u001b[0m        \u001b[35m1.3538\u001b[0m  0.0608\n",
      "     65        \u001b[36m0.0078\u001b[0m       0.9187        0.5105  0.0788\n",
      "     64        0.0054       0.9250        1.2038  0.0799\n",
      "     66        \u001b[36m0.0354\u001b[0m       0.9583        0.3653  0.0798\n",
      "     86        \u001b[36m1.1065\u001b[0m       0.5479        \u001b[35m1.0678\u001b[0m  0.0618\n",
      "     47        1.8759       0.2354        1.8739  0.1111\n",
      "     86        1.8741       0.2354        1.8746  0.0627\n",
      "     50        2.1227       0.3708        2.0229  0.1106\n",
      "     84        \u001b[36m1.4176\u001b[0m       \u001b[32m0.4375\u001b[0m        \u001b[35m1.3230\u001b[0m  0.0633\n",
      "     66        \u001b[36m0.0072\u001b[0m       0.9229        0.5723  0.0803\n",
      "     87        \u001b[36m1.0680\u001b[0m       0.5479        1.0748  0.0606\n",
      "     65        \u001b[36m0.0039\u001b[0m       0.9250        1.2105  0.0806\n",
      "     87        1.8741       0.2354        1.8746  0.0622\n",
      "     50        2.4889       0.2750        2.9648  0.1108\n",
      "     67        \u001b[36m0.0324\u001b[0m       0.9500        0.3414  0.0792\n",
      "     85        \u001b[36m1.3746\u001b[0m       \u001b[32m0.4542\u001b[0m        \u001b[35m1.2972\u001b[0m  0.0616\n",
      "     48        1.8759       0.2354        1.8739  0.1093\n",
      "     88        \u001b[36m1.0340\u001b[0m       \u001b[32m0.5563\u001b[0m        \u001b[35m1.0524\u001b[0m  0.0612\n",
      "     51        2.1068       0.3438        2.2941  0.1112\n",
      "     88        1.8741       0.2354        1.8746  0.0614\n",
      "     67        \u001b[36m0.0065\u001b[0m       0.9187        0.6242  0.0805\n",
      "     66        \u001b[36m0.0035\u001b[0m       0.9229        1.2760  0.0789\n",
      "     86        \u001b[36m1.3076\u001b[0m       \u001b[32m0.5146\u001b[0m        \u001b[35m1.2805\u001b[0m  0.0615\n",
      "     68        \u001b[36m0.0182\u001b[0m       0.9646        0.3120  0.0794\n",
      "     51        2.4408       0.2729        2.8987  0.1098\n",
      "     89        \u001b[36m1.0177\u001b[0m       \u001b[32m0.5625\u001b[0m        \u001b[35m1.0209\u001b[0m  0.0603\n",
      "     89        1.8741       0.2354        1.8746  0.0622\n",
      "     68        \u001b[36m0.0058\u001b[0m       0.9229        0.6914  0.0790\n",
      "     87        \u001b[36m1.2960\u001b[0m       \u001b[32m0.5896\u001b[0m        \u001b[35m1.2502\u001b[0m  0.0629\n",
      "     49        1.8759       0.2354        1.8739  0.1083\n",
      "     67        \u001b[36m0.0033\u001b[0m       0.9250        1.2898  0.0793\n",
      "     69        \u001b[36m0.0154\u001b[0m       0.9604        0.3395  0.0786\n",
      "     52        2.0043       0.3563        1.8766  0.1108\n",
      "     90        \u001b[36m1.0052\u001b[0m       0.5437        1.1866  0.0604\n",
      "     90        1.8741       0.2354        1.8746  0.0620\n",
      "     88        1.7179       0.3688        1.6654  0.0611\n",
      "     52        2.4062       0.2771        2.9067  0.1106\n",
      "     69        \u001b[36m0.0054\u001b[0m       0.9229        0.5353  0.0800\n",
      "     68        \u001b[36m0.0031\u001b[0m       0.9229        1.3460  0.0797\n",
      "     70        \u001b[36m0.0138\u001b[0m       0.9583        0.3524  0.0792\n",
      "     50        1.8759       0.2354        1.8739  0.1108\n",
      "     89        \u001b[36m1.2849\u001b[0m       0.5813        \u001b[35m1.1721\u001b[0m  0.0629\n",
      "     53        1.9857       0.3583        1.9194  0.1121\n",
      "     70        \u001b[36m0.0052\u001b[0m       0.9229        0.5453  0.0823\n",
      "     69        0.0043       0.9250        1.2953  0.0808\n",
      "     71        \u001b[36m0.0125\u001b[0m       0.9583        0.3695  0.0781\n",
      "     90        \u001b[36m1.1918\u001b[0m       0.5896        1.1844  0.0605\n",
      "     53        2.3949       0.2896        2.8134  0.1123\n",
      "     51        1.8759       0.2354        1.8739  0.1068\n",
      "     71        \u001b[36m0.0048\u001b[0m       0.9229        0.5624  0.0783\n",
      "     54        2.0299       0.3438        2.1911  0.1082\n",
      "     70        0.0067       0.9187        1.5204  0.0779\n",
      "     72        \u001b[36m0.0116\u001b[0m       0.9542        0.4078  0.0773\n",
      "     54        2.3346       0.2729        2.8460  0.1080\n",
      "     72        \u001b[36m0.0045\u001b[0m       0.9229        0.6090  0.0788\n",
      "     71        0.0252       0.9187        1.2469  0.0855\n",
      "     52        1.8759       0.2354        1.8739  0.1157\n",
      "     73        \u001b[36m0.0106\u001b[0m       0.9542        0.4350  0.0842\n",
      "     55        2.0593       0.3438        2.0528  0.1120\n",
      "     73        \u001b[36m0.0042\u001b[0m       0.9187        0.6214  0.0843\n",
      "     55        2.3572       0.3000        2.7398  0.1118\n",
      "     72        0.0121       0.9187        1.2388  0.0768\n",
      "     74        \u001b[36m0.0098\u001b[0m       0.9563        0.4430  0.0772\n",
      "     53        1.8759       0.2354        1.8739  0.1059\n",
      "     56        2.0183       0.3583        2.3741  0.1056\n",
      "     74        \u001b[36m0.0039\u001b[0m       0.9187        0.6625  0.0769\n",
      "     73        0.0647       0.9187        1.4937  0.0770\n",
      "     75        \u001b[36m0.0093\u001b[0m       0.9542        0.4601  0.0757\n",
      "     56        2.2960       0.2729        2.7799  0.1057\n",
      "     54        1.8759       0.2354        1.8739  0.1056\n",
      "     75        \u001b[36m0.0037\u001b[0m       0.9187        0.7170  0.0742\n",
      "     74        0.0311       0.9229        1.1061  0.0753\n",
      "     57        2.0363       0.2938        2.0463  0.1063\n",
      "     76        \u001b[36m0.0088\u001b[0m       0.9563        0.4733  0.0766\n",
      "     57        2.3178       0.2979        2.6659  0.1044\n",
      "     76        \u001b[36m0.0035\u001b[0m       0.9208        0.5680  0.0768\n",
      "     75        0.0386       0.9313        1.1958  0.0763\n",
      "     77        \u001b[36m0.0083\u001b[0m       0.9521        0.4889  0.0769\n",
      "     55        1.8759       0.2354        1.8739  0.1038\n",
      "     58        1.9799       0.2750        2.1778  0.1030\n",
      "     77        \u001b[36m0.0034\u001b[0m       0.9208        0.5909  0.0745\n",
      "     76        0.0157       0.9333        1.3127  0.0758\n",
      "     58        2.2460       0.2750        2.6852  0.1044\n",
      "     78        \u001b[36m0.0079\u001b[0m       0.9521        0.4916  0.0744\n",
      "     56        1.8759       0.2354        1.8739  0.1038\n",
      "     59        1.9913       0.2896        2.0158  0.1024\n",
      "     78        \u001b[36m0.0032\u001b[0m       0.9187        0.6205  0.0766\n",
      "     77        0.0185       \u001b[32m0.9375\u001b[0m        1.2873  0.0767\n",
      "     79        \u001b[36m0.0075\u001b[0m       0.9521        0.5106  0.0771\n",
      "     59        2.2544       0.3021        2.5957  0.1051\n",
      "     79        \u001b[36m0.0030\u001b[0m       0.9187        0.6663  0.0763\n",
      "     57        1.8759       0.2354        1.8739  0.1060\n",
      "     78        0.1093       0.9271        1.3968  0.0771\n",
      "     80        \u001b[36m0.0071\u001b[0m       0.9521        0.5226  0.0756\n",
      "     60        1.9441       0.2833        2.0213  0.1059\n",
      "     60        2.1921       0.2854        2.6253  0.1048\n",
      "     80        \u001b[36m0.0029\u001b[0m       0.9187        0.7070  0.0764\n",
      "     81        \u001b[36m0.0065\u001b[0m       0.9500        0.5447  0.0750\n",
      "     79        0.0641       0.9083        1.4313  0.0753\n",
      "     58        1.8759       0.2354        1.8739  0.1037\n",
      "     61        1.9584       0.2833        2.0571  0.1046\n",
      "     81        0.0032       0.9167        0.5156  0.0750\n",
      "     82        0.0069       0.9500        0.5454  0.0744\n",
      "     80        0.1635       0.8833        1.7419  0.0753\n",
      "     61        2.1972       0.3042        2.5480  0.1038\n",
      "     59        1.8759       0.2354        1.8739  0.1030\n",
      "     82        0.0032       0.9187        0.5150  0.0753\n",
      "     62        1.9715       0.2854        2.0553  0.1048\n",
      "     83        \u001b[36m0.0062\u001b[0m       0.9521        0.5724  0.0749\n",
      "     81        0.2391       0.9042        1.6705  0.0768\n",
      "     62        2.1556       0.2833        2.5963  0.1042\n",
      "     83        \u001b[36m0.0028\u001b[0m       0.9208        0.5397  0.0752\n",
      "     60        1.8758       0.2354        1.8739  0.1040\n",
      "     84        \u001b[36m0.0062\u001b[0m       0.9500        0.5745  0.0750\n",
      "     82        0.1867       0.9104        1.0345  0.0760\n",
      "     63        1.9546       0.2833        2.0360  0.1035\n",
      "     84        \u001b[36m0.0025\u001b[0m       0.9208        0.5678  0.0768\n",
      "     63        2.1677       0.3104        2.5322  0.1054\n",
      "     85        \u001b[36m0.0061\u001b[0m       0.9500        0.6134  0.0753\n",
      "     83        0.9108       0.7479        1.8871  0.0766\n",
      "     61        1.8758       0.2354        1.8739  0.1041\n",
      "     64        1.9331       0.2917        2.0010  0.1052\n",
      "     85        \u001b[36m0.0023\u001b[0m       0.9208        0.6763  0.0812\n",
      "     86        \u001b[36m0.0058\u001b[0m       0.9521        0.5925  0.0778\n",
      "     84        4.1765       0.6271        5.8411  0.0829\n",
      "     64        2.1283       0.2917        2.5563  0.1131\n",
      "     62        1.8758       0.2354        1.8739  0.1124\n",
      "     86        \u001b[36m0.0021\u001b[0m       0.9187        0.5988  0.0807\n",
      "     87        \u001b[36m0.0057\u001b[0m       0.9521        0.6515  0.0794\n",
      "     85        2.5586       0.6917        1.2260  0.0802\n",
      "     87        \u001b[36m0.0021\u001b[0m       0.9187        0.6110  0.0785\n",
      "     88        0.0058       0.9542        0.6303  0.0775\n",
      "     63        1.8758       0.2354        1.8739  0.1075\n",
      "     86        1.5143       0.6687        1.8300  0.0783\n",
      "     88        \u001b[36m0.0018\u001b[0m       0.9208        0.6411  0.0786\n",
      "     89        \u001b[36m0.0056\u001b[0m       0.9500        0.6543  0.0764\n",
      "     87        1.1794       0.6667        1.1947  0.0776\n",
      "     64        1.8758       0.2354        1.8739  0.1049\n",
      "     89        \u001b[36m0.0017\u001b[0m       0.9167        0.6856  0.0802\n",
      "     90        \u001b[36m0.0051\u001b[0m       0.9521        0.6281  0.0803\n",
      "     88        3.0061       0.3458        5.0146  0.0806\n",
      "     90        \u001b[36m0.0015\u001b[0m       0.9167        0.6724  0.0804\n",
      "     91        \u001b[36m0.0049\u001b[0m       0.9521        0.6805  0.0793\n",
      "     89        1.6973       0.6292        1.8882  0.0787\n",
      "     91        \u001b[36m0.0014\u001b[0m       0.9167        0.7046  0.0788\n",
      "     92        0.0053       0.9521        0.6649  0.0800\n",
      "     90        0.8858       0.7250        3.0098  0.0792\n",
      "     92        \u001b[36m0.0013\u001b[0m       0.9167        0.7310  0.0768\n",
      "     93        \u001b[36m0.0049\u001b[0m       0.9500        0.6702  0.0779\n",
      "     91        0.5242       0.7229        0.8981  0.0767\n",
      "     93        \u001b[36m0.0012\u001b[0m       0.9146        0.7559  0.0753\n",
      "     94        \u001b[36m0.0044\u001b[0m       0.9500        0.6665  0.0769\n",
      "     92        0.4429       0.7625        1.1142  0.0765\n",
      "     94        \u001b[36m0.0011\u001b[0m       0.9146        0.7737  0.0762\n",
      "     95        \u001b[36m0.0041\u001b[0m       0.9479        0.6530  0.0750\n",
      "     93        0.4093       0.7771        1.5674  0.0759\n",
      "     95        \u001b[36m0.0010\u001b[0m       0.9146        0.8354  0.0765\n",
      "     96        \u001b[36m0.0039\u001b[0m       0.9437        0.6655  0.0766\n",
      "     94        0.3454       0.8042        2.6091  0.0731\n",
      "     97        \u001b[36m0.0035\u001b[0m       0.9458        0.6502  0.0715\n",
      "     96        0.0029       0.9167        0.5473  0.0752\n",
      "     95        0.3124       0.7979        1.6402  0.0720\n",
      "     98        0.0040       0.9500        0.7721  0.0727\n",
      "     97        0.0026       0.9208        0.5604  0.0745\n",
      "     96        0.2889       0.8063        2.1347  0.0726\n",
      "     99        0.1126       0.9250        0.4641  0.0784\n",
      "     98        0.0017       0.9250        0.5666  0.0772\n",
      "     97        0.2663       0.8104        1.7683  0.0774\n",
      "     99        0.0014       0.9250        0.5851  0.0757    100        0.2326       0.9208        1.6302  0.0761\n",
      "\n",
      "     98        0.2507       0.8083        1.4635  0.0777\n",
      "    100        0.0012       0.9250        0.6038  0.0750\n",
      "    101        0.4029       0.9229        0.4974  0.0761\n",
      "     99        0.2428       0.8187        1.4137  0.0747\n",
      "    102        0.4407       0.8187        1.2704  0.0743\n",
      "    101        0.0010       0.9250        0.6367  0.0755\n",
      "    100        0.2279       0.8167        1.2110  0.0765\n",
      "    103        1.9639       0.6542        3.3395  0.0734\n",
      "    102        \u001b[36m0.0009\u001b[0m       0.9208        0.6767  0.0751\n",
      "    101        0.2192       0.8396        1.3329  0.0738\n",
      "    104        8.7387       0.4604       19.3367  0.0757\n",
      "    103        \u001b[36m0.0009\u001b[0m       0.9208        0.7679  0.0738\n",
      "    102        0.2170       0.8354        1.2226  0.0760\n",
      "    105       14.1657       0.3208        4.4657  0.0768\n",
      "    104        \u001b[36m0.0008\u001b[0m       0.9208        0.8174  0.0767\n",
      "    103        0.2085       0.8396        1.3870  0.0763\n",
      "    106        4.1166       0.4188        2.5424  0.0724\n",
      "    105        \u001b[36m0.0008\u001b[0m       0.9208        0.9383  0.0741\n",
      "    104        0.2046       0.8292        1.3499  0.0742\n",
      "    107        1.9239       0.5062        1.5535  0.0761\n",
      "    106        \u001b[36m0.0008\u001b[0m       0.9187        0.8896  0.0769\n",
      "    105        0.2052       0.8229        1.4023  0.0759\n",
      "    108        1.2498       0.6021        1.0877  0.0757\n",
      "    107        \u001b[36m0.0007\u001b[0m       0.9187        0.9583  0.0751\n",
      "    106        0.1960       0.8208        1.4137  0.0755\n",
      "    109        1.0147       0.6937        0.9520  0.0756\n",
      "    108        \u001b[36m0.0007\u001b[0m       0.9187        0.9966  0.0743\n",
      "    107        0.1943       0.8271        1.4618  0.0762\n",
      "    109        \u001b[36m0.0007\u001b[0m       0.9187        1.0979  0.0722\n",
      "    110        0.9766       0.7167        0.9456  0.0725\n",
      "    108        0.2002       0.8229        1.4760  0.0747\n",
      "    111        0.8770       0.6604        0.9397  0.0705\n",
      "    110        0.0007       0.9208        0.9305  0.0711\n",
      "    109        0.2033       0.8187        1.5045  0.0711\n",
      "    112        0.8349       0.7375        0.8024  0.0768\n",
      "    111        0.0477       0.9125        0.4926  0.0790\n",
      "    110        0.2062       0.8187        1.5062  0.0779\n",
      "    113        0.7347       0.7312        0.8017  0.0746\n",
      "    112        0.0449       0.9187        0.4963  0.0749\n",
      "    111        0.2093       0.8187        1.4865  0.0741\n",
      "    114        0.6554       0.7500        0.7907  0.0762\n",
      "    113        0.0438       0.9083        0.6163  0.0727\n",
      "    112        0.2099       0.8271        1.5019  0.0732\n",
      "    115        0.5998       0.7729        0.7961  0.0755\n",
      "    114        0.0401       0.9250        0.5500  0.0755\n",
      "    113        0.2207       0.8167        1.4735  0.0753\n",
      "    115        0.0330       0.9000        0.8382  0.0733\n",
      "    116        0.6119       0.7583        0.7500  0.0750\n",
      "    114        0.2280       0.8229        1.3900  0.0747\n",
      "    116        0.0520       0.8750        1.0875  0.0741\n",
      "    117        0.6908       0.7688        0.7521  0.0737\n",
      "    115        0.2699       0.8646        1.2520  0.0719\n",
      "    118        0.6118       0.7771        0.6839  0.0720\n",
      "    117        0.0738       0.8500        0.9513  0.0730\n",
      "    116        0.3061       0.8396        1.2397  0.0750\n",
      "    118        0.1870       0.8771        0.7554  0.0703\n",
      "    119        0.5247       0.7833        0.6558  0.0710\n",
      "    117        0.2524       0.8292        1.3508  0.0721\n",
      "    119        0.2745       0.8833        0.8238  0.0699\n",
      "    120        0.4787       0.7896        0.6394  0.0719\n",
      "    118        0.1920       0.8771        1.1386  0.0701\n",
      "    120        0.2530       0.8854        0.8147  0.0739\n",
      "    121        0.4575       0.8083        0.6033  0.0745\n",
      "    119        0.1233       0.8792        1.1030  0.0742\n",
      "    121        0.1140       0.8583        1.2843  0.0727\n",
      "    122        0.4246       0.8083        0.6009  0.0718\n",
      "    120        0.2226       0.8208        1.0654  0.0722\n",
      "    122        0.1354       0.8875        0.9806  0.0711\n",
      "    123        0.4548       0.7937        0.6720  0.0710\n",
      "    121        0.2364       0.8500        1.2406  0.0699\n",
      "    123        0.2110       0.8167        1.4287  0.0723\n",
      "    124        0.6584       0.6896        1.0371  0.0727\n",
      "    122        0.1787       0.8625        1.3677  0.0752\n",
      "    124        0.3413       0.8958        0.6211  0.0742\n",
      "    125        1.0350       0.7688        0.7851  0.0805\n",
      "    123        0.1413       0.8583        1.0860  0.0809\n",
      "    125        1.8532       0.6250        3.1538  0.0779\n",
      "    126        0.5760       0.7937        0.6921  0.0734\n",
      "    124        0.1182       0.8833        0.9739  0.0712\n",
      "    126        2.4332       0.7354        3.0824  0.0723\n",
      "    127        0.4631       0.7812        0.8135  0.0739\n",
      "    125        0.0934       0.8771        1.1199  0.0717\n",
      "    127        1.7038       0.7167        1.3415  0.0733\n",
      "    128        0.4263       0.8083        0.7393  0.0742\n",
      "    126        0.0843       0.8812        1.0888  0.0731\n",
      "    128        1.1698       0.5813        1.8241  0.0727\n",
      "    129        0.3833       0.8167        0.6133  0.0743\n",
      "    127        0.0767       0.8792        1.0643  0.0734\n",
      "    129        0.7964       0.8271        1.1433  0.0735\n",
      "    130        0.3401       0.8333        0.6713  0.0746\n",
      "    128        0.0698       0.8812        1.0440  0.0740\n",
      "    130        0.4802       0.8396        0.8279  0.0723\n",
      "    131        0.3219       0.8146        0.5864  0.0721\n",
      "    129        0.0628       0.8854        1.0175  0.0723\n",
      "    131        0.2875       0.8500        0.7268  0.0727\n",
      "    132        0.3312       0.8313        0.5400  0.0720\n",
      "    130        0.0579       0.8896        0.9980  0.0718\n",
      "    132        0.2199       0.8875        0.6829  0.0731\n",
      "    133        0.2925       0.8417        0.5059  0.0706\n",
      "    131        0.0521       0.8896        0.9908  0.0704\n",
      "    133        0.1834       0.8875        0.6401  0.0712\n",
      "    134       21.6829       0.3625       24.3265  0.0704\n",
      "    132        0.0456       0.8875        0.9822  0.0708\n",
      "    134        0.1363       0.8833        0.6611  0.0734\n",
      "    135       34.3952       0.4000       28.1471  0.0727\n",
      "    133        0.0418       0.8917        0.9723  0.0713\n",
      "    135        0.1126       0.9042        0.6587  0.0710\n",
      "    136       29.7524       0.2771       16.9012  0.0703\n",
      "    134        0.0386       0.8938        0.9647  0.0738\n",
      "    136        0.0941       0.9021        0.6974  0.0724\n",
      "    137       19.6983       0.3708        8.5835  0.0727\n",
      "    135        0.0365       0.8958        0.9680  0.0710\n",
      "    137        0.0913       0.9000        0.7148  0.0744\n",
      "    138        7.0056       0.5792        4.0577  0.0736\n",
      "    136        0.0344       0.8938        0.9640  0.0744\n",
      "    138        0.0991       0.8958        0.7732  0.0757\n",
      "    139        2.7809       0.5000        2.2421  0.0760\n",
      "    137        0.0327       0.8979        0.9651  0.0736\n",
      "    139        0.0787       0.9000        0.7956  0.0730\n",
      "    140        1.6502       0.6813        1.3337  0.0737\n",
      "    138        0.0307       0.8958        0.9546  0.0743\n",
      "    140        0.0872       0.9083        0.8360  0.0743\n",
      "    141        1.3137       0.6750        1.1289  0.0742\n",
      "    139        0.0290       0.8958        0.9687  0.0740\n",
      "    141        0.0740       0.9042        0.9002  0.0714\n",
      "    142        1.0633       0.7125        0.9792  0.0735\n",
      "    140        0.0253       0.8938        0.9835  0.0741\n",
      "    142        0.0573       0.8938        0.9837  0.0748\n",
      "    143        0.9901       0.6979        0.9445  0.0755\n",
      "    141        0.0227       0.9000        0.9793  0.0758\n",
      "    143        0.0565       0.9021        0.8405  0.0745\n",
      "    144        0.9277       0.6937        0.9424  0.0756\n",
      "    142        0.0212       0.9000        0.9853  0.0726\n",
      "    144        0.0439       0.9083        0.7868  0.0774\n",
      "    145        0.8733       0.7125        0.8944  0.0741\n",
      "    143        0.0200       0.8958        0.9902  0.0738\n",
      "    145        0.0387       0.9083        0.8143  0.0700\n",
      "    146        0.8163       0.7104        0.8544  0.0702\n",
      "    144        0.0183       0.9021        0.9742  0.0699\n",
      "    146        0.0346       0.9042        0.8178  0.0704\n",
      "    147        0.7756       0.7208        0.8185  0.0718\n",
      "    145        0.0175       0.9021        0.9699  0.0717\n",
      "    147        0.0308       0.9042        0.8245  0.0720\n",
      "    146        0.0161       0.8958        0.9780  0.0739\n",
      "    147        0.0148       0.9021        1.0018  0.0703\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.8009\u001b[0m       \u001b[32m0.4868\u001b[0m        \u001b[35m1.3409\u001b[0m  0.1118\n",
      "      2        \u001b[36m1.1139\u001b[0m       \u001b[32m0.6759\u001b[0m        \u001b[35m0.9235\u001b[0m  0.1040\n",
      "      3        \u001b[36m0.6598\u001b[0m       \u001b[32m0.8178\u001b[0m        \u001b[35m0.5646\u001b[0m  0.1010\n",
      "      4        \u001b[36m0.3630\u001b[0m       \u001b[32m0.8679\u001b[0m        \u001b[35m0.4762\u001b[0m  0.1016\n",
      "      5        \u001b[36m0.2564\u001b[0m       \u001b[32m0.8957\u001b[0m        \u001b[35m0.4559\u001b[0m  0.1036\n",
      "      6        0.2607       \u001b[32m0.9124\u001b[0m        \u001b[35m0.3834\u001b[0m  0.1040\n",
      "      7        \u001b[36m0.1988\u001b[0m       \u001b[32m0.9138\u001b[0m        0.4134  0.1025\n",
      "      8        0.2705       0.9026        0.5162  0.0996\n",
      "      9        0.4506       0.9013        0.5249  0.1011\n",
      "     10        0.2562       0.9096        0.5022  0.1008\n",
      "     11        0.3443       \u001b[32m0.9277\u001b[0m        \u001b[35m0.3641\u001b[0m  0.1049\n",
      "     12        0.2987       0.8261        0.7414  0.1050\n",
      "     13        0.3704       0.8581        0.6812  0.1052\n",
      "     14        1.2939       0.6495        6.3925  0.1026\n",
      "     15        3.5347       0.2643        7.1723  0.1020\n",
      "     16        3.0333       0.7371        0.8714  0.1034\n",
      "     17        1.1639       0.7469        0.7975  0.1041\n",
      "     18        0.5614       0.8581        0.4925  0.1011\n",
      "     19        0.3468       0.8832        0.4369  0.1006\n",
      "     20        0.2448       0.8901        0.3877  0.1006\n",
      "     21        0.2030       0.8943        0.3683  0.1018\n",
      "     22        \u001b[36m0.1745\u001b[0m       0.9026        \u001b[35m0.3611\u001b[0m  0.1024\n",
      "     23        \u001b[36m0.1573\u001b[0m       0.9040        \u001b[35m0.3389\u001b[0m  0.1037\n",
      "     24        \u001b[36m0.1329\u001b[0m       0.9152        \u001b[35m0.3284\u001b[0m  0.1007\n",
      "     25        \u001b[36m0.1144\u001b[0m       0.9179        \u001b[35m0.3056\u001b[0m  0.1051\n",
      "     26        0.1339       0.9068        0.3349  0.1013\n",
      "     27        0.1658       0.9026        0.3486  0.1000\n",
      "     28        \u001b[36m0.1048\u001b[0m       0.9221        \u001b[35m0.2845\u001b[0m  0.1007\n",
      "     29        \u001b[36m0.0667\u001b[0m       \u001b[32m0.9346\u001b[0m        \u001b[35m0.2763\u001b[0m  0.0992\n",
      "     30        \u001b[36m0.0519\u001b[0m       0.9291        0.2902  0.1023\n",
      "     31        \u001b[36m0.0448\u001b[0m       0.9263        0.3001  0.1032\n",
      "     32        \u001b[36m0.0429\u001b[0m       0.9221        0.3112  0.1021\n",
      "     33        0.0439       0.9277        0.3218  0.1001\n",
      "     34        0.0547       0.9221        0.3340  0.1011\n",
      "     35        \u001b[36m0.0405\u001b[0m       0.9249        0.3750  0.1035\n",
      "     36        0.0497       0.9263        0.3848  0.1025\n",
      "     37        \u001b[36m0.0394\u001b[0m       0.9305        0.3116  0.1001\n",
      "     38        \u001b[36m0.0352\u001b[0m       0.9305        0.3218  0.1006\n",
      "     39        \u001b[36m0.0285\u001b[0m       \u001b[32m0.9374\u001b[0m        0.4183  0.1004\n",
      "     40        \u001b[36m0.0247\u001b[0m       \u001b[32m0.9388\u001b[0m        0.4030  0.1015\n",
      "     41        \u001b[36m0.0206\u001b[0m       0.9388        0.3927  0.1016\n",
      "     42        0.0211       0.9360        0.4190  0.1009\n",
      "     43        0.0294       0.9221        0.5266  0.1001\n",
      "     44        0.0215       0.9360        0.4991  0.1013\n",
      "     45        \u001b[36m0.0184\u001b[0m       0.9305        0.4058  0.1009\n",
      "     46        \u001b[36m0.0132\u001b[0m       0.9388        0.3845  0.0999\n",
      "     47        0.0235       \u001b[32m0.9430\u001b[0m        0.3580  0.0982\n",
      "     48        0.0181       0.9332        0.5292  0.0997\n",
      "     49        \u001b[36m0.0093\u001b[0m       0.9430        0.4252  0.1005\n",
      "     50        \u001b[36m0.0063\u001b[0m       \u001b[32m0.9458\u001b[0m        0.4363  0.1018\n",
      "     51        \u001b[36m0.0054\u001b[0m       0.9458        0.4770  0.1008\n",
      "     52        \u001b[36m0.0050\u001b[0m       0.9444        0.5010  0.0983\n",
      "     53        \u001b[36m0.0045\u001b[0m       0.9458        0.5533  0.1015\n",
      "     54        \u001b[36m0.0041\u001b[0m       0.9444        0.6022  0.1014\n",
      "     55        0.0200       0.9360        0.4173  0.1026\n",
      "     56        0.0218       0.9402        0.3777  0.1008\n",
      "     57        0.0133       0.9360        0.3713  0.1000\n",
      "     58        0.0113       0.9388        0.3617  0.0960\n",
      "     59        0.0096       0.9305        0.3960  0.1020\n",
      "     60        0.0081       0.9388        0.3912  0.0998\n",
      "     61        0.0054       0.9416        0.4159  0.1008\n",
      "     62        0.0057       0.9416        0.3428  0.1000\n",
      "     63        0.0088       0.9402        0.5376  0.1007\n",
      "     64        0.0045       0.9430        0.4304  0.1065\n",
      "     65        \u001b[36m0.0034\u001b[0m       0.9416        0.4700  0.1029\n",
      "     66        \u001b[36m0.0033\u001b[0m       0.9430        0.4904  0.1034\n",
      "     67        \u001b[36m0.0029\u001b[0m       0.9430        0.4981  0.1021\n",
      "     68        \u001b[36m0.0027\u001b[0m       0.9430        0.5050  0.1007\n",
      "     69        \u001b[36m0.0026\u001b[0m       0.9430        0.5161  0.1000\n",
      "     70        \u001b[36m0.0025\u001b[0m       0.9430        0.5243  0.0997\n",
      "     71        \u001b[36m0.0025\u001b[0m       0.9444        0.5444  0.1009\n",
      "     72        \u001b[36m0.0024\u001b[0m       0.9430        0.5439  0.0980\n",
      "     73        \u001b[36m0.0023\u001b[0m       0.9444        0.5618  0.0991\n",
      "     74        \u001b[36m0.0022\u001b[0m       0.9444        0.5652  0.0997\n",
      "     75        \u001b[36m0.0021\u001b[0m       0.9444        0.5827  0.0990\n",
      "     76        \u001b[36m0.0020\u001b[0m       0.9444        0.5932  0.0988\n",
      "     77        \u001b[36m0.0020\u001b[0m       0.9444        0.6162  0.0982\n",
      "     78        \u001b[36m0.0019\u001b[0m       0.9444        0.6212  0.0991\n",
      "     79        \u001b[36m0.0019\u001b[0m       0.9444        0.6455  0.0999\n",
      "     80        \u001b[36m0.0018\u001b[0m       0.9458        0.6598  0.0997\n",
      "     81        \u001b[36m0.0017\u001b[0m       0.9458        0.6853  0.0979\n",
      "     82        \u001b[36m0.0016\u001b[0m       0.9458        0.6951  0.0991\n",
      "     83        \u001b[36m0.0016\u001b[0m       0.9458        0.7287  0.0997\n",
      "     84        \u001b[36m0.0015\u001b[0m       0.9458        0.7630  0.0994\n",
      "     85        \u001b[36m0.0015\u001b[0m       0.9444        0.7917  0.0985\n",
      "     86        \u001b[36m0.0014\u001b[0m       0.9430        0.8024  0.0982\n",
      "     87        \u001b[36m0.0014\u001b[0m       0.9444        0.8315  0.0991\n",
      "     88        \u001b[36m0.0013\u001b[0m       0.9444        0.8370  0.0987\n",
      "     89        \u001b[36m0.0013\u001b[0m       0.9458        0.8725  0.0987\n",
      "     90        \u001b[36m0.0012\u001b[0m       0.9444        0.8571  0.0980\n",
      "     91        \u001b[36m0.0012\u001b[0m       0.9444        0.9419  0.0987\n",
      "     92        \u001b[36m0.0011\u001b[0m       0.9458        0.9359  0.0989\n",
      "     93        \u001b[36m0.0011\u001b[0m       0.9444        0.9782  0.0999\n",
      "     94        \u001b[36m0.0011\u001b[0m       0.9430        0.9514  0.1025\n",
      "     95        0.0011       0.9416        1.0326  0.1088\n",
      "     96        0.0072       0.9458        0.6234  0.1059\n",
      "     97        0.0064       0.9444        0.5819  0.1065\n",
      "     98        0.0033       0.9458        0.5796  0.1094\n",
      "     99        0.0024       0.9458        0.5639  0.1075\n",
      "    100        0.0026       0.9416        0.6227  0.1068\n",
      "    101        0.0028       \u001b[32m0.9527\u001b[0m        0.5212  0.1064\n",
      "    102        0.0032       0.9471        0.6911  0.1055\n",
      "    103        0.0022       0.9444        0.5492  0.1074\n",
      "    104        0.0017       0.9458        0.6214  0.1042\n",
      "    105        0.0014       0.9471        0.6667  0.1064\n",
      "    106        0.0015       0.9485        0.6506  0.1023\n",
      "    107        0.0013       0.9471        0.6463  0.0997\n",
      "    108        0.0014       0.9485        0.6405  0.0975\n",
      "    109        0.0013       0.9458        0.6450  0.0981\n",
      "    110        0.0012       0.9485        0.6449  0.0978\n",
      "    111        0.0012       0.9471        0.6489  0.0975\n",
      "    112        0.0011       0.9485        0.6477  0.0997\n",
      "    113        0.0011       0.9471        0.6554  0.0980\n",
      "    114        0.0011       0.9471        0.6551  0.0980\n",
      "    115        \u001b[36m0.0010\u001b[0m       0.9471        0.6638  0.0990\n",
      "    116        0.0011       0.9471        0.6573  0.0992\n",
      "    117        \u001b[36m0.0009\u001b[0m       0.9471        0.6679  0.0987\n",
      "    118        0.0010       0.9458        0.6648  0.0969\n",
      "    119        \u001b[36m0.0006\u001b[0m       0.9458        0.7073  0.0990\n",
      "    120        0.0007       0.9444        0.6943  0.1021\n",
      "    121        0.0008       0.9430        0.6856  0.1123\n",
      "    122        0.0009       0.9430        0.6780  0.1002\n",
      "    123        \u001b[36m0.0005\u001b[0m       0.9444        0.6958  0.0978\n",
      "    124        \u001b[36m0.0005\u001b[0m       0.9444        0.6839  0.0976\n",
      "    125        \u001b[36m0.0003\u001b[0m       0.9416        0.6913  0.0977\n",
      "    126        \u001b[36m0.0003\u001b[0m       0.9430        0.6923  0.0985\n",
      "    127        \u001b[36m0.0002\u001b[0m       0.9416        0.7081  0.0987\n",
      "    128        \u001b[36m0.0001\u001b[0m       0.9402        0.7139  0.0968\n",
      "    129        \u001b[36m0.0001\u001b[0m       0.9416        0.7195  0.0993\n",
      "    130        \u001b[36m0.0001\u001b[0m       0.9402        0.7260  0.0984\n",
      "    131        \u001b[36m0.0001\u001b[0m       0.9416        0.7293  0.0977\n",
      "    132        \u001b[36m0.0001\u001b[0m       0.9402        0.7357  0.0989\n",
      "    133        \u001b[36m0.0001\u001b[0m       0.9402        0.7368  0.0971\n",
      "    134        \u001b[36m0.0001\u001b[0m       0.9416        0.7416  0.0983\n",
      "    135        \u001b[36m0.0001\u001b[0m       0.9402        0.7452  0.0984\n",
      "    136        \u001b[36m0.0001\u001b[0m       0.9416        0.7487  0.0972\n",
      "    137        \u001b[36m0.0001\u001b[0m       0.9402        0.7543  0.0983\n",
      "    138        \u001b[36m0.0001\u001b[0m       0.9402        0.7555  0.0977\n",
      "    139        \u001b[36m0.0001\u001b[0m       0.9388        0.7593  0.0980\n",
      "    140        \u001b[36m0.0001\u001b[0m       0.9388        0.7625  0.0985\n",
      "    141        \u001b[36m0.0001\u001b[0m       0.9374        0.7660  0.0989\n",
      "    142        \u001b[36m0.0001\u001b[0m       0.9374        0.7670  0.0983\n",
      "    143        \u001b[36m0.0001\u001b[0m       0.9374        0.7707  0.0980\n",
      "    144        \u001b[36m0.0001\u001b[0m       0.9374        0.7745  0.0987\n",
      "    145        \u001b[36m0.0001\u001b[0m       0.9374        0.7771  0.0977\n",
      "    146        \u001b[36m0.0001\u001b[0m       0.9360        0.7783  0.0975\n",
      "    147        \u001b[36m0.0001\u001b[0m       0.9360        0.7813  0.0980\n"
     ]
    }
   ],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train).type(torch.LongTensor)\n",
    "\n",
    "skorch_model = NeuralNetClassifier(module=NNClassifier,\n",
    "                                   criterion=nn.CrossEntropyLoss,\n",
    "                                   device=device)\n",
    "\n",
    "param_grid = {\n",
    "    'max_epochs': np.arange(50,151),\n",
    "    'optimizer': [optim.Adam,optim.SGD, optim.RMSprop],\n",
    "    'optimizer__lr': np.arange(1e-4, 1e-1, 0.001),\n",
    "    'module__weight_init': [init.kaiming_normal_, init.kaiming_uniform_, init.xavier_normal_, init.xavier_uniform_],\n",
    "    'module__layer_dims': [[50,100,50,15],\n",
    "                           [15,30,45,90,60,30,10]],\n",
    "    'module__layer_acts': [\"ReLU\", \"LeakyReLU\", \"RReLU\"],\n",
    "    'module__input_dim_size': [X_train.shape[1]],\n",
    "    'module__output_dim_size': [7] \n",
    "}\n",
    "\n",
    "grid_search = RandomizedSearchCV(skorch_model, \n",
    "                                param_grid, \n",
    "                                scoring=\"accuracy\",\n",
    "                                cv=3, n_iter=5, \n",
    "                                n_jobs=-1, verbose=False)\n",
    "grid_search = grid_search.fit(X_train_tensor, y_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective Value: 0.8473201327576355\n",
      "Parameters:\n",
      "  optimizer__lr: 0.0641\n",
      "  optimizer: <class 'torch.optim.adam.Adam'>\n",
      "  module__weight_init: <function kaiming_normal_ at 0x10d080e00>\n",
      "  module__output_dim_size: 7\n",
      "  module__layer_dims: [50, 100, 50, 15]\n",
      "  module__layer_acts: LeakyReLU\n",
      "  module__input_dim_size: 1024\n",
      "  max_epochs: 147\n"
     ]
    }
   ],
   "source": [
    "best_params_skorch = grid_search.best_params_\n",
    "\n",
    "print(\"Objective Value:\", grid_search.best_score_)\n",
    "print(\"Parameters:\")\n",
    "for key, value in best_params_skorch.items():\n",
    "    print(\"  {}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model is on this device:  mps\n",
      "********\n",
      "Best model's train accuracy score:  0.9872044506258693\n",
      "Best model's test accuracy score:  0.8016336056009334\n",
      "Best model's validation accuracy score:  0.8163265306122449\n"
     ]
    }
   ],
   "source": [
    "best_model_skorch = grid_search.best_estimator_\n",
    "print(\"Best model is on this device: \", best_model_skorch.device)\n",
    "print(\"********\")\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32, device=device)\n",
    "\n",
    "y_train_preds = best_model_skorch.predict(X_train_tensor)\n",
    "y_test_preds = best_model_skorch.predict(X_test_tensor)\n",
    "y_val_preds = best_model_skorch.predict(X_val_tensor)\n",
    "\n",
    "train_score = accuracy_score(y_train, y_train_preds)\n",
    "test_score = accuracy_score(y_test, y_test_preds)\n",
    "val_score = accuracy_score(y_val, y_val_preds)\n",
    "\n",
    "print(\"Best model's train accuracy score: \", train_score)\n",
    "print(\"Best model's test accuracy score: \", test_score)\n",
    "print(\"Best model's validation accuracy score: \", val_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_loaders_from_raw_data():\n",
    "    with open(\"datasets/processed/embed_and_cat_multilingual.pkl\", \"rb\") as f:\n",
    "        embed_and_cat_multilingual = pickle.load(f)\n",
    "\n",
    "    X_train, y_train, X_test, y_test, X_val, y_val  = [], [], [], [], [], []\n",
    "    for lang, split_dict in embed_and_cat_multilingual.items(): \n",
    "        X_train += split_dict[\"train\"][\"embedding\"]\n",
    "        y_train += split_dict[\"train\"][\"category\"]\n",
    "        X_test += split_dict[\"test\"][\"embedding\"]\n",
    "        y_test += split_dict[\"test\"][\"category\"]\n",
    "        X_val += split_dict[\"validation\"][\"embedding\"]\n",
    "        y_val += split_dict[\"validation\"][\"category\"]\n",
    "    X_train, y_train, X_test, y_test, X_val, y_val = np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test), np.array(X_val), np.array(y_val)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler = fit_scaler(X_train=X_train, scaler=scaler)\n",
    "\n",
    "    X_train_scaled = scale_features(X_train, scaler)\n",
    "    X_test_scaled = scale_features(X_test, scaler)\n",
    "    X_val_scaled = scale_features(X_val, scaler)\n",
    "\n",
    "    train_loader = make_dataloader(X=X_train_scaled, y=y_train, batch_size=64, shuffle=True, seed=42)\n",
    "    test_loader = make_dataloader(X=X_test_scaled, y=y_test, batch_size=16, shuffle=False, seed=42)\n",
    "    val_loader = make_dataloader(X=X_val_scaled, y=y_val, batch_size=1, shuffle=False, seed=42)\n",
    "\n",
    "    return train_loader, test_loader, val_loader\n",
    "    \n",
    "\n",
    "def train_classifier(config):\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    weight_init_name = config[\"weight_init_name\"]\n",
    "    weight_init = getattr(init, weight_init_name)\n",
    "\n",
    "    layer_dims = config[\"layer_dims\"]\n",
    "    act_name = config[\"act_name\"]\n",
    "    layer_acts = [getattr(nn, act_name)() for _ in range(len(layer_dims))]\n",
    "\n",
    "    model = NNClassifier(input_dim_size=1024,\n",
    "                         output_dim_size=7,\n",
    "                         layer_dims=layer_dims,\n",
    "                         layer_acts=layer_acts,\n",
    "                         weight_init=weight_init)\n",
    "    learning_rate = config[\"learning_rate\"]\n",
    "    optimizer_name = config[\"optimizer_name\"]\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loader, test_loader, _ = generate_loaders_from_raw_data()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        _ = train_step(data_loader=train_loader,\n",
    "                        model=model,\n",
    "                        loss_fn=criterion,\n",
    "                        optimizer=optimizer,\n",
    "                        accuracy_fn=accuracy_fn,\n",
    "                        device=device,\n",
    "                        verbose=False)\n",
    "        \n",
    "        test_loss, test_acc = test_step(data_loader=test_loader,\n",
    "                                model=model,\n",
    "                                loss_fn=criterion,\n",
    "                                accuracy_fn=accuracy_fn,\n",
    "                                device=device,\n",
    "                                verbose=False)\n",
    "    tune.report({\"loss\": test_loss, \"accuracy\": test_acc})\n",
    "        \n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-03-08 17:12:17</td></tr>\n",
       "<tr><td>Running for: </td><td>00:02:09.88        </td></tr>\n",
       "<tr><td>Memory:      </td><td>45.4/64.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 80.000: None | Iter 40.000: None | Iter 20.000: None | Iter 10.000: None | Iter 5.000: None<br>Logical resource usage: 10.0/12 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc  </th><th>act_name  </th><th>layer_dims  </th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  num_epochs</th><th>optimizer_name  </th><th>weight_init_name  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_classifier_0a580_00000</td><td>PENDING </td><td>     </td><td>RReLU     </td><td>[15, 30, 15]</td><td style=\"text-align: right;\">    0.00293318 </td><td style=\"text-align: right;\">           5</td><td>SGD             </td><td>kaiming_uniform_  </td></tr>\n",
       "<tr><td>train_classifier_0a580_00001</td><td>PENDING </td><td>     </td><td>ReLU      </td><td>[15, 90, 10]</td><td style=\"text-align: right;\">    0.000342652</td><td style=\"text-align: right;\">           7</td><td>RMSprop         </td><td>kaiming_uniform_  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m bash: /Users/toygunkarabas/Development/NLP: No such file or directory\n",
      "\u001b[33m(raylet)\u001b[0m bash: line 0: exec: /Users/toygunkarabas/Development/NLP: cannot execute: No such file or directory\n",
      "\u001b[33m(raylet)\u001b[0m bash: SLP/nlp_slp_env/bin/python: No such file or directory\n",
      "\u001b[33m(raylet)\u001b[0m [2025-03-08 17:11:08,039 E 40153 28468980] (raylet) worker_pool.cc:581: Some workers of the worker process(40169) have not registered within the timeout. The process is dead, probably it crashed during start.\n",
      "\u001b[33m(raylet)\u001b[0m bash: /Users/toygunkarabas/Development/NLP: No such file or directory\n",
      "\u001b[33m(raylet)\u001b[0m bash: line 0: exec: /Users/toygunkarabas/Development/NLP: cannot execute: No such file or directory\n",
      "\u001b[33m(raylet)\u001b[0m bash: SLP/nlp_slp_env/bin/python: No such file or directory\n",
      "\u001b[33m(raylet)\u001b[0m bash: /Users/toygunkarabas/Development/NLP: No such file or directory\n",
      "\u001b[33m(raylet)\u001b[0m bash: line 0: exec: /Users/toygunkarabas/Development/NLP: cannot execute: No such file or directory\n",
      "\u001b[33m(raylet)\u001b[0m bash: SLP/nlp_slp_env/bin/python: No such file or directory\n",
      "\u001b[33m(raylet)\u001b[0m [2025-03-08 17:12:08,070 E 40153 28468980] (raylet) worker_pool.cc:581: Some workers of the worker process(40182) have not registered within the timeout. The process is dead, probably it crashed during start.\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m bash: /Users/toygunkarabas/Development/NLP: No such file or directory\n",
      "\u001b[33m(raylet)\u001b[0m bash: line 0: exec: /Users/toygunkarabas/Development/NLP: cannot execute: No such file or directory\n",
      "\u001b[33m(raylet)\u001b[0m bash: SLP/nlp_slp_env/bin/python: No such file or directory\n",
      "\u001b[33m(raylet)\u001b[0m bash: /Users/toygunkarabas/Development/NLP: No such file or directory\n",
      "\u001b[33m(raylet)\u001b[0m bash: line 0: exec: /Users/toygunkarabas/Development/NLP: cannot execute: No such file or directory\n",
      "\u001b[33m(raylet)\u001b[0m bash: SLP/nlp_slp_env/bin/python: No such file or directory\n",
      "2025-03-08 17:12:17,800\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2025-03-08 17:12:17,803\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/toygunkarabas/ray_results/train_classifier_2025-03-08_17-10-07' in 0.0024s.\n",
      "2025-03-08 17:12:27,842\tINFO tune.py:1041 -- Total run time: 139.92 seconds (129.87 seconds for the tuning loop).\n",
      "2025-03-08 17:12:27,843\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: Tuner.restore(path=\"/Users/toygunkarabas/ray_results/train_classifier_2025-03-08_17-10-07\", trainable=...)\n",
      "2025-03-08 17:12:27,845\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 2 trial(s):\n",
      "- train_classifier_0a580_00000: FileNotFoundError('Could not fetch metrics for train_classifier_0a580_00000: both result.json and progress.csv were not found at /Users/toygunkarabas/ray_results/train_classifier_2025-03-08_17-10-07/train_classifier_0a580_00000_0_act_name=RReLU,layer_dims=15_30_15,learning_rate=0.0029,num_epochs=5,optimizer_name=SGD,weight_init_2025-03-08_17-10-07')\n",
      "- train_classifier_0a580_00001: FileNotFoundError('Could not fetch metrics for train_classifier_0a580_00001: both result.json and progress.csv were not found at /Users/toygunkarabas/ray_results/train_classifier_2025-03-08_17-10-07/train_classifier_0a580_00001_1_act_name=ReLU,layer_dims=15_90_10,learning_rate=0.0003,num_epochs=7,optimizer_name=RMSprop,weight_i_2025-03-08_17-10-07')\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'num_epochs': tune.randint(5, 10),\n",
    "    'layer_dims': tune.choice([[15,30,15],\n",
    "                               [15,90,10]]),\n",
    "    'act_name': tune.choice([\"ReLU\", \"LeakyReLU\", \"RReLU\"]),\n",
    "    'weight_init_name': tune.choice([\"kaiming_normal_\", \"kaiming_uniform_\", \"xavier_normal_\", \"xavier_uniform_\"]),\n",
    "    'learning_rate': tune.loguniform(1e-4, 1e-1),\n",
    "    'optimizer_name': tune.choice([\"Adam\", \"SGD\", \"RMSprop\"])\n",
    "}\n",
    "\n",
    "ray.shutdown()  # Clean any previous Ray instances\n",
    "# Start Ray with specific CPU/GPU allocation\n",
    "ray.init(num_cpus=12, num_gpus=0)\n",
    "\n",
    "scheduler = ASHAScheduler(grace_period=5, # Run at least 5 epochs before stopping trials\n",
    "                          reduction_factor=2 # # Reduce number of trials by 0x per iteration\n",
    "                        )\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(\n",
    "        tune.with_parameters(train_classifier),\n",
    "        resources={\"cpu\": 5} #, \"gpu\": 0, \"num_workers\": 2}\n",
    "    ),\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"accuracy\",\n",
    "        mode=\"max\",\n",
    "        scheduler=scheduler,\n",
    "        num_samples=3, # equivalent to parameter 'n_trials' of optuna\n",
    "        max_concurrent_trials=2 # Run 2 trials in parallel\n",
    "    ),\n",
    "    param_space=config,\n",
    ")\n",
    "ray_tuner_results = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above implementation, something went wrong. I will check it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allegro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be implemented later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna\n",
    "- https://www.geeksforgeeks.org/hyperparameter-tuning-with-optuna-in-pytorch/\n",
    "\n",
    "### Skorch\n",
    "- https://machinelearningmastery.com/how-to-grid-search-hyperparameters-for-pytorch-models/\n",
    "\n",
    "- https://memudualimatou.medium.com/skorch-hyper-parameter-tuning-with-pytorch-b5af0ba8d45c\n",
    "\n",
    "- https://debuggercafe.com/hyperparameter-search-with-pytorch-and-skorch/\n",
    "\n",
    "### Ray Tune\n",
    "- https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html\n",
    "\n",
    "- https://debuggercafe.com/hyperparameter-tuning-with-pytorch-and-ray-tune/\n",
    "\n",
    "- https://www.geeksforgeeks.org/hyperparameter-tuning-with-ray-tune-in-pytorch/\n",
    "\n",
    "- https://docs.ray.io/en/latest/tune/index.html\n",
    "\n",
    "### Allegro\n",
    "- https://medium.com/pytorch/accelerate-your-hyperparameter-optimization-with-pytorchs-ecosystem-tools-bc17001b9a49\n",
    "\n",
    "- https://github.com/clearml/clearml/tree/master/examples/frameworks/pytorch/notebooks/image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_slp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
