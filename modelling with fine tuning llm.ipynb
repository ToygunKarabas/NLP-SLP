{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from typing import Literal, Union, List, Any, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchinfo import summary\n",
    "\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer, BitsAndBytesConfig, Gemma3ForCausalLM, DataCollatorForLanguageModeling\n",
    "from peft import PeftModel, get_peft_model, LoraConfig, prepare_model_for_kbit_training, TaskType\n",
    "import evaluate\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    if torch.backends.mps.is_built():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "if device==\"mps\":\n",
    "    torch.mps.manual_seed(42)\n",
    "\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>domain</th>\n",
       "      <th>topic</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>split</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>processed_lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tr-TR</td>\n",
       "      <td>wikibooks</td>\n",
       "      <td>geography, turkey</td>\n",
       "      <td>Türkiye'nin üç tarafı denizlerle çevrilidir: B...</td>\n",
       "      <td>6</td>\n",
       "      <td>train</td>\n",
       "      <td>türkiye'nin üç tarafı denizlerle çevrilidir ba...</td>\n",
       "      <td>türkiye üç taraf deniz çevri batı ege deniz ku...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tr-TR</td>\n",
       "      <td>wikibooks</td>\n",
       "      <td>world war ii, submarines</td>\n",
       "      <td>Savaşın başlangıcında çoğunlukla denizin yüzey...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>savaşın başlangıcında denizin yüzeyinde seyaha...</td>\n",
       "      <td>savaş başlangıç deniz yüzey seyahat etmek rada...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tr-TR</td>\n",
       "      <td>wikivoyage</td>\n",
       "      <td>natural wonders, hill stations in india</td>\n",
       "      <td>Ancak, kış aylarında farklı bir güzelliği ve ç...</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>ancak kış aylarında farklı güzelliği çekiciliğ...</td>\n",
       "      <td>kış ay fark güzel çekici var birçok tepe şehir...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language      domain                                    topic  \\\n",
       "0    tr-TR   wikibooks                        geography, turkey   \n",
       "1    tr-TR   wikibooks                 world war ii, submarines   \n",
       "2    tr-TR  wikivoyage  natural wonders, hill stations in india   \n",
       "\n",
       "                                                text  category  split  \\\n",
       "0  Türkiye'nin üç tarafı denizlerle çevrilidir: B...         6  train   \n",
       "1  Savaşın başlangıcında çoğunlukla denizin yüzey...         0  train   \n",
       "2  Ancak, kış aylarında farklı bir güzelliği ve ç...         5  train   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0  türkiye'nin üç tarafı denizlerle çevrilidir ba...   \n",
       "1  savaşın başlangıcında denizin yüzeyinde seyaha...   \n",
       "2  ancak kış aylarında farklı güzelliği çekiciliğ...   \n",
       "\n",
       "                                    processed_lemmas  \n",
       "0  türkiye üç taraf deniz çevri batı ege deniz ku...  \n",
       "1  savaş başlangıç deniz yüzey seyahat etmek rada...  \n",
       "2  kış ay fark güzel çekici var birçok tepe şehir...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"datasets/processed/prepared_data.csv\", index_col=[0])\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533fbca7d1ff4e9b85cb2fd1fde5f5f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4795 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f02096cb6db04fcc8fe791d68049e5da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4795 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370bd605953c43a1a4a793d80c33a1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4795 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 3595\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 857\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 343\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(data[[\"split\",\"text\",\"category\"]], preserve_index=False)\n",
    "\n",
    "# Split dataset into train, test, validation\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": dataset.filter(lambda x: x[\"split\"] == \"train\"),\n",
    "    \"test\": dataset.filter(lambda x: x[\"split\"] == \"test\"),\n",
    "    \"validation\": dataset.filter(lambda x: x[\"split\"] == \"validation\"),\n",
    "})\n",
    "\n",
    "# Remove the 'split' column as it's no longer needed\n",
    "for split in dataset_dict.keys():\n",
    "    dataset_dict[split] = dataset_dict[split].remove_columns(column_names=[\"split\"])\n",
    "    dataset_dict[split] = dataset_dict[split].rename_column(\"category\", \"labels\")\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2category = {\n",
    "    0: \"science/technology\",\n",
    "    1: \"travel\",\n",
    "    2: \"politics\",\n",
    "    3: \"sports\",\n",
    "    4: \"health\",\n",
    "    5: \"entertainment\",\n",
    "    6: \"geography\"\n",
    "}\n",
    "category2id = {\n",
    "    \"science/technology\": 0,\n",
    "    \"travel\": 1,\n",
    "    \"politics\": 2,\n",
    "    \"sports\": 3,\n",
    "    \"health\": 4,\n",
    "    \"entertainment\": 5,\n",
    "    \"geography\": 6\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraning All Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Teacher Model \n",
    "teacher_model_id = 'xlm-roberta-large'\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    teacher_model_id,\n",
    "    num_labels=7,\n",
    "    id2label=id2category,\n",
    "    label2id=category2id\n",
    ")\n",
    "tokenizer_teacher = AutoTokenizer.from_pretrained(teacher_model_id)\n",
    "# tokenizer_teacher.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7fb845ba2434199b1b3cf66d14809c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3595 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c2bc5e36f554413bf3b8debd9de1b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/857 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02750f518ff44ea4a1f7c5044ecdc962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "tokenized_data = dataset_dict.map(lambda x: tokenizer_teacher(x[\"text\"]), batched=True)\n",
    "\n",
    "# Define Accuracy Metric\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    acc = accuracy.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    return {\"accuracy\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fq/v75g6pgd5djdqc2pz_0cgsmw0000gn/T/ipykernel_56205/3307780425.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2250/2250 27:37, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.945800</td>\n",
       "      <td>1.900039</td>\n",
       "      <td>0.299883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.917100</td>\n",
       "      <td>1.852447</td>\n",
       "      <td>0.299883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.912500</td>\n",
       "      <td>1.876590</td>\n",
       "      <td>0.299883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.901700</td>\n",
       "      <td>1.879262</td>\n",
       "      <td>0.299883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.901500</td>\n",
       "      <td>1.869760</td>\n",
       "      <td>0.299883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.893700</td>\n",
       "      <td>1.881078</td>\n",
       "      <td>0.155193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.892500</td>\n",
       "      <td>1.864643</td>\n",
       "      <td>0.299883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.885800</td>\n",
       "      <td>1.870180</td>\n",
       "      <td>0.299883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.885000</td>\n",
       "      <td>1.863566</td>\n",
       "      <td>0.299883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.882100</td>\n",
       "      <td>1.871811</td>\n",
       "      <td>0.299883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2250, training_loss=1.9017613254123265, metrics={'train_runtime': 1660.2411, 'train_samples_per_second': 21.653, 'train_steps_per_second': 1.355, 'total_flos': 3894661567799532.0, 'train_loss': 1.9017613254123265, 'epoch': 10.0})"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training HyperParameters\n",
    "lr = 2e-4\n",
    "weight_decay = 0.01\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "\n",
    "teacher_training_args = TrainingArguments(\n",
    "    # output_dir=\"bert-category-classifier-teacher\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer_teacher)\n",
    "# Use the custom compute_loss function in Trainer\n",
    "trainer = Trainer(\n",
    "    model=teacher_model,\n",
    "    args=teacher_training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    processing_class=tokenizer_teacher,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "tokenized_val_data = tokenizer_teacher(\n",
    "    dataset_dict[\"validation\"][\"text\"], \n",
    "    padding=True, \n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer_teacher, return_tensors=\"pt\")\n",
    "batch_inputs = data_collator(tokenized_val_data)\n",
    "\n",
    "teacher_model.eval()\n",
    "with torch.no_grad():\n",
    "    teacher_outputs = teacher_model.to(\"cpu\")(**batch_inputs)\n",
    "teacher_logits = teacher_outputs.logits\n",
    "teacher_preds = torch.softmax(teacher_logits, dim=1).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.27988338192419826}"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(eval_pred=(teacher_logits.numpy(), dataset_dict[\"validation\"][\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_models/fine_tuned_roberta_teacher_model/tokenizer_config.json',\n",
       " './fine_tuned_models/fine_tuned_roberta_teacher_model/special_tokens_map.json',\n",
       " './fine_tuned_models/fine_tuned_roberta_teacher_model/sentencepiece.bpe.model',\n",
       " './fine_tuned_models/fine_tuned_roberta_teacher_model/added_tokens.json',\n",
       " './fine_tuned_models/fine_tuned_roberta_teacher_model/tokenizer.json')"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_directory = \"./fine_tuned_models/fine_tuned_roberta_teacher_model\"\n",
    "\n",
    "# Save model\n",
    "teacher_model.save_pretrained(save_directory)\n",
    "# Save tokenizer\n",
    "tokenizer_teacher.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Fine Tuning only classifier layer of Teacher Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Teacher Model \n",
    "teacher_model_id2 = 'xlm-roberta-large'\n",
    "teacher_model2 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    teacher_model_id2,\n",
    "    num_labels=7,\n",
    "    id2label=id2category,\n",
    "    label2id=category2id\n",
    ")\n",
    "tokenizer_teacher2 = AutoTokenizer.from_pretrained(teacher_model_id2)\n",
    "# tokenizer_teacher2.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.embeddings.word_embeddings.weight\n",
      "roberta.embeddings.position_embeddings.weight\n",
      "roberta.embeddings.token_type_embeddings.weight\n",
      "roberta.embeddings.LayerNorm.weight\n",
      "roberta.embeddings.LayerNorm.bias\n",
      "roberta.encoder.layer.0.attention.self.query.weight\n",
      "roberta.encoder.layer.0.attention.self.query.bias\n",
      "roberta.encoder.layer.0.attention.self.key.weight\n",
      "roberta.encoder.layer.0.attention.self.key.bias\n",
      "roberta.encoder.layer.0.attention.self.value.weight\n",
      "roberta.encoder.layer.0.attention.self.value.bias\n",
      "roberta.encoder.layer.0.attention.output.dense.weight\n",
      "roberta.encoder.layer.0.attention.output.dense.bias\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.0.intermediate.dense.weight\n",
      "roberta.encoder.layer.0.intermediate.dense.bias\n",
      "roberta.encoder.layer.0.output.dense.weight\n",
      "roberta.encoder.layer.0.output.dense.bias\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias\n",
      "roberta.encoder.layer.1.attention.self.query.weight\n",
      "roberta.encoder.layer.1.attention.self.query.bias\n",
      "roberta.encoder.layer.1.attention.self.key.weight\n",
      "roberta.encoder.layer.1.attention.self.key.bias\n",
      "roberta.encoder.layer.1.attention.self.value.weight\n",
      "roberta.encoder.layer.1.attention.self.value.bias\n",
      "roberta.encoder.layer.1.attention.output.dense.weight\n",
      "roberta.encoder.layer.1.attention.output.dense.bias\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.1.intermediate.dense.weight\n",
      "roberta.encoder.layer.1.intermediate.dense.bias\n",
      "roberta.encoder.layer.1.output.dense.weight\n",
      "roberta.encoder.layer.1.output.dense.bias\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.attention.self.query.weight\n",
      "roberta.encoder.layer.2.attention.self.query.bias\n",
      "roberta.encoder.layer.2.attention.self.key.weight\n",
      "roberta.encoder.layer.2.attention.self.key.bias\n",
      "roberta.encoder.layer.2.attention.self.value.weight\n",
      "roberta.encoder.layer.2.attention.self.value.bias\n",
      "roberta.encoder.layer.2.attention.output.dense.weight\n",
      "roberta.encoder.layer.2.attention.output.dense.bias\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.intermediate.dense.weight\n",
      "roberta.encoder.layer.2.intermediate.dense.bias\n",
      "roberta.encoder.layer.2.output.dense.weight\n",
      "roberta.encoder.layer.2.output.dense.bias\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.attention.self.query.weight\n",
      "roberta.encoder.layer.3.attention.self.query.bias\n",
      "roberta.encoder.layer.3.attention.self.key.weight\n",
      "roberta.encoder.layer.3.attention.self.key.bias\n",
      "roberta.encoder.layer.3.attention.self.value.weight\n",
      "roberta.encoder.layer.3.attention.self.value.bias\n",
      "roberta.encoder.layer.3.attention.output.dense.weight\n",
      "roberta.encoder.layer.3.attention.output.dense.bias\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.intermediate.dense.weight\n",
      "roberta.encoder.layer.3.intermediate.dense.bias\n",
      "roberta.encoder.layer.3.output.dense.weight\n",
      "roberta.encoder.layer.3.output.dense.bias\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.attention.self.query.weight\n",
      "roberta.encoder.layer.4.attention.self.query.bias\n",
      "roberta.encoder.layer.4.attention.self.key.weight\n",
      "roberta.encoder.layer.4.attention.self.key.bias\n",
      "roberta.encoder.layer.4.attention.self.value.weight\n",
      "roberta.encoder.layer.4.attention.self.value.bias\n",
      "roberta.encoder.layer.4.attention.output.dense.weight\n",
      "roberta.encoder.layer.4.attention.output.dense.bias\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.intermediate.dense.weight\n",
      "roberta.encoder.layer.4.intermediate.dense.bias\n",
      "roberta.encoder.layer.4.output.dense.weight\n",
      "roberta.encoder.layer.4.output.dense.bias\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.attention.self.query.weight\n",
      "roberta.encoder.layer.5.attention.self.query.bias\n",
      "roberta.encoder.layer.5.attention.self.key.weight\n",
      "roberta.encoder.layer.5.attention.self.key.bias\n",
      "roberta.encoder.layer.5.attention.self.value.weight\n",
      "roberta.encoder.layer.5.attention.self.value.bias\n",
      "roberta.encoder.layer.5.attention.output.dense.weight\n",
      "roberta.encoder.layer.5.attention.output.dense.bias\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.intermediate.dense.weight\n",
      "roberta.encoder.layer.5.intermediate.dense.bias\n",
      "roberta.encoder.layer.5.output.dense.weight\n",
      "roberta.encoder.layer.5.output.dense.bias\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.attention.self.query.weight\n",
      "roberta.encoder.layer.6.attention.self.query.bias\n",
      "roberta.encoder.layer.6.attention.self.key.weight\n",
      "roberta.encoder.layer.6.attention.self.key.bias\n",
      "roberta.encoder.layer.6.attention.self.value.weight\n",
      "roberta.encoder.layer.6.attention.self.value.bias\n",
      "roberta.encoder.layer.6.attention.output.dense.weight\n",
      "roberta.encoder.layer.6.attention.output.dense.bias\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.intermediate.dense.weight\n",
      "roberta.encoder.layer.6.intermediate.dense.bias\n",
      "roberta.encoder.layer.6.output.dense.weight\n",
      "roberta.encoder.layer.6.output.dense.bias\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.attention.self.query.weight\n",
      "roberta.encoder.layer.7.attention.self.query.bias\n",
      "roberta.encoder.layer.7.attention.self.key.weight\n",
      "roberta.encoder.layer.7.attention.self.key.bias\n",
      "roberta.encoder.layer.7.attention.self.value.weight\n",
      "roberta.encoder.layer.7.attention.self.value.bias\n",
      "roberta.encoder.layer.7.attention.output.dense.weight\n",
      "roberta.encoder.layer.7.attention.output.dense.bias\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.intermediate.dense.weight\n",
      "roberta.encoder.layer.7.intermediate.dense.bias\n",
      "roberta.encoder.layer.7.output.dense.weight\n",
      "roberta.encoder.layer.7.output.dense.bias\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.attention.self.query.weight\n",
      "roberta.encoder.layer.8.attention.self.query.bias\n",
      "roberta.encoder.layer.8.attention.self.key.weight\n",
      "roberta.encoder.layer.8.attention.self.key.bias\n",
      "roberta.encoder.layer.8.attention.self.value.weight\n",
      "roberta.encoder.layer.8.attention.self.value.bias\n",
      "roberta.encoder.layer.8.attention.output.dense.weight\n",
      "roberta.encoder.layer.8.attention.output.dense.bias\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.intermediate.dense.weight\n",
      "roberta.encoder.layer.8.intermediate.dense.bias\n",
      "roberta.encoder.layer.8.output.dense.weight\n",
      "roberta.encoder.layer.8.output.dense.bias\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.attention.self.query.weight\n",
      "roberta.encoder.layer.9.attention.self.query.bias\n",
      "roberta.encoder.layer.9.attention.self.key.weight\n",
      "roberta.encoder.layer.9.attention.self.key.bias\n",
      "roberta.encoder.layer.9.attention.self.value.weight\n",
      "roberta.encoder.layer.9.attention.self.value.bias\n",
      "roberta.encoder.layer.9.attention.output.dense.weight\n",
      "roberta.encoder.layer.9.attention.output.dense.bias\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.intermediate.dense.weight\n",
      "roberta.encoder.layer.9.intermediate.dense.bias\n",
      "roberta.encoder.layer.9.output.dense.weight\n",
      "roberta.encoder.layer.9.output.dense.bias\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.attention.self.query.weight\n",
      "roberta.encoder.layer.10.attention.self.query.bias\n",
      "roberta.encoder.layer.10.attention.self.key.weight\n",
      "roberta.encoder.layer.10.attention.self.key.bias\n",
      "roberta.encoder.layer.10.attention.self.value.weight\n",
      "roberta.encoder.layer.10.attention.self.value.bias\n",
      "roberta.encoder.layer.10.attention.output.dense.weight\n",
      "roberta.encoder.layer.10.attention.output.dense.bias\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.intermediate.dense.weight\n",
      "roberta.encoder.layer.10.intermediate.dense.bias\n",
      "roberta.encoder.layer.10.output.dense.weight\n",
      "roberta.encoder.layer.10.output.dense.bias\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.attention.self.query.weight\n",
      "roberta.encoder.layer.11.attention.self.query.bias\n",
      "roberta.encoder.layer.11.attention.self.key.weight\n",
      "roberta.encoder.layer.11.attention.self.key.bias\n",
      "roberta.encoder.layer.11.attention.self.value.weight\n",
      "roberta.encoder.layer.11.attention.self.value.bias\n",
      "roberta.encoder.layer.11.attention.output.dense.weight\n",
      "roberta.encoder.layer.11.attention.output.dense.bias\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.intermediate.dense.weight\n",
      "roberta.encoder.layer.11.intermediate.dense.bias\n",
      "roberta.encoder.layer.11.output.dense.weight\n",
      "roberta.encoder.layer.11.output.dense.bias\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "roberta.encoder.layer.12.attention.self.query.weight\n",
      "roberta.encoder.layer.12.attention.self.query.bias\n",
      "roberta.encoder.layer.12.attention.self.key.weight\n",
      "roberta.encoder.layer.12.attention.self.key.bias\n",
      "roberta.encoder.layer.12.attention.self.value.weight\n",
      "roberta.encoder.layer.12.attention.self.value.bias\n",
      "roberta.encoder.layer.12.attention.output.dense.weight\n",
      "roberta.encoder.layer.12.attention.output.dense.bias\n",
      "roberta.encoder.layer.12.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.12.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.12.intermediate.dense.weight\n",
      "roberta.encoder.layer.12.intermediate.dense.bias\n",
      "roberta.encoder.layer.12.output.dense.weight\n",
      "roberta.encoder.layer.12.output.dense.bias\n",
      "roberta.encoder.layer.12.output.LayerNorm.weight\n",
      "roberta.encoder.layer.12.output.LayerNorm.bias\n",
      "roberta.encoder.layer.13.attention.self.query.weight\n",
      "roberta.encoder.layer.13.attention.self.query.bias\n",
      "roberta.encoder.layer.13.attention.self.key.weight\n",
      "roberta.encoder.layer.13.attention.self.key.bias\n",
      "roberta.encoder.layer.13.attention.self.value.weight\n",
      "roberta.encoder.layer.13.attention.self.value.bias\n",
      "roberta.encoder.layer.13.attention.output.dense.weight\n",
      "roberta.encoder.layer.13.attention.output.dense.bias\n",
      "roberta.encoder.layer.13.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.13.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.13.intermediate.dense.weight\n",
      "roberta.encoder.layer.13.intermediate.dense.bias\n",
      "roberta.encoder.layer.13.output.dense.weight\n",
      "roberta.encoder.layer.13.output.dense.bias\n",
      "roberta.encoder.layer.13.output.LayerNorm.weight\n",
      "roberta.encoder.layer.13.output.LayerNorm.bias\n",
      "roberta.encoder.layer.14.attention.self.query.weight\n",
      "roberta.encoder.layer.14.attention.self.query.bias\n",
      "roberta.encoder.layer.14.attention.self.key.weight\n",
      "roberta.encoder.layer.14.attention.self.key.bias\n",
      "roberta.encoder.layer.14.attention.self.value.weight\n",
      "roberta.encoder.layer.14.attention.self.value.bias\n",
      "roberta.encoder.layer.14.attention.output.dense.weight\n",
      "roberta.encoder.layer.14.attention.output.dense.bias\n",
      "roberta.encoder.layer.14.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.14.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.14.intermediate.dense.weight\n",
      "roberta.encoder.layer.14.intermediate.dense.bias\n",
      "roberta.encoder.layer.14.output.dense.weight\n",
      "roberta.encoder.layer.14.output.dense.bias\n",
      "roberta.encoder.layer.14.output.LayerNorm.weight\n",
      "roberta.encoder.layer.14.output.LayerNorm.bias\n",
      "roberta.encoder.layer.15.attention.self.query.weight\n",
      "roberta.encoder.layer.15.attention.self.query.bias\n",
      "roberta.encoder.layer.15.attention.self.key.weight\n",
      "roberta.encoder.layer.15.attention.self.key.bias\n",
      "roberta.encoder.layer.15.attention.self.value.weight\n",
      "roberta.encoder.layer.15.attention.self.value.bias\n",
      "roberta.encoder.layer.15.attention.output.dense.weight\n",
      "roberta.encoder.layer.15.attention.output.dense.bias\n",
      "roberta.encoder.layer.15.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.15.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.15.intermediate.dense.weight\n",
      "roberta.encoder.layer.15.intermediate.dense.bias\n",
      "roberta.encoder.layer.15.output.dense.weight\n",
      "roberta.encoder.layer.15.output.dense.bias\n",
      "roberta.encoder.layer.15.output.LayerNorm.weight\n",
      "roberta.encoder.layer.15.output.LayerNorm.bias\n",
      "roberta.encoder.layer.16.attention.self.query.weight\n",
      "roberta.encoder.layer.16.attention.self.query.bias\n",
      "roberta.encoder.layer.16.attention.self.key.weight\n",
      "roberta.encoder.layer.16.attention.self.key.bias\n",
      "roberta.encoder.layer.16.attention.self.value.weight\n",
      "roberta.encoder.layer.16.attention.self.value.bias\n",
      "roberta.encoder.layer.16.attention.output.dense.weight\n",
      "roberta.encoder.layer.16.attention.output.dense.bias\n",
      "roberta.encoder.layer.16.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.16.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.16.intermediate.dense.weight\n",
      "roberta.encoder.layer.16.intermediate.dense.bias\n",
      "roberta.encoder.layer.16.output.dense.weight\n",
      "roberta.encoder.layer.16.output.dense.bias\n",
      "roberta.encoder.layer.16.output.LayerNorm.weight\n",
      "roberta.encoder.layer.16.output.LayerNorm.bias\n",
      "roberta.encoder.layer.17.attention.self.query.weight\n",
      "roberta.encoder.layer.17.attention.self.query.bias\n",
      "roberta.encoder.layer.17.attention.self.key.weight\n",
      "roberta.encoder.layer.17.attention.self.key.bias\n",
      "roberta.encoder.layer.17.attention.self.value.weight\n",
      "roberta.encoder.layer.17.attention.self.value.bias\n",
      "roberta.encoder.layer.17.attention.output.dense.weight\n",
      "roberta.encoder.layer.17.attention.output.dense.bias\n",
      "roberta.encoder.layer.17.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.17.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.17.intermediate.dense.weight\n",
      "roberta.encoder.layer.17.intermediate.dense.bias\n",
      "roberta.encoder.layer.17.output.dense.weight\n",
      "roberta.encoder.layer.17.output.dense.bias\n",
      "roberta.encoder.layer.17.output.LayerNorm.weight\n",
      "roberta.encoder.layer.17.output.LayerNorm.bias\n",
      "roberta.encoder.layer.18.attention.self.query.weight\n",
      "roberta.encoder.layer.18.attention.self.query.bias\n",
      "roberta.encoder.layer.18.attention.self.key.weight\n",
      "roberta.encoder.layer.18.attention.self.key.bias\n",
      "roberta.encoder.layer.18.attention.self.value.weight\n",
      "roberta.encoder.layer.18.attention.self.value.bias\n",
      "roberta.encoder.layer.18.attention.output.dense.weight\n",
      "roberta.encoder.layer.18.attention.output.dense.bias\n",
      "roberta.encoder.layer.18.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.18.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.18.intermediate.dense.weight\n",
      "roberta.encoder.layer.18.intermediate.dense.bias\n",
      "roberta.encoder.layer.18.output.dense.weight\n",
      "roberta.encoder.layer.18.output.dense.bias\n",
      "roberta.encoder.layer.18.output.LayerNorm.weight\n",
      "roberta.encoder.layer.18.output.LayerNorm.bias\n",
      "roberta.encoder.layer.19.attention.self.query.weight\n",
      "roberta.encoder.layer.19.attention.self.query.bias\n",
      "roberta.encoder.layer.19.attention.self.key.weight\n",
      "roberta.encoder.layer.19.attention.self.key.bias\n",
      "roberta.encoder.layer.19.attention.self.value.weight\n",
      "roberta.encoder.layer.19.attention.self.value.bias\n",
      "roberta.encoder.layer.19.attention.output.dense.weight\n",
      "roberta.encoder.layer.19.attention.output.dense.bias\n",
      "roberta.encoder.layer.19.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.19.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.19.intermediate.dense.weight\n",
      "roberta.encoder.layer.19.intermediate.dense.bias\n",
      "roberta.encoder.layer.19.output.dense.weight\n",
      "roberta.encoder.layer.19.output.dense.bias\n",
      "roberta.encoder.layer.19.output.LayerNorm.weight\n",
      "roberta.encoder.layer.19.output.LayerNorm.bias\n",
      "roberta.encoder.layer.20.attention.self.query.weight\n",
      "roberta.encoder.layer.20.attention.self.query.bias\n",
      "roberta.encoder.layer.20.attention.self.key.weight\n",
      "roberta.encoder.layer.20.attention.self.key.bias\n",
      "roberta.encoder.layer.20.attention.self.value.weight\n",
      "roberta.encoder.layer.20.attention.self.value.bias\n",
      "roberta.encoder.layer.20.attention.output.dense.weight\n",
      "roberta.encoder.layer.20.attention.output.dense.bias\n",
      "roberta.encoder.layer.20.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.20.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.20.intermediate.dense.weight\n",
      "roberta.encoder.layer.20.intermediate.dense.bias\n",
      "roberta.encoder.layer.20.output.dense.weight\n",
      "roberta.encoder.layer.20.output.dense.bias\n",
      "roberta.encoder.layer.20.output.LayerNorm.weight\n",
      "roberta.encoder.layer.20.output.LayerNorm.bias\n",
      "roberta.encoder.layer.21.attention.self.query.weight\n",
      "roberta.encoder.layer.21.attention.self.query.bias\n",
      "roberta.encoder.layer.21.attention.self.key.weight\n",
      "roberta.encoder.layer.21.attention.self.key.bias\n",
      "roberta.encoder.layer.21.attention.self.value.weight\n",
      "roberta.encoder.layer.21.attention.self.value.bias\n",
      "roberta.encoder.layer.21.attention.output.dense.weight\n",
      "roberta.encoder.layer.21.attention.output.dense.bias\n",
      "roberta.encoder.layer.21.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.21.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.21.intermediate.dense.weight\n",
      "roberta.encoder.layer.21.intermediate.dense.bias\n",
      "roberta.encoder.layer.21.output.dense.weight\n",
      "roberta.encoder.layer.21.output.dense.bias\n",
      "roberta.encoder.layer.21.output.LayerNorm.weight\n",
      "roberta.encoder.layer.21.output.LayerNorm.bias\n",
      "roberta.encoder.layer.22.attention.self.query.weight\n",
      "roberta.encoder.layer.22.attention.self.query.bias\n",
      "roberta.encoder.layer.22.attention.self.key.weight\n",
      "roberta.encoder.layer.22.attention.self.key.bias\n",
      "roberta.encoder.layer.22.attention.self.value.weight\n",
      "roberta.encoder.layer.22.attention.self.value.bias\n",
      "roberta.encoder.layer.22.attention.output.dense.weight\n",
      "roberta.encoder.layer.22.attention.output.dense.bias\n",
      "roberta.encoder.layer.22.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.22.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.22.intermediate.dense.weight\n",
      "roberta.encoder.layer.22.intermediate.dense.bias\n",
      "roberta.encoder.layer.22.output.dense.weight\n",
      "roberta.encoder.layer.22.output.dense.bias\n",
      "roberta.encoder.layer.22.output.LayerNorm.weight\n",
      "roberta.encoder.layer.22.output.LayerNorm.bias\n",
      "roberta.encoder.layer.23.attention.self.query.weight\n",
      "roberta.encoder.layer.23.attention.self.query.bias\n",
      "roberta.encoder.layer.23.attention.self.key.weight\n",
      "roberta.encoder.layer.23.attention.self.key.bias\n",
      "roberta.encoder.layer.23.attention.self.value.weight\n",
      "roberta.encoder.layer.23.attention.self.value.bias\n",
      "roberta.encoder.layer.23.attention.output.dense.weight\n",
      "roberta.encoder.layer.23.attention.output.dense.bias\n",
      "roberta.encoder.layer.23.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.23.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.23.intermediate.dense.weight\n",
      "roberta.encoder.layer.23.intermediate.dense.bias\n",
      "roberta.encoder.layer.23.output.dense.weight\n",
      "roberta.encoder.layer.23.output.dense.bias\n",
      "roberta.encoder.layer.23.output.LayerNorm.weight\n",
      "roberta.encoder.layer.23.output.LayerNorm.bias\n",
      "classifier.dense.weight\n",
      "classifier.dense.bias\n",
      "classifier.out_proj.weight\n",
      "classifier.out_proj.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in teacher_model2.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only classifier layer trainable and the rest will be remained frozen\n",
    "for name, param in teacher_model2.named_parameters():\n",
    "    if \"classifier\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31c6826413546dbba3e30cc54eb3c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3595 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3933fb062044c9a900028f0d87caefd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/857 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a85ef4da192410f85ec54128a945bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "tokenized_data = dataset_dict.map(lambda x: tokenizer_teacher2(x[\"text\"]), batched=True)\n",
    "\n",
    "# Define Accuracy Metric\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2250/2250 05:22, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.894100</td>\n",
       "      <td>1.694619</td>\n",
       "      <td>0.389732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.645400</td>\n",
       "      <td>1.447400</td>\n",
       "      <td>0.408401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.475800</td>\n",
       "      <td>1.229917</td>\n",
       "      <td>0.627771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.370900</td>\n",
       "      <td>1.194873</td>\n",
       "      <td>0.572929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.288000</td>\n",
       "      <td>0.982774</td>\n",
       "      <td>0.694282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.225600</td>\n",
       "      <td>1.036065</td>\n",
       "      <td>0.638273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.169400</td>\n",
       "      <td>0.928050</td>\n",
       "      <td>0.708285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.106200</td>\n",
       "      <td>0.879331</td>\n",
       "      <td>0.726954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.081800</td>\n",
       "      <td>0.868958</td>\n",
       "      <td>0.747958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.050300</td>\n",
       "      <td>0.838500</td>\n",
       "      <td>0.746791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2250, training_loss=1.3307618476019965, metrics={'train_runtime': 322.4626, 'train_samples_per_second': 111.486, 'train_steps_per_second': 6.978, 'total_flos': 3894661567799532.0, 'train_loss': 1.3307618476019965, 'epoch': 10.0})"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training HyperParameters\n",
    "lr = 1e-3\n",
    "weight_decay = 0.01\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "\n",
    "teacher_training_args2 = TrainingArguments(\n",
    "    # output_dir=\"bert-category-classifier-teacher\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "data_collator2 = DataCollatorWithPadding(tokenizer=tokenizer_teacher2)\n",
    "# Use the custom compute_loss function in Trainer\n",
    "trainer2 = Trainer(\n",
    "    model=teacher_model2,\n",
    "    args=teacher_training_args2,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    processing_class=tokenizer_teacher2,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator2\n",
    ")\n",
    "\n",
    "trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "tokenized_val_data = tokenizer_teacher2(\n",
    "    dataset_dict[\"validation\"][\"text\"], \n",
    "    padding=True, \n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer_teacher2, return_tensors=\"pt\")\n",
    "batch_inputs = data_collator(tokenized_val_data)\n",
    "\n",
    "teacher_model2.eval()\n",
    "with torch.no_grad():\n",
    "    teacher_outputs2 = teacher_model2.to(\"cpu\")(**batch_inputs)\n",
    "teacher_logits2 = teacher_outputs2.logits\n",
    "teacher_preds2 = torch.softmax(teacher_logits2, dim=1).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7638483965014577}"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(eval_pred=(teacher_logits2.numpy(), dataset_dict[\"validation\"][\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_models/fine_tuned_roberta_teacher_model2/tokenizer_config.json',\n",
       " './fine_tuned_models/fine_tuned_roberta_teacher_model2/special_tokens_map.json',\n",
       " './fine_tuned_models/fine_tuned_roberta_teacher_model2/sentencepiece.bpe.model',\n",
       " './fine_tuned_models/fine_tuned_roberta_teacher_model2/added_tokens.json',\n",
       " './fine_tuned_models/fine_tuned_roberta_teacher_model2/tokenizer.json')"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_directory = \"./fine_tuned_models/fine_tuned_roberta_teacher_model2\"\n",
    "\n",
    "# Save model\n",
    "teacher_model2.save_pretrained(save_directory)\n",
    "# Save tokenizer\n",
    "tokenizer_teacher2.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Fine Tuning Student Model with Fine-Tuned Teacher Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Student Model\n",
    "student_model_id = 'bert-base-multilingual-cased'\n",
    "student_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    student_model_id,\n",
    "    num_labels=7,\n",
    "    id2label=id2category,\n",
    "    label2id=category2id\n",
    ")\n",
    "tokenizer_student = AutoTokenizer.from_pretrained(student_model_id)\n",
    "tokenizer_student.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight\n",
      "bert.embeddings.LayerNorm.bias\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "bert.encoder.layer.0.attention.self.query.bias\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "bert.encoder.layer.0.attention.self.key.bias\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "bert.encoder.layer.0.attention.self.value.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "bert.encoder.layer.0.intermediate.dense.bias\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "bert.encoder.layer.0.output.dense.bias\n",
      "bert.encoder.layer.0.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "bert.encoder.layer.1.attention.self.query.bias\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "bert.encoder.layer.1.attention.self.key.bias\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "bert.encoder.layer.1.attention.self.value.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "bert.encoder.layer.1.intermediate.dense.bias\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "bert.encoder.layer.1.output.dense.bias\n",
      "bert.encoder.layer.1.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.attention.self.query.weight\n",
      "bert.encoder.layer.2.attention.self.query.bias\n",
      "bert.encoder.layer.2.attention.self.key.weight\n",
      "bert.encoder.layer.2.attention.self.key.bias\n",
      "bert.encoder.layer.2.attention.self.value.weight\n",
      "bert.encoder.layer.2.attention.self.value.bias\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "bert.encoder.layer.2.attention.output.dense.bias\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.intermediate.dense.weight\n",
      "bert.encoder.layer.2.intermediate.dense.bias\n",
      "bert.encoder.layer.2.output.dense.weight\n",
      "bert.encoder.layer.2.output.dense.bias\n",
      "bert.encoder.layer.2.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.attention.self.query.weight\n",
      "bert.encoder.layer.3.attention.self.query.bias\n",
      "bert.encoder.layer.3.attention.self.key.weight\n",
      "bert.encoder.layer.3.attention.self.key.bias\n",
      "bert.encoder.layer.3.attention.self.value.weight\n",
      "bert.encoder.layer.3.attention.self.value.bias\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "bert.encoder.layer.3.attention.output.dense.bias\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.intermediate.dense.weight\n",
      "bert.encoder.layer.3.intermediate.dense.bias\n",
      "bert.encoder.layer.3.output.dense.weight\n",
      "bert.encoder.layer.3.output.dense.bias\n",
      "bert.encoder.layer.3.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.attention.self.query.weight\n",
      "bert.encoder.layer.4.attention.self.query.bias\n",
      "bert.encoder.layer.4.attention.self.key.weight\n",
      "bert.encoder.layer.4.attention.self.key.bias\n",
      "bert.encoder.layer.4.attention.self.value.weight\n",
      "bert.encoder.layer.4.attention.self.value.bias\n",
      "bert.encoder.layer.4.attention.output.dense.weight\n",
      "bert.encoder.layer.4.attention.output.dense.bias\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.intermediate.dense.weight\n",
      "bert.encoder.layer.4.intermediate.dense.bias\n",
      "bert.encoder.layer.4.output.dense.weight\n",
      "bert.encoder.layer.4.output.dense.bias\n",
      "bert.encoder.layer.4.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.attention.self.query.weight\n",
      "bert.encoder.layer.5.attention.self.query.bias\n",
      "bert.encoder.layer.5.attention.self.key.weight\n",
      "bert.encoder.layer.5.attention.self.key.bias\n",
      "bert.encoder.layer.5.attention.self.value.weight\n",
      "bert.encoder.layer.5.attention.self.value.bias\n",
      "bert.encoder.layer.5.attention.output.dense.weight\n",
      "bert.encoder.layer.5.attention.output.dense.bias\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.intermediate.dense.weight\n",
      "bert.encoder.layer.5.intermediate.dense.bias\n",
      "bert.encoder.layer.5.output.dense.weight\n",
      "bert.encoder.layer.5.output.dense.bias\n",
      "bert.encoder.layer.5.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.attention.self.query.weight\n",
      "bert.encoder.layer.6.attention.self.query.bias\n",
      "bert.encoder.layer.6.attention.self.key.weight\n",
      "bert.encoder.layer.6.attention.self.key.bias\n",
      "bert.encoder.layer.6.attention.self.value.weight\n",
      "bert.encoder.layer.6.attention.self.value.bias\n",
      "bert.encoder.layer.6.attention.output.dense.weight\n",
      "bert.encoder.layer.6.attention.output.dense.bias\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.intermediate.dense.weight\n",
      "bert.encoder.layer.6.intermediate.dense.bias\n",
      "bert.encoder.layer.6.output.dense.weight\n",
      "bert.encoder.layer.6.output.dense.bias\n",
      "bert.encoder.layer.6.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.attention.self.query.weight\n",
      "bert.encoder.layer.7.attention.self.query.bias\n",
      "bert.encoder.layer.7.attention.self.key.weight\n",
      "bert.encoder.layer.7.attention.self.key.bias\n",
      "bert.encoder.layer.7.attention.self.value.weight\n",
      "bert.encoder.layer.7.attention.self.value.bias\n",
      "bert.encoder.layer.7.attention.output.dense.weight\n",
      "bert.encoder.layer.7.attention.output.dense.bias\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.intermediate.dense.weight\n",
      "bert.encoder.layer.7.intermediate.dense.bias\n",
      "bert.encoder.layer.7.output.dense.weight\n",
      "bert.encoder.layer.7.output.dense.bias\n",
      "bert.encoder.layer.7.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.attention.self.query.weight\n",
      "bert.encoder.layer.8.attention.self.query.bias\n",
      "bert.encoder.layer.8.attention.self.key.weight\n",
      "bert.encoder.layer.8.attention.self.key.bias\n",
      "bert.encoder.layer.8.attention.self.value.weight\n",
      "bert.encoder.layer.8.attention.self.value.bias\n",
      "bert.encoder.layer.8.attention.output.dense.weight\n",
      "bert.encoder.layer.8.attention.output.dense.bias\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.intermediate.dense.weight\n",
      "bert.encoder.layer.8.intermediate.dense.bias\n",
      "bert.encoder.layer.8.output.dense.weight\n",
      "bert.encoder.layer.8.output.dense.bias\n",
      "bert.encoder.layer.8.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.attention.self.query.weight\n",
      "bert.encoder.layer.9.attention.self.query.bias\n",
      "bert.encoder.layer.9.attention.self.key.weight\n",
      "bert.encoder.layer.9.attention.self.key.bias\n",
      "bert.encoder.layer.9.attention.self.value.weight\n",
      "bert.encoder.layer.9.attention.self.value.bias\n",
      "bert.encoder.layer.9.attention.output.dense.weight\n",
      "bert.encoder.layer.9.attention.output.dense.bias\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.intermediate.dense.weight\n",
      "bert.encoder.layer.9.intermediate.dense.bias\n",
      "bert.encoder.layer.9.output.dense.weight\n",
      "bert.encoder.layer.9.output.dense.bias\n",
      "bert.encoder.layer.9.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.output.LayerNorm.bias\n",
      "bert.pooler.dense.weight\n",
      "bert.pooler.dense.bias\n",
      "classifier.weight\n",
      "classifier.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in student_model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all student model parameters except pooler and classifier layers\n",
    "for name, param in student_model.named_parameters():\n",
    "    if (\"pooler\" in name) | (\"classifier\" in name):\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6fe89f2e52f40ca9529ff8fc3fd3b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3595 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f1dee4a7ba459a8d7c2e84faeddf70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/857 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7552882226d2425aa7f6e6007203e32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "tokenized_data = dataset_dict.map(lambda x: tokenizer_student(x[\"text\"]), batched=True)\n",
    "\n",
    "# Define Accuracy Metric\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = torch.softmax(torch.tensor(logits, dtype=torch.float32), dim=1).argmax(dim=1)\n",
    "    acc = accuracy.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "# Custom Trainer Class for Knowledge Distillation\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        # Get student model's input features\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        labels = inputs[\"labels\"]\n",
    "        \n",
    "        # Get teacher's logits\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = self.teacher_model(input_ids).logits\n",
    "\n",
    "        # Forward pass through student model\n",
    "        student_outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        student_logits = student_outputs.logits\n",
    "\n",
    "        # Compute the distillation loss (KL Divergence)\n",
    "        student_probs = torch.softmax(student_logits / 2.0, dim=1)  # Temperature=2\n",
    "        teacher_probs = torch.softmax(teacher_logits / 2.0, dim=1)\n",
    "        if num_items_in_batch is not None:\n",
    "            kl_loss = F.kl_div(student_probs.log(), teacher_probs, reduction='batchmean')\n",
    "        else:\n",
    "            kl_loss = F.kl_div(student_probs.log(), teacher_probs, reduction=\"none\")\n",
    "        \n",
    "        # Compute Cross-Entropy loss\n",
    "        ce_loss = torch.nn.functional.cross_entropy(student_logits, labels)\n",
    "        \n",
    "        # Compute weighted fina loss\n",
    "        alpha = 0.5\n",
    "        final_loss = alpha * kl_loss + (1-alpha) * ce_loss\n",
    "\n",
    "        return (final_loss, student_outputs) if return_outputs else final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2250/2250 06:55, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.661900</td>\n",
       "      <td>0.394438</td>\n",
       "      <td>0.786464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.560200</td>\n",
       "      <td>0.371054</td>\n",
       "      <td>0.803967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.539600</td>\n",
       "      <td>0.367315</td>\n",
       "      <td>0.807468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.524200</td>\n",
       "      <td>0.380781</td>\n",
       "      <td>0.786464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.525400</td>\n",
       "      <td>0.360209</td>\n",
       "      <td>0.812135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.516600</td>\n",
       "      <td>0.365204</td>\n",
       "      <td>0.805134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.499100</td>\n",
       "      <td>0.358351</td>\n",
       "      <td>0.815636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.500300</td>\n",
       "      <td>0.360284</td>\n",
       "      <td>0.808635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.492700</td>\n",
       "      <td>0.349690</td>\n",
       "      <td>0.819137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.480500</td>\n",
       "      <td>0.348302</td>\n",
       "      <td>0.822637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2250, training_loss=0.5300369160970052, metrics={'train_runtime': 416.1976, 'train_samples_per_second': 86.377, 'train_steps_per_second': 5.406, 'total_flos': 1152083474035200.0, 'train_loss': 0.5300369160970052, 'epoch': 10.0})"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training HyperParameters\n",
    "lr = 1e-3\n",
    "weight_decay = 0.01\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "\n",
    "training_args_student = TrainingArguments(\n",
    "    # output_dir=\"bert-category-classifier-student\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "data_collator_student = DataCollatorWithPadding(tokenizer=tokenizer_student)\n",
    "# Use the custom compute_loss function in Trainer\n",
    "trainer_student = DistillationTrainer(\n",
    "    model=student_model,\n",
    "    args=training_args_student,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator_student,\n",
    "    teacher_model=teacher_model2\n",
    ")\n",
    "\n",
    "trainer_student.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "tokenized_val_data = tokenizer_student(\n",
    "    dataset_dict[\"validation\"][\"text\"], \n",
    "    padding=False, \n",
    "    truncation=True,\n",
    "    # return_tensors=\"pt\",\n",
    ")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer_student, return_tensors=\"pt\")\n",
    "batch_inputs = data_collator(tokenized_val_data)\n",
    "\n",
    "student_model.eval()\n",
    "with torch.no_grad():\n",
    "    student_outputs = student_model.to(\"cpu\")(**batch_inputs)\n",
    "student_logits = student_outputs.logits\n",
    "student_preds = torch.softmax(student_logits, dim=1).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8688046647230321}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(eval_pred=(student_logits.numpy(), dataset_dict[\"validation\"][\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_models/fine_tuned_bert_student_model/tokenizer_config.json',\n",
       " './fine_tuned_models/fine_tuned_bert_student_model/special_tokens_map.json',\n",
       " './fine_tuned_models/fine_tuned_bert_student_model/vocab.txt',\n",
       " './fine_tuned_models/fine_tuned_bert_student_model/added_tokens.json',\n",
       " './fine_tuned_models/fine_tuned_bert_student_model/tokenizer.json')"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_directory = \"./fine_tuned_models/fine_tuned_bert_student_model\"\n",
    "\n",
    "# Save model\n",
    "student_model.save_pretrained(save_directory)\n",
    "# Save tokenizer\n",
    "tokenizer_student.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Efficient Fine-Tuning (PEFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Low-Rank Adaptation (LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Teacher Model \n",
    "model_wout_lora_id = 'xlm-roberta-large'\n",
    "model_wout_lora = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_wout_lora_id,\n",
    "    num_labels=7,\n",
    "    id2label=id2category,\n",
    "    label2id=category2id\n",
    ")\n",
    "tokenizer_lora = AutoTokenizer.from_pretrained(model_wout_lora_id)\n",
    "# tokenizer_teacher2.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.embeddings.word_embeddings.weight\n",
      "roberta.embeddings.position_embeddings.weight\n",
      "roberta.embeddings.token_type_embeddings.weight\n",
      "roberta.embeddings.LayerNorm.weight\n",
      "roberta.embeddings.LayerNorm.bias\n",
      "roberta.encoder.layer.0.attention.self.query.weight\n",
      "roberta.encoder.layer.0.attention.self.query.bias\n",
      "roberta.encoder.layer.0.attention.self.key.weight\n",
      "roberta.encoder.layer.0.attention.self.key.bias\n",
      "roberta.encoder.layer.0.attention.self.value.weight\n",
      "roberta.encoder.layer.0.attention.self.value.bias\n",
      "roberta.encoder.layer.0.attention.output.dense.weight\n",
      "roberta.encoder.layer.0.attention.output.dense.bias\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.0.intermediate.dense.weight\n",
      "roberta.encoder.layer.0.intermediate.dense.bias\n",
      "roberta.encoder.layer.0.output.dense.weight\n",
      "roberta.encoder.layer.0.output.dense.bias\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias\n",
      "roberta.encoder.layer.1.attention.self.query.weight\n",
      "roberta.encoder.layer.1.attention.self.query.bias\n",
      "roberta.encoder.layer.1.attention.self.key.weight\n",
      "roberta.encoder.layer.1.attention.self.key.bias\n",
      "roberta.encoder.layer.1.attention.self.value.weight\n",
      "roberta.encoder.layer.1.attention.self.value.bias\n",
      "roberta.encoder.layer.1.attention.output.dense.weight\n",
      "roberta.encoder.layer.1.attention.output.dense.bias\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.1.intermediate.dense.weight\n",
      "roberta.encoder.layer.1.intermediate.dense.bias\n",
      "roberta.encoder.layer.1.output.dense.weight\n",
      "roberta.encoder.layer.1.output.dense.bias\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.attention.self.query.weight\n",
      "roberta.encoder.layer.2.attention.self.query.bias\n",
      "roberta.encoder.layer.2.attention.self.key.weight\n",
      "roberta.encoder.layer.2.attention.self.key.bias\n",
      "roberta.encoder.layer.2.attention.self.value.weight\n",
      "roberta.encoder.layer.2.attention.self.value.bias\n",
      "roberta.encoder.layer.2.attention.output.dense.weight\n",
      "roberta.encoder.layer.2.attention.output.dense.bias\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.intermediate.dense.weight\n",
      "roberta.encoder.layer.2.intermediate.dense.bias\n",
      "roberta.encoder.layer.2.output.dense.weight\n",
      "roberta.encoder.layer.2.output.dense.bias\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.attention.self.query.weight\n",
      "roberta.encoder.layer.3.attention.self.query.bias\n",
      "roberta.encoder.layer.3.attention.self.key.weight\n",
      "roberta.encoder.layer.3.attention.self.key.bias\n",
      "roberta.encoder.layer.3.attention.self.value.weight\n",
      "roberta.encoder.layer.3.attention.self.value.bias\n",
      "roberta.encoder.layer.3.attention.output.dense.weight\n",
      "roberta.encoder.layer.3.attention.output.dense.bias\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.intermediate.dense.weight\n",
      "roberta.encoder.layer.3.intermediate.dense.bias\n",
      "roberta.encoder.layer.3.output.dense.weight\n",
      "roberta.encoder.layer.3.output.dense.bias\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.attention.self.query.weight\n",
      "roberta.encoder.layer.4.attention.self.query.bias\n",
      "roberta.encoder.layer.4.attention.self.key.weight\n",
      "roberta.encoder.layer.4.attention.self.key.bias\n",
      "roberta.encoder.layer.4.attention.self.value.weight\n",
      "roberta.encoder.layer.4.attention.self.value.bias\n",
      "roberta.encoder.layer.4.attention.output.dense.weight\n",
      "roberta.encoder.layer.4.attention.output.dense.bias\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.intermediate.dense.weight\n",
      "roberta.encoder.layer.4.intermediate.dense.bias\n",
      "roberta.encoder.layer.4.output.dense.weight\n",
      "roberta.encoder.layer.4.output.dense.bias\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.attention.self.query.weight\n",
      "roberta.encoder.layer.5.attention.self.query.bias\n",
      "roberta.encoder.layer.5.attention.self.key.weight\n",
      "roberta.encoder.layer.5.attention.self.key.bias\n",
      "roberta.encoder.layer.5.attention.self.value.weight\n",
      "roberta.encoder.layer.5.attention.self.value.bias\n",
      "roberta.encoder.layer.5.attention.output.dense.weight\n",
      "roberta.encoder.layer.5.attention.output.dense.bias\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.intermediate.dense.weight\n",
      "roberta.encoder.layer.5.intermediate.dense.bias\n",
      "roberta.encoder.layer.5.output.dense.weight\n",
      "roberta.encoder.layer.5.output.dense.bias\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.attention.self.query.weight\n",
      "roberta.encoder.layer.6.attention.self.query.bias\n",
      "roberta.encoder.layer.6.attention.self.key.weight\n",
      "roberta.encoder.layer.6.attention.self.key.bias\n",
      "roberta.encoder.layer.6.attention.self.value.weight\n",
      "roberta.encoder.layer.6.attention.self.value.bias\n",
      "roberta.encoder.layer.6.attention.output.dense.weight\n",
      "roberta.encoder.layer.6.attention.output.dense.bias\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.intermediate.dense.weight\n",
      "roberta.encoder.layer.6.intermediate.dense.bias\n",
      "roberta.encoder.layer.6.output.dense.weight\n",
      "roberta.encoder.layer.6.output.dense.bias\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.attention.self.query.weight\n",
      "roberta.encoder.layer.7.attention.self.query.bias\n",
      "roberta.encoder.layer.7.attention.self.key.weight\n",
      "roberta.encoder.layer.7.attention.self.key.bias\n",
      "roberta.encoder.layer.7.attention.self.value.weight\n",
      "roberta.encoder.layer.7.attention.self.value.bias\n",
      "roberta.encoder.layer.7.attention.output.dense.weight\n",
      "roberta.encoder.layer.7.attention.output.dense.bias\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.intermediate.dense.weight\n",
      "roberta.encoder.layer.7.intermediate.dense.bias\n",
      "roberta.encoder.layer.7.output.dense.weight\n",
      "roberta.encoder.layer.7.output.dense.bias\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.attention.self.query.weight\n",
      "roberta.encoder.layer.8.attention.self.query.bias\n",
      "roberta.encoder.layer.8.attention.self.key.weight\n",
      "roberta.encoder.layer.8.attention.self.key.bias\n",
      "roberta.encoder.layer.8.attention.self.value.weight\n",
      "roberta.encoder.layer.8.attention.self.value.bias\n",
      "roberta.encoder.layer.8.attention.output.dense.weight\n",
      "roberta.encoder.layer.8.attention.output.dense.bias\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.intermediate.dense.weight\n",
      "roberta.encoder.layer.8.intermediate.dense.bias\n",
      "roberta.encoder.layer.8.output.dense.weight\n",
      "roberta.encoder.layer.8.output.dense.bias\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.attention.self.query.weight\n",
      "roberta.encoder.layer.9.attention.self.query.bias\n",
      "roberta.encoder.layer.9.attention.self.key.weight\n",
      "roberta.encoder.layer.9.attention.self.key.bias\n",
      "roberta.encoder.layer.9.attention.self.value.weight\n",
      "roberta.encoder.layer.9.attention.self.value.bias\n",
      "roberta.encoder.layer.9.attention.output.dense.weight\n",
      "roberta.encoder.layer.9.attention.output.dense.bias\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.intermediate.dense.weight\n",
      "roberta.encoder.layer.9.intermediate.dense.bias\n",
      "roberta.encoder.layer.9.output.dense.weight\n",
      "roberta.encoder.layer.9.output.dense.bias\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.attention.self.query.weight\n",
      "roberta.encoder.layer.10.attention.self.query.bias\n",
      "roberta.encoder.layer.10.attention.self.key.weight\n",
      "roberta.encoder.layer.10.attention.self.key.bias\n",
      "roberta.encoder.layer.10.attention.self.value.weight\n",
      "roberta.encoder.layer.10.attention.self.value.bias\n",
      "roberta.encoder.layer.10.attention.output.dense.weight\n",
      "roberta.encoder.layer.10.attention.output.dense.bias\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.intermediate.dense.weight\n",
      "roberta.encoder.layer.10.intermediate.dense.bias\n",
      "roberta.encoder.layer.10.output.dense.weight\n",
      "roberta.encoder.layer.10.output.dense.bias\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.attention.self.query.weight\n",
      "roberta.encoder.layer.11.attention.self.query.bias\n",
      "roberta.encoder.layer.11.attention.self.key.weight\n",
      "roberta.encoder.layer.11.attention.self.key.bias\n",
      "roberta.encoder.layer.11.attention.self.value.weight\n",
      "roberta.encoder.layer.11.attention.self.value.bias\n",
      "roberta.encoder.layer.11.attention.output.dense.weight\n",
      "roberta.encoder.layer.11.attention.output.dense.bias\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.intermediate.dense.weight\n",
      "roberta.encoder.layer.11.intermediate.dense.bias\n",
      "roberta.encoder.layer.11.output.dense.weight\n",
      "roberta.encoder.layer.11.output.dense.bias\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "roberta.encoder.layer.12.attention.self.query.weight\n",
      "roberta.encoder.layer.12.attention.self.query.bias\n",
      "roberta.encoder.layer.12.attention.self.key.weight\n",
      "roberta.encoder.layer.12.attention.self.key.bias\n",
      "roberta.encoder.layer.12.attention.self.value.weight\n",
      "roberta.encoder.layer.12.attention.self.value.bias\n",
      "roberta.encoder.layer.12.attention.output.dense.weight\n",
      "roberta.encoder.layer.12.attention.output.dense.bias\n",
      "roberta.encoder.layer.12.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.12.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.12.intermediate.dense.weight\n",
      "roberta.encoder.layer.12.intermediate.dense.bias\n",
      "roberta.encoder.layer.12.output.dense.weight\n",
      "roberta.encoder.layer.12.output.dense.bias\n",
      "roberta.encoder.layer.12.output.LayerNorm.weight\n",
      "roberta.encoder.layer.12.output.LayerNorm.bias\n",
      "roberta.encoder.layer.13.attention.self.query.weight\n",
      "roberta.encoder.layer.13.attention.self.query.bias\n",
      "roberta.encoder.layer.13.attention.self.key.weight\n",
      "roberta.encoder.layer.13.attention.self.key.bias\n",
      "roberta.encoder.layer.13.attention.self.value.weight\n",
      "roberta.encoder.layer.13.attention.self.value.bias\n",
      "roberta.encoder.layer.13.attention.output.dense.weight\n",
      "roberta.encoder.layer.13.attention.output.dense.bias\n",
      "roberta.encoder.layer.13.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.13.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.13.intermediate.dense.weight\n",
      "roberta.encoder.layer.13.intermediate.dense.bias\n",
      "roberta.encoder.layer.13.output.dense.weight\n",
      "roberta.encoder.layer.13.output.dense.bias\n",
      "roberta.encoder.layer.13.output.LayerNorm.weight\n",
      "roberta.encoder.layer.13.output.LayerNorm.bias\n",
      "roberta.encoder.layer.14.attention.self.query.weight\n",
      "roberta.encoder.layer.14.attention.self.query.bias\n",
      "roberta.encoder.layer.14.attention.self.key.weight\n",
      "roberta.encoder.layer.14.attention.self.key.bias\n",
      "roberta.encoder.layer.14.attention.self.value.weight\n",
      "roberta.encoder.layer.14.attention.self.value.bias\n",
      "roberta.encoder.layer.14.attention.output.dense.weight\n",
      "roberta.encoder.layer.14.attention.output.dense.bias\n",
      "roberta.encoder.layer.14.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.14.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.14.intermediate.dense.weight\n",
      "roberta.encoder.layer.14.intermediate.dense.bias\n",
      "roberta.encoder.layer.14.output.dense.weight\n",
      "roberta.encoder.layer.14.output.dense.bias\n",
      "roberta.encoder.layer.14.output.LayerNorm.weight\n",
      "roberta.encoder.layer.14.output.LayerNorm.bias\n",
      "roberta.encoder.layer.15.attention.self.query.weight\n",
      "roberta.encoder.layer.15.attention.self.query.bias\n",
      "roberta.encoder.layer.15.attention.self.key.weight\n",
      "roberta.encoder.layer.15.attention.self.key.bias\n",
      "roberta.encoder.layer.15.attention.self.value.weight\n",
      "roberta.encoder.layer.15.attention.self.value.bias\n",
      "roberta.encoder.layer.15.attention.output.dense.weight\n",
      "roberta.encoder.layer.15.attention.output.dense.bias\n",
      "roberta.encoder.layer.15.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.15.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.15.intermediate.dense.weight\n",
      "roberta.encoder.layer.15.intermediate.dense.bias\n",
      "roberta.encoder.layer.15.output.dense.weight\n",
      "roberta.encoder.layer.15.output.dense.bias\n",
      "roberta.encoder.layer.15.output.LayerNorm.weight\n",
      "roberta.encoder.layer.15.output.LayerNorm.bias\n",
      "roberta.encoder.layer.16.attention.self.query.weight\n",
      "roberta.encoder.layer.16.attention.self.query.bias\n",
      "roberta.encoder.layer.16.attention.self.key.weight\n",
      "roberta.encoder.layer.16.attention.self.key.bias\n",
      "roberta.encoder.layer.16.attention.self.value.weight\n",
      "roberta.encoder.layer.16.attention.self.value.bias\n",
      "roberta.encoder.layer.16.attention.output.dense.weight\n",
      "roberta.encoder.layer.16.attention.output.dense.bias\n",
      "roberta.encoder.layer.16.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.16.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.16.intermediate.dense.weight\n",
      "roberta.encoder.layer.16.intermediate.dense.bias\n",
      "roberta.encoder.layer.16.output.dense.weight\n",
      "roberta.encoder.layer.16.output.dense.bias\n",
      "roberta.encoder.layer.16.output.LayerNorm.weight\n",
      "roberta.encoder.layer.16.output.LayerNorm.bias\n",
      "roberta.encoder.layer.17.attention.self.query.weight\n",
      "roberta.encoder.layer.17.attention.self.query.bias\n",
      "roberta.encoder.layer.17.attention.self.key.weight\n",
      "roberta.encoder.layer.17.attention.self.key.bias\n",
      "roberta.encoder.layer.17.attention.self.value.weight\n",
      "roberta.encoder.layer.17.attention.self.value.bias\n",
      "roberta.encoder.layer.17.attention.output.dense.weight\n",
      "roberta.encoder.layer.17.attention.output.dense.bias\n",
      "roberta.encoder.layer.17.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.17.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.17.intermediate.dense.weight\n",
      "roberta.encoder.layer.17.intermediate.dense.bias\n",
      "roberta.encoder.layer.17.output.dense.weight\n",
      "roberta.encoder.layer.17.output.dense.bias\n",
      "roberta.encoder.layer.17.output.LayerNorm.weight\n",
      "roberta.encoder.layer.17.output.LayerNorm.bias\n",
      "roberta.encoder.layer.18.attention.self.query.weight\n",
      "roberta.encoder.layer.18.attention.self.query.bias\n",
      "roberta.encoder.layer.18.attention.self.key.weight\n",
      "roberta.encoder.layer.18.attention.self.key.bias\n",
      "roberta.encoder.layer.18.attention.self.value.weight\n",
      "roberta.encoder.layer.18.attention.self.value.bias\n",
      "roberta.encoder.layer.18.attention.output.dense.weight\n",
      "roberta.encoder.layer.18.attention.output.dense.bias\n",
      "roberta.encoder.layer.18.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.18.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.18.intermediate.dense.weight\n",
      "roberta.encoder.layer.18.intermediate.dense.bias\n",
      "roberta.encoder.layer.18.output.dense.weight\n",
      "roberta.encoder.layer.18.output.dense.bias\n",
      "roberta.encoder.layer.18.output.LayerNorm.weight\n",
      "roberta.encoder.layer.18.output.LayerNorm.bias\n",
      "roberta.encoder.layer.19.attention.self.query.weight\n",
      "roberta.encoder.layer.19.attention.self.query.bias\n",
      "roberta.encoder.layer.19.attention.self.key.weight\n",
      "roberta.encoder.layer.19.attention.self.key.bias\n",
      "roberta.encoder.layer.19.attention.self.value.weight\n",
      "roberta.encoder.layer.19.attention.self.value.bias\n",
      "roberta.encoder.layer.19.attention.output.dense.weight\n",
      "roberta.encoder.layer.19.attention.output.dense.bias\n",
      "roberta.encoder.layer.19.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.19.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.19.intermediate.dense.weight\n",
      "roberta.encoder.layer.19.intermediate.dense.bias\n",
      "roberta.encoder.layer.19.output.dense.weight\n",
      "roberta.encoder.layer.19.output.dense.bias\n",
      "roberta.encoder.layer.19.output.LayerNorm.weight\n",
      "roberta.encoder.layer.19.output.LayerNorm.bias\n",
      "roberta.encoder.layer.20.attention.self.query.weight\n",
      "roberta.encoder.layer.20.attention.self.query.bias\n",
      "roberta.encoder.layer.20.attention.self.key.weight\n",
      "roberta.encoder.layer.20.attention.self.key.bias\n",
      "roberta.encoder.layer.20.attention.self.value.weight\n",
      "roberta.encoder.layer.20.attention.self.value.bias\n",
      "roberta.encoder.layer.20.attention.output.dense.weight\n",
      "roberta.encoder.layer.20.attention.output.dense.bias\n",
      "roberta.encoder.layer.20.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.20.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.20.intermediate.dense.weight\n",
      "roberta.encoder.layer.20.intermediate.dense.bias\n",
      "roberta.encoder.layer.20.output.dense.weight\n",
      "roberta.encoder.layer.20.output.dense.bias\n",
      "roberta.encoder.layer.20.output.LayerNorm.weight\n",
      "roberta.encoder.layer.20.output.LayerNorm.bias\n",
      "roberta.encoder.layer.21.attention.self.query.weight\n",
      "roberta.encoder.layer.21.attention.self.query.bias\n",
      "roberta.encoder.layer.21.attention.self.key.weight\n",
      "roberta.encoder.layer.21.attention.self.key.bias\n",
      "roberta.encoder.layer.21.attention.self.value.weight\n",
      "roberta.encoder.layer.21.attention.self.value.bias\n",
      "roberta.encoder.layer.21.attention.output.dense.weight\n",
      "roberta.encoder.layer.21.attention.output.dense.bias\n",
      "roberta.encoder.layer.21.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.21.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.21.intermediate.dense.weight\n",
      "roberta.encoder.layer.21.intermediate.dense.bias\n",
      "roberta.encoder.layer.21.output.dense.weight\n",
      "roberta.encoder.layer.21.output.dense.bias\n",
      "roberta.encoder.layer.21.output.LayerNorm.weight\n",
      "roberta.encoder.layer.21.output.LayerNorm.bias\n",
      "roberta.encoder.layer.22.attention.self.query.weight\n",
      "roberta.encoder.layer.22.attention.self.query.bias\n",
      "roberta.encoder.layer.22.attention.self.key.weight\n",
      "roberta.encoder.layer.22.attention.self.key.bias\n",
      "roberta.encoder.layer.22.attention.self.value.weight\n",
      "roberta.encoder.layer.22.attention.self.value.bias\n",
      "roberta.encoder.layer.22.attention.output.dense.weight\n",
      "roberta.encoder.layer.22.attention.output.dense.bias\n",
      "roberta.encoder.layer.22.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.22.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.22.intermediate.dense.weight\n",
      "roberta.encoder.layer.22.intermediate.dense.bias\n",
      "roberta.encoder.layer.22.output.dense.weight\n",
      "roberta.encoder.layer.22.output.dense.bias\n",
      "roberta.encoder.layer.22.output.LayerNorm.weight\n",
      "roberta.encoder.layer.22.output.LayerNorm.bias\n",
      "roberta.encoder.layer.23.attention.self.query.weight\n",
      "roberta.encoder.layer.23.attention.self.query.bias\n",
      "roberta.encoder.layer.23.attention.self.key.weight\n",
      "roberta.encoder.layer.23.attention.self.key.bias\n",
      "roberta.encoder.layer.23.attention.self.value.weight\n",
      "roberta.encoder.layer.23.attention.self.value.bias\n",
      "roberta.encoder.layer.23.attention.output.dense.weight\n",
      "roberta.encoder.layer.23.attention.output.dense.bias\n",
      "roberta.encoder.layer.23.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.23.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.23.intermediate.dense.weight\n",
      "roberta.encoder.layer.23.intermediate.dense.bias\n",
      "roberta.encoder.layer.23.output.dense.weight\n",
      "roberta.encoder.layer.23.output.dense.bias\n",
      "roberta.encoder.layer.23.output.LayerNorm.weight\n",
      "roberta.encoder.layer.23.output.LayerNorm.bias\n",
      "classifier.dense.weight\n",
      "classifier.dense.bias\n",
      "classifier.out_proj.weight\n",
      "classifier.out_proj.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_wout_lora.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only classifier layer trainable and the rest will be remained frozen\n",
    "for name, param in model_wout_lora.named_parameters():\n",
    "    if \"classifier\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "roberta\n",
      "roberta.embeddings\n",
      "roberta.embeddings.word_embeddings\n",
      "roberta.embeddings.position_embeddings\n",
      "roberta.embeddings.token_type_embeddings\n",
      "roberta.embeddings.LayerNorm\n",
      "roberta.embeddings.dropout\n",
      "roberta.encoder\n",
      "roberta.encoder.layer\n",
      "roberta.encoder.layer.0\n",
      "roberta.encoder.layer.0.attention\n",
      "roberta.encoder.layer.0.attention.self\n",
      "roberta.encoder.layer.0.attention.self.query\n",
      "roberta.encoder.layer.0.attention.self.key\n",
      "roberta.encoder.layer.0.attention.self.value\n",
      "roberta.encoder.layer.0.attention.self.dropout\n",
      "roberta.encoder.layer.0.attention.output\n",
      "roberta.encoder.layer.0.attention.output.dense\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm\n",
      "roberta.encoder.layer.0.attention.output.dropout\n",
      "roberta.encoder.layer.0.intermediate\n",
      "roberta.encoder.layer.0.intermediate.dense\n",
      "roberta.encoder.layer.0.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.0.output\n",
      "roberta.encoder.layer.0.output.dense\n",
      "roberta.encoder.layer.0.output.LayerNorm\n",
      "roberta.encoder.layer.0.output.dropout\n",
      "roberta.encoder.layer.1\n",
      "roberta.encoder.layer.1.attention\n",
      "roberta.encoder.layer.1.attention.self\n",
      "roberta.encoder.layer.1.attention.self.query\n",
      "roberta.encoder.layer.1.attention.self.key\n",
      "roberta.encoder.layer.1.attention.self.value\n",
      "roberta.encoder.layer.1.attention.self.dropout\n",
      "roberta.encoder.layer.1.attention.output\n",
      "roberta.encoder.layer.1.attention.output.dense\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm\n",
      "roberta.encoder.layer.1.attention.output.dropout\n",
      "roberta.encoder.layer.1.intermediate\n",
      "roberta.encoder.layer.1.intermediate.dense\n",
      "roberta.encoder.layer.1.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.1.output\n",
      "roberta.encoder.layer.1.output.dense\n",
      "roberta.encoder.layer.1.output.LayerNorm\n",
      "roberta.encoder.layer.1.output.dropout\n",
      "roberta.encoder.layer.2\n",
      "roberta.encoder.layer.2.attention\n",
      "roberta.encoder.layer.2.attention.self\n",
      "roberta.encoder.layer.2.attention.self.query\n",
      "roberta.encoder.layer.2.attention.self.key\n",
      "roberta.encoder.layer.2.attention.self.value\n",
      "roberta.encoder.layer.2.attention.self.dropout\n",
      "roberta.encoder.layer.2.attention.output\n",
      "roberta.encoder.layer.2.attention.output.dense\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm\n",
      "roberta.encoder.layer.2.attention.output.dropout\n",
      "roberta.encoder.layer.2.intermediate\n",
      "roberta.encoder.layer.2.intermediate.dense\n",
      "roberta.encoder.layer.2.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.2.output\n",
      "roberta.encoder.layer.2.output.dense\n",
      "roberta.encoder.layer.2.output.LayerNorm\n",
      "roberta.encoder.layer.2.output.dropout\n",
      "roberta.encoder.layer.3\n",
      "roberta.encoder.layer.3.attention\n",
      "roberta.encoder.layer.3.attention.self\n",
      "roberta.encoder.layer.3.attention.self.query\n",
      "roberta.encoder.layer.3.attention.self.key\n",
      "roberta.encoder.layer.3.attention.self.value\n",
      "roberta.encoder.layer.3.attention.self.dropout\n",
      "roberta.encoder.layer.3.attention.output\n",
      "roberta.encoder.layer.3.attention.output.dense\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm\n",
      "roberta.encoder.layer.3.attention.output.dropout\n",
      "roberta.encoder.layer.3.intermediate\n",
      "roberta.encoder.layer.3.intermediate.dense\n",
      "roberta.encoder.layer.3.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.3.output\n",
      "roberta.encoder.layer.3.output.dense\n",
      "roberta.encoder.layer.3.output.LayerNorm\n",
      "roberta.encoder.layer.3.output.dropout\n",
      "roberta.encoder.layer.4\n",
      "roberta.encoder.layer.4.attention\n",
      "roberta.encoder.layer.4.attention.self\n",
      "roberta.encoder.layer.4.attention.self.query\n",
      "roberta.encoder.layer.4.attention.self.key\n",
      "roberta.encoder.layer.4.attention.self.value\n",
      "roberta.encoder.layer.4.attention.self.dropout\n",
      "roberta.encoder.layer.4.attention.output\n",
      "roberta.encoder.layer.4.attention.output.dense\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm\n",
      "roberta.encoder.layer.4.attention.output.dropout\n",
      "roberta.encoder.layer.4.intermediate\n",
      "roberta.encoder.layer.4.intermediate.dense\n",
      "roberta.encoder.layer.4.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.4.output\n",
      "roberta.encoder.layer.4.output.dense\n",
      "roberta.encoder.layer.4.output.LayerNorm\n",
      "roberta.encoder.layer.4.output.dropout\n",
      "roberta.encoder.layer.5\n",
      "roberta.encoder.layer.5.attention\n",
      "roberta.encoder.layer.5.attention.self\n",
      "roberta.encoder.layer.5.attention.self.query\n",
      "roberta.encoder.layer.5.attention.self.key\n",
      "roberta.encoder.layer.5.attention.self.value\n",
      "roberta.encoder.layer.5.attention.self.dropout\n",
      "roberta.encoder.layer.5.attention.output\n",
      "roberta.encoder.layer.5.attention.output.dense\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm\n",
      "roberta.encoder.layer.5.attention.output.dropout\n",
      "roberta.encoder.layer.5.intermediate\n",
      "roberta.encoder.layer.5.intermediate.dense\n",
      "roberta.encoder.layer.5.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.5.output\n",
      "roberta.encoder.layer.5.output.dense\n",
      "roberta.encoder.layer.5.output.LayerNorm\n",
      "roberta.encoder.layer.5.output.dropout\n",
      "roberta.encoder.layer.6\n",
      "roberta.encoder.layer.6.attention\n",
      "roberta.encoder.layer.6.attention.self\n",
      "roberta.encoder.layer.6.attention.self.query\n",
      "roberta.encoder.layer.6.attention.self.key\n",
      "roberta.encoder.layer.6.attention.self.value\n",
      "roberta.encoder.layer.6.attention.self.dropout\n",
      "roberta.encoder.layer.6.attention.output\n",
      "roberta.encoder.layer.6.attention.output.dense\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm\n",
      "roberta.encoder.layer.6.attention.output.dropout\n",
      "roberta.encoder.layer.6.intermediate\n",
      "roberta.encoder.layer.6.intermediate.dense\n",
      "roberta.encoder.layer.6.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.6.output\n",
      "roberta.encoder.layer.6.output.dense\n",
      "roberta.encoder.layer.6.output.LayerNorm\n",
      "roberta.encoder.layer.6.output.dropout\n",
      "roberta.encoder.layer.7\n",
      "roberta.encoder.layer.7.attention\n",
      "roberta.encoder.layer.7.attention.self\n",
      "roberta.encoder.layer.7.attention.self.query\n",
      "roberta.encoder.layer.7.attention.self.key\n",
      "roberta.encoder.layer.7.attention.self.value\n",
      "roberta.encoder.layer.7.attention.self.dropout\n",
      "roberta.encoder.layer.7.attention.output\n",
      "roberta.encoder.layer.7.attention.output.dense\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm\n",
      "roberta.encoder.layer.7.attention.output.dropout\n",
      "roberta.encoder.layer.7.intermediate\n",
      "roberta.encoder.layer.7.intermediate.dense\n",
      "roberta.encoder.layer.7.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.7.output\n",
      "roberta.encoder.layer.7.output.dense\n",
      "roberta.encoder.layer.7.output.LayerNorm\n",
      "roberta.encoder.layer.7.output.dropout\n",
      "roberta.encoder.layer.8\n",
      "roberta.encoder.layer.8.attention\n",
      "roberta.encoder.layer.8.attention.self\n",
      "roberta.encoder.layer.8.attention.self.query\n",
      "roberta.encoder.layer.8.attention.self.key\n",
      "roberta.encoder.layer.8.attention.self.value\n",
      "roberta.encoder.layer.8.attention.self.dropout\n",
      "roberta.encoder.layer.8.attention.output\n",
      "roberta.encoder.layer.8.attention.output.dense\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm\n",
      "roberta.encoder.layer.8.attention.output.dropout\n",
      "roberta.encoder.layer.8.intermediate\n",
      "roberta.encoder.layer.8.intermediate.dense\n",
      "roberta.encoder.layer.8.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.8.output\n",
      "roberta.encoder.layer.8.output.dense\n",
      "roberta.encoder.layer.8.output.LayerNorm\n",
      "roberta.encoder.layer.8.output.dropout\n",
      "roberta.encoder.layer.9\n",
      "roberta.encoder.layer.9.attention\n",
      "roberta.encoder.layer.9.attention.self\n",
      "roberta.encoder.layer.9.attention.self.query\n",
      "roberta.encoder.layer.9.attention.self.key\n",
      "roberta.encoder.layer.9.attention.self.value\n",
      "roberta.encoder.layer.9.attention.self.dropout\n",
      "roberta.encoder.layer.9.attention.output\n",
      "roberta.encoder.layer.9.attention.output.dense\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm\n",
      "roberta.encoder.layer.9.attention.output.dropout\n",
      "roberta.encoder.layer.9.intermediate\n",
      "roberta.encoder.layer.9.intermediate.dense\n",
      "roberta.encoder.layer.9.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.9.output\n",
      "roberta.encoder.layer.9.output.dense\n",
      "roberta.encoder.layer.9.output.LayerNorm\n",
      "roberta.encoder.layer.9.output.dropout\n",
      "roberta.encoder.layer.10\n",
      "roberta.encoder.layer.10.attention\n",
      "roberta.encoder.layer.10.attention.self\n",
      "roberta.encoder.layer.10.attention.self.query\n",
      "roberta.encoder.layer.10.attention.self.key\n",
      "roberta.encoder.layer.10.attention.self.value\n",
      "roberta.encoder.layer.10.attention.self.dropout\n",
      "roberta.encoder.layer.10.attention.output\n",
      "roberta.encoder.layer.10.attention.output.dense\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm\n",
      "roberta.encoder.layer.10.attention.output.dropout\n",
      "roberta.encoder.layer.10.intermediate\n",
      "roberta.encoder.layer.10.intermediate.dense\n",
      "roberta.encoder.layer.10.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.10.output\n",
      "roberta.encoder.layer.10.output.dense\n",
      "roberta.encoder.layer.10.output.LayerNorm\n",
      "roberta.encoder.layer.10.output.dropout\n",
      "roberta.encoder.layer.11\n",
      "roberta.encoder.layer.11.attention\n",
      "roberta.encoder.layer.11.attention.self\n",
      "roberta.encoder.layer.11.attention.self.query\n",
      "roberta.encoder.layer.11.attention.self.key\n",
      "roberta.encoder.layer.11.attention.self.value\n",
      "roberta.encoder.layer.11.attention.self.dropout\n",
      "roberta.encoder.layer.11.attention.output\n",
      "roberta.encoder.layer.11.attention.output.dense\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm\n",
      "roberta.encoder.layer.11.attention.output.dropout\n",
      "roberta.encoder.layer.11.intermediate\n",
      "roberta.encoder.layer.11.intermediate.dense\n",
      "roberta.encoder.layer.11.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.11.output\n",
      "roberta.encoder.layer.11.output.dense\n",
      "roberta.encoder.layer.11.output.LayerNorm\n",
      "roberta.encoder.layer.11.output.dropout\n",
      "roberta.encoder.layer.12\n",
      "roberta.encoder.layer.12.attention\n",
      "roberta.encoder.layer.12.attention.self\n",
      "roberta.encoder.layer.12.attention.self.query\n",
      "roberta.encoder.layer.12.attention.self.key\n",
      "roberta.encoder.layer.12.attention.self.value\n",
      "roberta.encoder.layer.12.attention.self.dropout\n",
      "roberta.encoder.layer.12.attention.output\n",
      "roberta.encoder.layer.12.attention.output.dense\n",
      "roberta.encoder.layer.12.attention.output.LayerNorm\n",
      "roberta.encoder.layer.12.attention.output.dropout\n",
      "roberta.encoder.layer.12.intermediate\n",
      "roberta.encoder.layer.12.intermediate.dense\n",
      "roberta.encoder.layer.12.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.12.output\n",
      "roberta.encoder.layer.12.output.dense\n",
      "roberta.encoder.layer.12.output.LayerNorm\n",
      "roberta.encoder.layer.12.output.dropout\n",
      "roberta.encoder.layer.13\n",
      "roberta.encoder.layer.13.attention\n",
      "roberta.encoder.layer.13.attention.self\n",
      "roberta.encoder.layer.13.attention.self.query\n",
      "roberta.encoder.layer.13.attention.self.key\n",
      "roberta.encoder.layer.13.attention.self.value\n",
      "roberta.encoder.layer.13.attention.self.dropout\n",
      "roberta.encoder.layer.13.attention.output\n",
      "roberta.encoder.layer.13.attention.output.dense\n",
      "roberta.encoder.layer.13.attention.output.LayerNorm\n",
      "roberta.encoder.layer.13.attention.output.dropout\n",
      "roberta.encoder.layer.13.intermediate\n",
      "roberta.encoder.layer.13.intermediate.dense\n",
      "roberta.encoder.layer.13.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.13.output\n",
      "roberta.encoder.layer.13.output.dense\n",
      "roberta.encoder.layer.13.output.LayerNorm\n",
      "roberta.encoder.layer.13.output.dropout\n",
      "roberta.encoder.layer.14\n",
      "roberta.encoder.layer.14.attention\n",
      "roberta.encoder.layer.14.attention.self\n",
      "roberta.encoder.layer.14.attention.self.query\n",
      "roberta.encoder.layer.14.attention.self.key\n",
      "roberta.encoder.layer.14.attention.self.value\n",
      "roberta.encoder.layer.14.attention.self.dropout\n",
      "roberta.encoder.layer.14.attention.output\n",
      "roberta.encoder.layer.14.attention.output.dense\n",
      "roberta.encoder.layer.14.attention.output.LayerNorm\n",
      "roberta.encoder.layer.14.attention.output.dropout\n",
      "roberta.encoder.layer.14.intermediate\n",
      "roberta.encoder.layer.14.intermediate.dense\n",
      "roberta.encoder.layer.14.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.14.output\n",
      "roberta.encoder.layer.14.output.dense\n",
      "roberta.encoder.layer.14.output.LayerNorm\n",
      "roberta.encoder.layer.14.output.dropout\n",
      "roberta.encoder.layer.15\n",
      "roberta.encoder.layer.15.attention\n",
      "roberta.encoder.layer.15.attention.self\n",
      "roberta.encoder.layer.15.attention.self.query\n",
      "roberta.encoder.layer.15.attention.self.key\n",
      "roberta.encoder.layer.15.attention.self.value\n",
      "roberta.encoder.layer.15.attention.self.dropout\n",
      "roberta.encoder.layer.15.attention.output\n",
      "roberta.encoder.layer.15.attention.output.dense\n",
      "roberta.encoder.layer.15.attention.output.LayerNorm\n",
      "roberta.encoder.layer.15.attention.output.dropout\n",
      "roberta.encoder.layer.15.intermediate\n",
      "roberta.encoder.layer.15.intermediate.dense\n",
      "roberta.encoder.layer.15.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.15.output\n",
      "roberta.encoder.layer.15.output.dense\n",
      "roberta.encoder.layer.15.output.LayerNorm\n",
      "roberta.encoder.layer.15.output.dropout\n",
      "roberta.encoder.layer.16\n",
      "roberta.encoder.layer.16.attention\n",
      "roberta.encoder.layer.16.attention.self\n",
      "roberta.encoder.layer.16.attention.self.query\n",
      "roberta.encoder.layer.16.attention.self.key\n",
      "roberta.encoder.layer.16.attention.self.value\n",
      "roberta.encoder.layer.16.attention.self.dropout\n",
      "roberta.encoder.layer.16.attention.output\n",
      "roberta.encoder.layer.16.attention.output.dense\n",
      "roberta.encoder.layer.16.attention.output.LayerNorm\n",
      "roberta.encoder.layer.16.attention.output.dropout\n",
      "roberta.encoder.layer.16.intermediate\n",
      "roberta.encoder.layer.16.intermediate.dense\n",
      "roberta.encoder.layer.16.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.16.output\n",
      "roberta.encoder.layer.16.output.dense\n",
      "roberta.encoder.layer.16.output.LayerNorm\n",
      "roberta.encoder.layer.16.output.dropout\n",
      "roberta.encoder.layer.17\n",
      "roberta.encoder.layer.17.attention\n",
      "roberta.encoder.layer.17.attention.self\n",
      "roberta.encoder.layer.17.attention.self.query\n",
      "roberta.encoder.layer.17.attention.self.key\n",
      "roberta.encoder.layer.17.attention.self.value\n",
      "roberta.encoder.layer.17.attention.self.dropout\n",
      "roberta.encoder.layer.17.attention.output\n",
      "roberta.encoder.layer.17.attention.output.dense\n",
      "roberta.encoder.layer.17.attention.output.LayerNorm\n",
      "roberta.encoder.layer.17.attention.output.dropout\n",
      "roberta.encoder.layer.17.intermediate\n",
      "roberta.encoder.layer.17.intermediate.dense\n",
      "roberta.encoder.layer.17.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.17.output\n",
      "roberta.encoder.layer.17.output.dense\n",
      "roberta.encoder.layer.17.output.LayerNorm\n",
      "roberta.encoder.layer.17.output.dropout\n",
      "roberta.encoder.layer.18\n",
      "roberta.encoder.layer.18.attention\n",
      "roberta.encoder.layer.18.attention.self\n",
      "roberta.encoder.layer.18.attention.self.query\n",
      "roberta.encoder.layer.18.attention.self.key\n",
      "roberta.encoder.layer.18.attention.self.value\n",
      "roberta.encoder.layer.18.attention.self.dropout\n",
      "roberta.encoder.layer.18.attention.output\n",
      "roberta.encoder.layer.18.attention.output.dense\n",
      "roberta.encoder.layer.18.attention.output.LayerNorm\n",
      "roberta.encoder.layer.18.attention.output.dropout\n",
      "roberta.encoder.layer.18.intermediate\n",
      "roberta.encoder.layer.18.intermediate.dense\n",
      "roberta.encoder.layer.18.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.18.output\n",
      "roberta.encoder.layer.18.output.dense\n",
      "roberta.encoder.layer.18.output.LayerNorm\n",
      "roberta.encoder.layer.18.output.dropout\n",
      "roberta.encoder.layer.19\n",
      "roberta.encoder.layer.19.attention\n",
      "roberta.encoder.layer.19.attention.self\n",
      "roberta.encoder.layer.19.attention.self.query\n",
      "roberta.encoder.layer.19.attention.self.key\n",
      "roberta.encoder.layer.19.attention.self.value\n",
      "roberta.encoder.layer.19.attention.self.dropout\n",
      "roberta.encoder.layer.19.attention.output\n",
      "roberta.encoder.layer.19.attention.output.dense\n",
      "roberta.encoder.layer.19.attention.output.LayerNorm\n",
      "roberta.encoder.layer.19.attention.output.dropout\n",
      "roberta.encoder.layer.19.intermediate\n",
      "roberta.encoder.layer.19.intermediate.dense\n",
      "roberta.encoder.layer.19.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.19.output\n",
      "roberta.encoder.layer.19.output.dense\n",
      "roberta.encoder.layer.19.output.LayerNorm\n",
      "roberta.encoder.layer.19.output.dropout\n",
      "roberta.encoder.layer.20\n",
      "roberta.encoder.layer.20.attention\n",
      "roberta.encoder.layer.20.attention.self\n",
      "roberta.encoder.layer.20.attention.self.query\n",
      "roberta.encoder.layer.20.attention.self.key\n",
      "roberta.encoder.layer.20.attention.self.value\n",
      "roberta.encoder.layer.20.attention.self.dropout\n",
      "roberta.encoder.layer.20.attention.output\n",
      "roberta.encoder.layer.20.attention.output.dense\n",
      "roberta.encoder.layer.20.attention.output.LayerNorm\n",
      "roberta.encoder.layer.20.attention.output.dropout\n",
      "roberta.encoder.layer.20.intermediate\n",
      "roberta.encoder.layer.20.intermediate.dense\n",
      "roberta.encoder.layer.20.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.20.output\n",
      "roberta.encoder.layer.20.output.dense\n",
      "roberta.encoder.layer.20.output.LayerNorm\n",
      "roberta.encoder.layer.20.output.dropout\n",
      "roberta.encoder.layer.21\n",
      "roberta.encoder.layer.21.attention\n",
      "roberta.encoder.layer.21.attention.self\n",
      "roberta.encoder.layer.21.attention.self.query\n",
      "roberta.encoder.layer.21.attention.self.key\n",
      "roberta.encoder.layer.21.attention.self.value\n",
      "roberta.encoder.layer.21.attention.self.dropout\n",
      "roberta.encoder.layer.21.attention.output\n",
      "roberta.encoder.layer.21.attention.output.dense\n",
      "roberta.encoder.layer.21.attention.output.LayerNorm\n",
      "roberta.encoder.layer.21.attention.output.dropout\n",
      "roberta.encoder.layer.21.intermediate\n",
      "roberta.encoder.layer.21.intermediate.dense\n",
      "roberta.encoder.layer.21.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.21.output\n",
      "roberta.encoder.layer.21.output.dense\n",
      "roberta.encoder.layer.21.output.LayerNorm\n",
      "roberta.encoder.layer.21.output.dropout\n",
      "roberta.encoder.layer.22\n",
      "roberta.encoder.layer.22.attention\n",
      "roberta.encoder.layer.22.attention.self\n",
      "roberta.encoder.layer.22.attention.self.query\n",
      "roberta.encoder.layer.22.attention.self.key\n",
      "roberta.encoder.layer.22.attention.self.value\n",
      "roberta.encoder.layer.22.attention.self.dropout\n",
      "roberta.encoder.layer.22.attention.output\n",
      "roberta.encoder.layer.22.attention.output.dense\n",
      "roberta.encoder.layer.22.attention.output.LayerNorm\n",
      "roberta.encoder.layer.22.attention.output.dropout\n",
      "roberta.encoder.layer.22.intermediate\n",
      "roberta.encoder.layer.22.intermediate.dense\n",
      "roberta.encoder.layer.22.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.22.output\n",
      "roberta.encoder.layer.22.output.dense\n",
      "roberta.encoder.layer.22.output.LayerNorm\n",
      "roberta.encoder.layer.22.output.dropout\n",
      "roberta.encoder.layer.23\n",
      "roberta.encoder.layer.23.attention\n",
      "roberta.encoder.layer.23.attention.self\n",
      "roberta.encoder.layer.23.attention.self.query\n",
      "roberta.encoder.layer.23.attention.self.key\n",
      "roberta.encoder.layer.23.attention.self.value\n",
      "roberta.encoder.layer.23.attention.self.dropout\n",
      "roberta.encoder.layer.23.attention.output\n",
      "roberta.encoder.layer.23.attention.output.dense\n",
      "roberta.encoder.layer.23.attention.output.LayerNorm\n",
      "roberta.encoder.layer.23.attention.output.dropout\n",
      "roberta.encoder.layer.23.intermediate\n",
      "roberta.encoder.layer.23.intermediate.dense\n",
      "roberta.encoder.layer.23.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.23.output\n",
      "roberta.encoder.layer.23.output.dense\n",
      "roberta.encoder.layer.23.output.LayerNorm\n",
      "roberta.encoder.layer.23.output.dropout\n",
      "classifier\n",
      "classifier.dense\n",
      "classifier.dropout\n",
      "classifier.out_proj\n"
     ]
    }
   ],
   "source": [
    "for name, module in model_wout_lora.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,629,639 || all params: 562,527,246 || trainable%: 0.4675\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\"]\n",
    ")\n",
    "\n",
    "model_lora = get_peft_model(model_wout_lora, lora_config)\n",
    "model_lora.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuqires grad: False --> base_model.model.roberta.embeddings.word_embeddings.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.embeddings.position_embeddings.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.embeddings.token_type_embeddings.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.embeddings.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.embeddings.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.0.attention.self.query.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.0.attention.self.query.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.0.attention.self.key.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.0.attention.self.key.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.0.attention.self.value.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.0.attention.self.value.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.0.attention.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.0.attention.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.0.intermediate.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.0.intermediate.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.0.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.0.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.0.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.0.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.1.attention.self.query.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.1.attention.self.query.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.1.attention.self.key.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.1.attention.self.key.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.1.attention.self.value.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.1.attention.self.value.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.1.attention.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.1.attention.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.1.intermediate.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.1.intermediate.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.1.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.1.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.1.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.1.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.2.attention.self.query.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.2.attention.self.query.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.2.attention.self.key.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.2.attention.self.key.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.2.attention.self.value.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.2.attention.self.value.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.2.attention.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.2.attention.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.2.intermediate.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.2.intermediate.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.2.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.2.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.2.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.2.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.3.attention.self.query.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.3.attention.self.query.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.3.attention.self.key.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.3.attention.self.key.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.3.attention.self.value.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.3.attention.self.value.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.3.attention.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.3.attention.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.3.intermediate.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.3.intermediate.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.3.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.3.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.3.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.3.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.4.attention.self.query.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.4.attention.self.query.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.4.attention.self.key.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.4.attention.self.key.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.4.attention.self.value.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.4.attention.self.value.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.4.attention.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.4.attention.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.4.intermediate.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.4.intermediate.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.4.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.4.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.4.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.4.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.5.attention.self.query.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.5.attention.self.query.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.5.attention.self.key.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.5.attention.self.key.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.5.attention.self.value.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.5.attention.self.value.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.5.attention.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.5.attention.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.5.intermediate.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.5.intermediate.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.5.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.5.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.5.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.5.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.6.attention.self.query.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.6.attention.self.query.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.6.attention.self.query.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.6.attention.self.key.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.6.attention.self.key.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.6.attention.self.value.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.6.attention.self.value.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.6.attention.self.value.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.6.attention.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.6.attention.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.6.intermediate.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.6.intermediate.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.6.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.6.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.6.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.6.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.7.attention.self.query.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.7.attention.self.query.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.7.attention.self.query.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.7.attention.self.key.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.7.attention.self.key.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.7.attention.self.value.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.7.attention.self.value.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.7.attention.self.value.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.7.attention.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.7.attention.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.7.intermediate.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.7.intermediate.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.7.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.7.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.7.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.7.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.8.attention.self.query.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.8.attention.self.query.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.8.attention.self.query.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.8.attention.self.key.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.8.attention.self.key.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.8.attention.self.value.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.8.attention.self.value.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.8.attention.self.value.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.8.attention.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.8.attention.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.8.intermediate.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.8.intermediate.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.8.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.8.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.8.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.8.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.9.attention.self.query.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.9.attention.self.query.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.9.attention.self.query.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.9.attention.self.key.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.9.attention.self.key.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.9.attention.self.value.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.9.attention.self.value.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.9.attention.self.value.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.9.attention.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.9.attention.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.9.intermediate.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.9.intermediate.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.9.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.9.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.10.attention.self.query.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.10.attention.self.query.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.10.attention.self.query.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.10.attention.self.key.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.10.attention.self.key.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.10.attention.self.value.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.10.attention.self.value.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.10.attention.self.value.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.10.attention.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.10.attention.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.10.intermediate.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.10.intermediate.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.10.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.10.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.11.attention.self.query.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.11.attention.self.query.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.11.attention.self.query.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.11.attention.self.key.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.11.attention.self.key.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.11.attention.self.value.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.11.attention.self.value.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.11.attention.self.value.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.11.attention.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.11.attention.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.11.intermediate.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.11.intermediate.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.11.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.11.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.12.attention.self.query.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.12.attention.self.query.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.12.attention.self.query.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.12.attention.self.query.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.12.attention.self.key.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.12.attention.self.key.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.12.attention.self.value.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.12.attention.self.value.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.12.attention.self.value.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.12.attention.self.value.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.12.attention.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.12.attention.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.12.attention.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.12.attention.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.12.intermediate.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.12.intermediate.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.12.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.12.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.12.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.12.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.13.attention.self.query.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.13.attention.self.query.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.13.attention.self.query.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.13.attention.self.query.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.13.attention.self.key.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.13.attention.self.key.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.13.attention.self.value.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.13.attention.self.value.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.13.attention.self.value.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.13.attention.self.value.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.13.attention.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.13.attention.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.13.attention.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.13.attention.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.13.intermediate.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.13.intermediate.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.13.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.13.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.13.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.13.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.14.attention.self.query.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.14.attention.self.query.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.14.attention.self.query.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.14.attention.self.query.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.14.attention.self.key.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.14.attention.self.key.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.14.attention.self.value.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.14.attention.self.value.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.14.attention.self.value.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.14.attention.self.value.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.14.attention.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.14.attention.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.14.attention.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.14.attention.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.14.intermediate.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.14.intermediate.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.14.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.14.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.14.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.14.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.15.attention.self.query.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.15.attention.self.query.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.15.attention.self.query.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.15.attention.self.query.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.15.attention.self.key.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.15.attention.self.key.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.15.attention.self.value.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.15.attention.self.value.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.15.attention.self.value.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.15.attention.self.value.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.15.attention.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.15.attention.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.15.attention.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.15.attention.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.15.intermediate.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.15.intermediate.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.15.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.15.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.15.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.15.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.16.attention.self.query.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.16.attention.self.query.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.16.attention.self.query.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.16.attention.self.query.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.16.attention.self.key.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.16.attention.self.key.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.16.attention.self.value.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.16.attention.self.value.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.16.attention.self.value.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.16.attention.self.value.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.16.attention.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.16.attention.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.16.attention.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.16.attention.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.16.intermediate.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.16.intermediate.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.16.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.16.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.16.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.16.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.17.attention.self.query.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.17.attention.self.query.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.17.attention.self.query.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.17.attention.self.query.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.17.attention.self.key.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.17.attention.self.key.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.17.attention.self.value.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.17.attention.self.value.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.17.attention.self.value.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.17.attention.self.value.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.17.attention.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.17.attention.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.17.attention.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.17.attention.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.17.intermediate.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.17.intermediate.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.17.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.17.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.17.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.17.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.18.attention.self.query.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.18.attention.self.query.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.18.attention.self.query.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.18.attention.self.query.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.18.attention.self.key.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.18.attention.self.key.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.18.attention.self.value.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.18.attention.self.value.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.18.attention.self.value.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.18.attention.self.value.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.18.attention.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.18.attention.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.18.attention.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.18.attention.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.18.intermediate.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.18.intermediate.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.18.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.18.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.18.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.18.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.19.attention.self.query.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.19.attention.self.query.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.19.attention.self.query.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.19.attention.self.query.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.19.attention.self.key.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.19.attention.self.key.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.19.attention.self.value.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.19.attention.self.value.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.19.attention.self.value.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.19.attention.self.value.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.19.attention.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.19.attention.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.19.attention.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.19.attention.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.19.intermediate.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.19.intermediate.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.19.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.19.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.19.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.19.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.20.attention.self.query.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.20.attention.self.query.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.20.attention.self.query.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.20.attention.self.query.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.20.attention.self.key.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.20.attention.self.key.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.20.attention.self.value.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.20.attention.self.value.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.20.attention.self.value.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.20.attention.self.value.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.20.attention.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.20.attention.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.20.attention.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.20.attention.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.20.intermediate.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.20.intermediate.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.20.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.20.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.20.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.20.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.21.attention.self.query.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.21.attention.self.query.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.21.attention.self.query.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.21.attention.self.query.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.21.attention.self.key.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.21.attention.self.key.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.21.attention.self.value.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.21.attention.self.value.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.21.attention.self.value.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.21.attention.self.value.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.21.attention.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.21.attention.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.21.attention.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.21.attention.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.21.intermediate.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.21.intermediate.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.21.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.21.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.21.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.21.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.22.attention.self.query.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.22.attention.self.query.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.22.attention.self.query.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.22.attention.self.query.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.22.attention.self.key.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.22.attention.self.key.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.22.attention.self.value.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.22.attention.self.value.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.22.attention.self.value.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.22.attention.self.value.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.22.attention.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.22.attention.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.22.attention.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.22.attention.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.22.intermediate.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.22.intermediate.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.22.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.22.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.22.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.22.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.23.attention.self.query.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.23.attention.self.query.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.23.attention.self.query.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.23.attention.self.query.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.23.attention.self.key.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.23.attention.self.key.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.23.attention.self.value.base_layer.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.23.attention.self.value.base_layer.bias\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.23.attention.self.value.lora_A.default.weight\n",
      "Reuqires grad: True --> base_model.model.roberta.encoder.layer.23.attention.self.value.lora_B.default.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.23.attention.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.23.attention.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.23.attention.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.23.attention.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.23.intermediate.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.23.intermediate.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.23.output.dense.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.23.output.dense.bias\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.23.output.LayerNorm.weight\n",
      "Reuqires grad: False --> base_model.model.roberta.encoder.layer.23.output.LayerNorm.bias\n",
      "Reuqires grad: False --> base_model.model.classifier.original_module.dense.weight\n",
      "Reuqires grad: False --> base_model.model.classifier.original_module.dense.bias\n",
      "Reuqires grad: False --> base_model.model.classifier.original_module.out_proj.weight\n",
      "Reuqires grad: False --> base_model.model.classifier.original_module.out_proj.bias\n",
      "Reuqires grad: True --> base_model.model.classifier.modules_to_save.default.dense.weight\n",
      "Reuqires grad: True --> base_model.model.classifier.modules_to_save.default.dense.bias\n",
      "Reuqires grad: True --> base_model.model.classifier.modules_to_save.default.out_proj.weight\n",
      "Reuqires grad: True --> base_model.model.classifier.modules_to_save.default.out_proj.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_lora.named_parameters():\n",
    "    print(f\"Reuqires grad: {param.requires_grad} --> {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4da84c852394b4d9991ef465ebb8554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3595 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c7c8d3d99a74167adba0f2e59ad179c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/857 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d83ba1d9b4e48f0ba3d439de203ba53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "tokenized_data = dataset_dict.map(lambda x: tokenizer_lora(x[\"text\"]), batched=True)\n",
    "\n",
    "# Define Accuracy Metric\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = torch.softmax(torch.tensor(logits, dtype=torch.float32), dim=1).argmax(dim=1)\n",
    "    return accuracy.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2250/2250 12:45, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.004700</td>\n",
       "      <td>1.939613</td>\n",
       "      <td>0.299883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.932800</td>\n",
       "      <td>1.859379</td>\n",
       "      <td>0.299883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.924300</td>\n",
       "      <td>1.863775</td>\n",
       "      <td>0.299883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.908400</td>\n",
       "      <td>1.903535</td>\n",
       "      <td>0.155193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.906500</td>\n",
       "      <td>1.894046</td>\n",
       "      <td>0.299883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.899400</td>\n",
       "      <td>1.880758</td>\n",
       "      <td>0.155193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.895100</td>\n",
       "      <td>1.863283</td>\n",
       "      <td>0.299883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.887800</td>\n",
       "      <td>1.873966</td>\n",
       "      <td>0.299883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.878900</td>\n",
       "      <td>1.862162</td>\n",
       "      <td>0.299883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.879300</td>\n",
       "      <td>1.869251</td>\n",
       "      <td>0.299883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2250, training_loss=1.9117214898003472, metrics={'train_runtime': 766.0434, 'train_samples_per_second': 46.929, 'train_steps_per_second': 2.937, 'total_flos': 3928421052096984.0, 'train_loss': 1.9117214898003472, 'epoch': 10.0})"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training HyperParameters\n",
    "lr = 1e-3\n",
    "weight_decay = 0.01\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "\n",
    "training_args_w_lora = TrainingArguments(\n",
    "    # output_dir=\"bert-category-classifier-teacher\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "data_collator_w_lora = DataCollatorWithPadding(tokenizer=tokenizer_lora)\n",
    "# Use the custom compute_loss function in Trainer\n",
    "trainer_w_lora = Trainer(\n",
    "    model=model_lora,\n",
    "    args=training_args_w_lora,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    processing_class=tokenizer_lora,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator_w_lora\n",
    ")\n",
    "\n",
    "trainer_w_lora.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_models/fine_tuned_roberta_w_lora/tokenizer_config.json',\n",
       " './fine_tuned_models/fine_tuned_roberta_w_lora/special_tokens_map.json',\n",
       " './fine_tuned_models/fine_tuned_roberta_w_lora/sentencepiece.bpe.model',\n",
       " './fine_tuned_models/fine_tuned_roberta_w_lora/added_tokens.json',\n",
       " './fine_tuned_models/fine_tuned_roberta_w_lora/tokenizer.json')"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_lora_model_directory = \"./fine_tuned_models/fine_tuned_roberta_w_lora\"\n",
    "\n",
    "# Save model\n",
    "model_lora.save_pretrained(save_lora_model_directory)\n",
    "# Save tokenizer\n",
    "tokenizer_lora.save_pretrained(save_lora_model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load base model first\n",
    "base_roberta_model = AutoModelForSequenceClassification.from_pretrained(model_wout_lora_id, \n",
    "                                                                        num_labels=7, \n",
    "                                                                        id2label=id2category, \n",
    "                                                                        label2id=category2id)\n",
    "\n",
    "# Load LoRA model on top of base model\n",
    "loaded_lora_model = PeftModel.from_pretrained(base_roberta_model, save_lora_model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "tokenized_val_data = tokenizer_lora(\n",
    "    dataset_dict[\"validation\"][\"text\"], \n",
    "    padding=True, \n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer_lora, return_tensors=\"pt\")\n",
    "batch_inputs = data_collator(tokenized_val_data)\n",
    "\n",
    "model_lora.eval()\n",
    "with torch.no_grad():\n",
    "    model_lora_outputs = loaded_lora_model.to(\"cpu\")(**batch_inputs)\n",
    "model_lora_outputs_logits = model_lora_outputs.logits\n",
    "model_lora_preds = torch.softmax(model_lora_outputs_logits, dim=1).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.27988338192419826}"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(eval_pred=(model_lora_outputs_logits.numpy(), dataset_dict[\"validation\"][\"labels\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Quantized Low-Rank Adaptation (QLoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Teacher Model \n",
    "model_wout_qlora_id = 'xlm-roberta-large'\n",
    "model_wout_qlora = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_wout_qlora_id,\n",
    "    num_labels=7,\n",
    "    id2label=id2category,\n",
    "    label2id=category2id\n",
    ")\n",
    "tokenizer_qlora = AutoTokenizer.from_pretrained(model_wout_qlora_id)\n",
    "# tokenizer_teacher2.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only classifier layer trainable and the rest will be remained frozen\n",
    "for name, param in model_wout_qlora.named_parameters():\n",
    "    if \"classifier\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,629,639 || all params: 562,527,246 || trainable%: 0.4675\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\"]\n",
    ")\n",
    "\n",
    "model_wout_qlora = prepare_model_for_kbit_training(model_wout_qlora)\n",
    "\n",
    "model_qlora = get_peft_model(model_wout_qlora, lora_config)\n",
    "model_qlora.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): XLMRobertaForSequenceClassification(\n",
       "      (roberta): XLMRobertaModel(\n",
       "        (embeddings): XLMRobertaEmbeddings(\n",
       "          (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "          (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "          (token_type_embeddings): Embedding(1, 1024)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): XLMRobertaEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-23): 24 x XLMRobertaLayer(\n",
       "              (attention): XLMRobertaAttention(\n",
       "                (self): XLMRobertaSdpaSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): XLMRobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): XLMRobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): XLMRobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): XLMRobertaClassificationHead(\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=7, bias=True)\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): XLMRobertaClassificationHead(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (out_proj): Linear(in_features=1024, out_features=7, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_qlora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b1ccb7e8344b8599e715fce7f3db3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3595 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d15e3f1c8694804bc2828f83f2701a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/857 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6dd9d4d8a9489097c7c8ff7aa609dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "tokenized_data = dataset_dict.map(lambda x: tokenizer_qlora(x[\"text\"]), batched=True)\n",
    "\n",
    "# Define Accuracy Metric\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = torch.softmax(torch.tensor(logits, dtype=torch.float32), dim=1).argmax(dim=1)\n",
    "    return accuracy.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 14:50, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.088400</td>\n",
       "      <td>0.267853</td>\n",
       "      <td>0.905484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.223600</td>\n",
       "      <td>0.319284</td>\n",
       "      <td>0.890315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.104700</td>\n",
       "      <td>0.404527</td>\n",
       "      <td>0.882147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.044300</td>\n",
       "      <td>0.439541</td>\n",
       "      <td>0.885648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.036100</td>\n",
       "      <td>0.515788</td>\n",
       "      <td>0.879813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.021100</td>\n",
       "      <td>0.591615</td>\n",
       "      <td>0.872812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.578610</td>\n",
       "      <td>0.885648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.589177</td>\n",
       "      <td>0.890315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.597647</td>\n",
       "      <td>0.896149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.586818</td>\n",
       "      <td>0.890315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=0.154475594162941, metrics={'train_runtime': 891.4032, 'train_samples_per_second': 40.33, 'train_steps_per_second': 1.683, 'total_flos': 4132795784105832.0, 'train_loss': 0.154475594162941, 'epoch': 10.0})"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training HyperParameters\n",
    "lr = 2e-4\n",
    "weight_decay = 0.01\n",
    "batch_size = 24\n",
    "num_epochs = 10\n",
    "\n",
    "training_args_w_qlora = TrainingArguments(\n",
    "    # output_dir=\"bert-category-classifier-teacher\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "data_collator_w_qlora = DataCollatorWithPadding(tokenizer=tokenizer_qlora)\n",
    "# Use the custom compute_loss function in Trainer\n",
    "trainer_w_qlora = Trainer(\n",
    "    model=model_qlora,\n",
    "    args=training_args_w_qlora,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    processing_class=tokenizer_qlora,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator_w_qlora\n",
    ")\n",
    "\n",
    "trainer_w_qlora.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_models/fine_tuned_roberta_w_qlora/tokenizer_config.json',\n",
       " './fine_tuned_models/fine_tuned_roberta_w_qlora/special_tokens_map.json',\n",
       " './fine_tuned_models/fine_tuned_roberta_w_qlora/sentencepiece.bpe.model',\n",
       " './fine_tuned_models/fine_tuned_roberta_w_qlora/added_tokens.json',\n",
       " './fine_tuned_models/fine_tuned_roberta_w_qlora/tokenizer.json')"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_qlora_model_directory = \"./fine_tuned_models/fine_tuned_roberta_w_qlora\"\n",
    "\n",
    "# Save model\n",
    "model_qlora.save_pretrained(save_qlora_model_directory)\n",
    "# Save tokenizer\n",
    "tokenizer_qlora.save_pretrained(save_qlora_model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load base model first\n",
    "base_roberta_model = AutoModelForSequenceClassification.from_pretrained(model_wout_qlora_id, \n",
    "                                                                        num_labels=7, \n",
    "                                                                        id2label=id2category, \n",
    "                                                                        label2id=category2id)\n",
    "\n",
    "# Load LoRA model on top of base model\n",
    "loaded_qlora_model = PeftModel.from_pretrained(base_roberta_model, save_qlora_model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "tokenized_val_data = tokenizer_qlora(\n",
    "    dataset_dict[\"validation\"][\"text\"], \n",
    "    padding=True, \n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer_qlora, return_tensors=\"pt\")\n",
    "batch_inputs = data_collator(tokenized_val_data)\n",
    "\n",
    "model_lora.eval()\n",
    "with torch.no_grad():\n",
    "    model_qlora_outputs = loaded_qlora_model.to(\"cpu\")(**batch_inputs)\n",
    "model_qlora_outputs_logits = model_qlora_outputs.logits\n",
    "model_qlora_preds = torch.softmax(model_qlora_outputs_logits, dim=1).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9037900874635568}"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(eval_pred=(model_qlora_outputs_logits.numpy(), dataset_dict[\"validation\"][\"labels\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. QLora with Gemma3-1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_compute_dtype= torch.bfloat16\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_model_id = \"google/gemma-3-1b-pt\"\n",
    "tokenizer_gemma = AutoTokenizer.from_pretrained(gemma_model_id)\n",
    "gemma_model_wout_qlora = Gemma3ForCausalLM.from_pretrained(gemma_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3ForCausalLM(\n",
       "  (model): Gemma3TextModel(\n",
       "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 1152, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear(in_features=1152, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1152, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "          (up_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "          (down_proj): Linear(in_features=6912, out_features=1152, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "    (rotary_emb): Gemma3RotaryEmbedding()\n",
       "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1152, out_features=262144, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_model_wout_qlora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262144"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding maximum token no\n",
    "max(tokenizer_gemma.vocab.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([262144, 1152])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each row in lm_head represent one token, so 262144 is vocab size and each row's index represent token id\n",
    "gemma_model_wout_qlora.lm_head.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[236771, 236770, 236778, 236800, 236812, 236810, 236825]\n"
     ]
    }
   ],
   "source": [
    "token_id_0 = tokenizer_gemma.encode(\"0\", add_special_tokens=False)[0]\n",
    "token_id_1 = tokenizer_gemma.encode(\"1\", add_special_tokens=False)[0]\n",
    "token_id_2 = tokenizer_gemma.encode(\"2\", add_special_tokens=False)[0]\n",
    "token_id_3 = tokenizer_gemma.encode(\"3\", add_special_tokens=False)[0]\n",
    "token_id_4 = tokenizer_gemma.encode(\"4\", add_special_tokens=False)[0]\n",
    "token_id_5 = tokenizer_gemma.encode(\"5\", add_special_tokens=False)[0]\n",
    "token_id_6 = tokenizer_gemma.encode(\"6\", add_special_tokens=False)[0]\n",
    "\n",
    "print([token_id_0,token_id_1,token_id_2,token_id_3,token_id_4,token_id_5,token_id_6])\n",
    "\n",
    "# keep only the 0,1,2,3,4,5,6 tokens from lm_head\n",
    "par = torch.nn.Parameter(torch.vstack([gemma_model_wout_qlora.lm_head.weight[token_id_0, :], \n",
    "                                       gemma_model_wout_qlora.lm_head.weight[token_id_1, :],\n",
    "                                       gemma_model_wout_qlora.lm_head.weight[token_id_2, :],\n",
    "                                       gemma_model_wout_qlora.lm_head.weight[token_id_3, :],\n",
    "                                       gemma_model_wout_qlora.lm_head.weight[token_id_4, :],\n",
    "                                       gemma_model_wout_qlora.lm_head.weight[token_id_5, :],\n",
    "                                       gemma_model_wout_qlora.lm_head.weight[token_id_6, :]\n",
    "                                    ]))\n",
    "gemma_model_wout_qlora.lm_head.weight = par\n",
    "\n",
    "# set vocab size as 7 (number of classes). we set this because while computing logits vocab_size parameter will be used!\n",
    "gemma_model_wout_qlora.vocab_size = 7\n",
    "gemma_model_wout_qlora.config.vocab_size = 7\n",
    "gemma_model_wout_qlora.lm_head.out_features=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 1152])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new lm_head will generate only this 7 tokens since we only keep seven of them \n",
    "gemma_model_wout_qlora.lm_head.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3ForCausalLM(\n",
       "  (model): Gemma3TextModel(\n",
       "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 1152, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear(in_features=1152, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1152, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "          (up_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "          (down_proj): Linear(in_features=6912, out_features=1152, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "    (rotary_emb): Gemma3RotaryEmbedding()\n",
       "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1152, out_features=7, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_model_wout_qlora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "trainable params: 1,490,944 || all params: 1,001,384,960 || trainable%: 0.1489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/toygunkarabas/Development/NLP & SLP/nlp_slp_env/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules = [\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "gemma_model_wout_qlora = prepare_model_for_kbit_training(gemma_model_wout_qlora)\n",
    "\n",
    "gemma_model_qlora = get_peft_model(gemma_model_wout_qlora, lora_config)\n",
    "gemma_model_qlora.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma3ForCausalLM(\n",
       "      (model): Gemma3TextModel(\n",
       "        (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 1152, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-25): 26 x Gemma3DecoderLayer(\n",
       "            (self_attn): Gemma3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1152, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1152, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1152, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1152, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=1024, out_features=1152, bias=False)\n",
       "              (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "            )\n",
       "            (mlp): Gemma3MLP(\n",
       "              (gate_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "              (up_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "              (down_proj): Linear(in_features=6912, out_features=1152, bias=False)\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "            (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "            (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "            (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (rotary_emb): Gemma3RotaryEmbedding()\n",
       "        (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1152, out_features=7, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_model_qlora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False - base_model.model.model.embed_tokens.weight\n",
      "False - base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.0.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.0.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.0.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.0.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.0.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.0.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.0.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.0.input_layernorm.weight\n",
      "False - base_model.model.model.layers.0.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.0.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.0.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.1.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.1.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.1.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.1.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.1.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.1.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.1.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.1.input_layernorm.weight\n",
      "False - base_model.model.model.layers.1.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.1.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.1.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.2.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.2.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.2.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.2.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.2.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.2.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.2.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.2.input_layernorm.weight\n",
      "False - base_model.model.model.layers.2.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.2.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.2.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.3.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.3.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.3.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.3.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.3.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.3.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.3.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.3.input_layernorm.weight\n",
      "False - base_model.model.model.layers.3.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.3.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.3.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.4.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.4.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.4.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.4.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.4.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.4.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.4.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.4.input_layernorm.weight\n",
      "False - base_model.model.model.layers.4.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.4.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.4.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.5.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.5.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.5.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.5.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.5.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.5.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.5.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.5.input_layernorm.weight\n",
      "False - base_model.model.model.layers.5.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.5.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.5.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.6.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.6.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.6.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.6.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.6.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.6.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.6.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.6.input_layernorm.weight\n",
      "False - base_model.model.model.layers.6.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.6.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.6.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.7.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.7.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.7.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.7.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.7.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.7.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.7.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.7.input_layernorm.weight\n",
      "False - base_model.model.model.layers.7.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.7.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.7.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.8.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.8.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.8.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.8.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.8.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.8.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.8.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.8.input_layernorm.weight\n",
      "False - base_model.model.model.layers.8.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.8.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.8.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.9.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.9.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.9.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.9.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.9.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.9.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.9.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.9.input_layernorm.weight\n",
      "False - base_model.model.model.layers.9.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.9.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.9.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.10.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.10.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.10.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.10.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.10.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.10.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.10.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.10.input_layernorm.weight\n",
      "False - base_model.model.model.layers.10.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.10.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.10.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.11.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.11.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.11.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.11.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.11.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.11.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.11.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.11.input_layernorm.weight\n",
      "False - base_model.model.model.layers.11.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.11.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.11.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.12.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.12.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.12.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.12.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.12.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.12.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.12.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.12.input_layernorm.weight\n",
      "False - base_model.model.model.layers.12.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.12.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.12.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.13.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.13.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.13.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.13.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.13.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.13.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.13.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.13.input_layernorm.weight\n",
      "False - base_model.model.model.layers.13.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.13.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.13.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.14.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.14.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.14.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.14.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.14.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.14.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.14.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.14.input_layernorm.weight\n",
      "False - base_model.model.model.layers.14.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.14.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.14.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.15.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.15.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.15.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.15.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.15.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.15.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.15.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.15.input_layernorm.weight\n",
      "False - base_model.model.model.layers.15.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.15.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.15.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.16.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.16.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.16.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.16.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.16.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.16.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.16.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.16.input_layernorm.weight\n",
      "False - base_model.model.model.layers.16.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.16.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.16.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.17.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.17.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.17.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.17.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.17.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.17.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.17.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.17.input_layernorm.weight\n",
      "False - base_model.model.model.layers.17.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.17.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.17.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.18.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.18.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.18.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.18.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.18.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.18.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.18.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.18.input_layernorm.weight\n",
      "False - base_model.model.model.layers.18.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.18.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.18.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.19.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.19.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.19.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.19.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.19.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.19.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.19.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.19.input_layernorm.weight\n",
      "False - base_model.model.model.layers.19.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.19.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.19.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.20.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.20.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.20.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.20.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.20.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.20.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.20.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.20.input_layernorm.weight\n",
      "False - base_model.model.model.layers.20.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.20.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.20.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.21.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.21.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.21.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.21.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.21.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.21.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.21.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.21.input_layernorm.weight\n",
      "False - base_model.model.model.layers.21.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.21.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.21.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.22.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.22.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.22.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.22.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.22.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.22.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.22.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.22.input_layernorm.weight\n",
      "False - base_model.model.model.layers.22.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.22.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.22.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.23.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.23.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.23.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.23.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.23.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.23.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.23.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.23.input_layernorm.weight\n",
      "False - base_model.model.model.layers.23.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.23.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.23.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.24.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.24.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.24.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.24.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.24.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.24.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.24.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.24.input_layernorm.weight\n",
      "False - base_model.model.model.layers.24.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.24.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.24.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.25.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.25.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.25.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.25.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.25.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.25.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.25.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.25.input_layernorm.weight\n",
      "False - base_model.model.model.layers.25.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.25.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.25.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.norm.weight\n",
      "False - base_model.model.lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "# Original model's parameters in lm_head is freezed automatically\n",
    "for name, param in gemma_model_qlora.named_parameters():\n",
    "    print(f\"{param.requires_grad} - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False - base_model.model.model.embed_tokens.weight\n",
      "False - base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.0.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.0.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.0.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.0.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.0.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.0.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.0.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.0.input_layernorm.weight\n",
      "False - base_model.model.model.layers.0.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.0.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.0.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.1.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.1.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.1.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.1.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.1.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.1.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.1.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.1.input_layernorm.weight\n",
      "False - base_model.model.model.layers.1.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.1.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.1.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.2.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.2.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.2.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.2.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.2.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.2.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.2.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.2.input_layernorm.weight\n",
      "False - base_model.model.model.layers.2.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.2.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.2.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.3.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.3.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.3.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.3.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.3.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.3.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.3.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.3.input_layernorm.weight\n",
      "False - base_model.model.model.layers.3.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.3.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.3.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.4.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.4.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.4.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.4.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.4.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.4.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.4.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.4.input_layernorm.weight\n",
      "False - base_model.model.model.layers.4.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.4.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.4.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.5.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.5.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.5.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.5.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.5.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.5.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.5.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.5.input_layernorm.weight\n",
      "False - base_model.model.model.layers.5.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.5.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.5.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.6.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.6.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.6.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.6.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.6.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.6.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.6.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.6.input_layernorm.weight\n",
      "False - base_model.model.model.layers.6.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.6.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.6.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.7.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.7.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.7.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.7.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.7.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.7.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.7.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.7.input_layernorm.weight\n",
      "False - base_model.model.model.layers.7.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.7.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.7.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.8.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.8.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.8.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.8.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.8.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.8.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.8.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.8.input_layernorm.weight\n",
      "False - base_model.model.model.layers.8.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.8.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.8.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.9.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.9.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.9.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.9.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.9.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.9.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.9.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.9.input_layernorm.weight\n",
      "False - base_model.model.model.layers.9.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.9.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.9.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.10.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.10.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.10.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.10.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.10.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.10.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.10.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.10.input_layernorm.weight\n",
      "False - base_model.model.model.layers.10.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.10.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.10.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.11.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.11.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.11.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.11.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.11.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.11.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.11.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.11.input_layernorm.weight\n",
      "False - base_model.model.model.layers.11.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.11.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.11.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.12.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.12.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.12.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.12.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.12.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.12.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.12.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.12.input_layernorm.weight\n",
      "False - base_model.model.model.layers.12.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.12.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.12.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.13.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.13.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.13.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.13.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.13.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.13.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.13.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.13.input_layernorm.weight\n",
      "False - base_model.model.model.layers.13.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.13.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.13.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.14.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.14.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.14.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.14.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.14.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.14.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.14.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.14.input_layernorm.weight\n",
      "False - base_model.model.model.layers.14.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.14.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.14.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.15.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.15.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.15.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.15.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.15.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.15.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.15.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.15.input_layernorm.weight\n",
      "False - base_model.model.model.layers.15.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.15.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.15.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.16.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.16.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.16.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.16.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.16.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.16.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.16.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.16.input_layernorm.weight\n",
      "False - base_model.model.model.layers.16.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.16.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.16.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.17.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.17.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.17.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.17.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.17.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.17.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.17.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.17.input_layernorm.weight\n",
      "False - base_model.model.model.layers.17.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.17.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.17.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.18.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.18.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.18.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.18.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.18.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.18.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.18.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.18.input_layernorm.weight\n",
      "False - base_model.model.model.layers.18.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.18.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.18.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.19.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.19.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.19.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.19.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.19.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.19.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.19.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.19.input_layernorm.weight\n",
      "False - base_model.model.model.layers.19.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.19.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.19.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.20.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.20.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.20.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.20.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.20.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.20.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.20.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.20.input_layernorm.weight\n",
      "False - base_model.model.model.layers.20.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.20.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.20.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.21.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.21.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.21.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.21.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.21.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.21.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.21.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.21.input_layernorm.weight\n",
      "False - base_model.model.model.layers.21.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.21.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.21.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.22.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.22.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.22.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.22.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.22.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.22.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.22.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.22.input_layernorm.weight\n",
      "False - base_model.model.model.layers.22.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.22.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.22.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.23.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.23.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.23.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.23.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.23.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.23.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.23.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.23.input_layernorm.weight\n",
      "False - base_model.model.model.layers.23.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.23.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.23.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.24.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.24.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.24.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.24.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.24.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.24.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.24.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.24.input_layernorm.weight\n",
      "False - base_model.model.model.layers.24.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.24.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.24.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.25.self_attn.k_proj.weight\n",
      "False - base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight\n",
      "True - base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
      "True - base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
      "False - base_model.model.model.layers.25.self_attn.o_proj.weight\n",
      "False - base_model.model.model.layers.25.self_attn.q_norm.weight\n",
      "False - base_model.model.model.layers.25.self_attn.k_norm.weight\n",
      "False - base_model.model.model.layers.25.mlp.gate_proj.weight\n",
      "False - base_model.model.model.layers.25.mlp.up_proj.weight\n",
      "False - base_model.model.model.layers.25.mlp.down_proj.weight\n",
      "False - base_model.model.model.layers.25.input_layernorm.weight\n",
      "False - base_model.model.model.layers.25.post_attention_layernorm.weight\n",
      "False - base_model.model.model.layers.25.pre_feedforward_layernorm.weight\n",
      "False - base_model.model.model.layers.25.post_feedforward_layernorm.weight\n",
      "False - base_model.model.model.norm.weight\n",
      "True - base_model.model.lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze lm_head parameters\n",
    "for name, param in gemma_model_qlora.named_parameters():\n",
    "    if \"lm_head\" in name:\n",
    "        param.requires_grad=True\n",
    "    \n",
    "    print(f\"{param.requires_grad} - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Here is a text:\n",
    "{}\n",
    "\n",
    "Classify it into one of the following class: [0, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "The correct class is: {}\"\"\"\n",
    "\n",
    "\n",
    "def formatting_prompts_func(dataset_):\n",
    "    # this is to fix an issue with the transformers library where the first time this function is called, it is called with a string for some reason\n",
    "    if isinstance(dataset_['text'], str):\n",
    "        return [\" \"]*100\n",
    "        \n",
    "    texts = []\n",
    "    for i in range(len(dataset_['text'])):\n",
    "        t = dataset_['text'][i]\n",
    "        label = dataset_['labels'][i]\n",
    "        text = prompt.format(t, label)\n",
    "        texts.append(text)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this custom collator is needed to change the sequence labels from yes_token_id and no_token_id to 1 and 0. It also trains only on the last token of the sequence.\n",
    "# NOT: Format of batch['labels'] is different than 'labels' column of training data!\n",
    "class DataCollatorForLastTokenLM(DataCollatorForLanguageModeling):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        mlm: bool = False,\n",
    "        ignore_index: int = -100, # this will be used in loss calculation in order not to include input tokens, we will try to include only target prediction token\n",
    "        pad_token_id: int = tokenizer_gemma.pad_token_type_id, # it should be padding token id of the chosen base model\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, mlm=mlm, **kwargs)\n",
    "        self.ignore_index = ignore_index\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        batch = super().torch_call(examples)\n",
    "\n",
    "        for i in range(len(examples)):\n",
    "            # Find the last non-padding token (since tokenizer of gemma3 uses left padding there is no need to use below code!)\n",
    "            ## last_token_idx = (batch[\"labels\"][i] != self.pad_token_id).nonzero()[-1].item()\n",
    "\n",
    "            last_token_idx = batch[\"labels\"][i].shape[0]-1\n",
    "            # Set all labels to ignore_index except for the last token\n",
    "            batch[\"labels\"][i, :last_token_idx] = self.ignore_index\n",
    "            # The old labels for the category tokens need to be mapped to their corresponding class numbers\n",
    "            last_token = batch[\"labels\"][i, last_token_idx]\n",
    "            if last_token==token_id_0:\n",
    "                batch[\"labels\"][i, last_token_idx] = 0\n",
    "            elif last_token==token_id_1:\n",
    "                batch[\"labels\"][i, last_token_idx] = 1\n",
    "            elif last_token==token_id_2:\n",
    "                batch[\"labels\"][i, last_token_idx] = 2\n",
    "            elif last_token==token_id_3:\n",
    "                batch[\"labels\"][i, last_token_idx] = 3\n",
    "            elif last_token==token_id_4:\n",
    "                batch[\"labels\"][i, last_token_idx] = 4\n",
    "            elif last_token==token_id_5:\n",
    "                batch[\"labels\"][i, last_token_idx] = 5\n",
    "            elif last_token==token_id_6:\n",
    "                batch[\"labels\"][i, last_token_idx] = 6\n",
    "\n",
    "        return batch\n",
    "\n",
    "collator = DataCollatorForLastTokenLM(tokenizer=tokenizer_gemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b809b9ccb2474d66a5df319ddb56151b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/3595 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edab631edff548988825264221bec127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/3595 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50bb0f2ec60f45c38b491034055fd247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/3595 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09cfa73475174dada1befd3504c7d636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/3595 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30cc36abe46041b2ac5fc7d10ac29ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/3595 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff36bdd7d24a42539295a1a9c016dd36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to eval dataset:   0%|          | 0/857 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3abdc1b4fd4eda967cad5ccfa5d4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/857 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acdf7755cbcd44e99b0048f1103ace77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/857 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d6c9a8eab34a648121c1adb54f2ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/857 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17315e6fd5f14224ab796a3f970d41ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/857 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "It is strongly recommended to train Gemma3 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 05:49, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.579900</td>\n",
       "      <td>0.419297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.132100</td>\n",
       "      <td>0.457240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/toygunkarabas/Development/NLP & SLP/nlp_slp_env/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/Users/toygunkarabas/Development/NLP & SLP/nlp_slp_env/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=450, training_loss=0.3559852345784505, metrics={'train_runtime': 353.8368, 'train_samples_per_second': 20.32, 'train_steps_per_second': 1.272, 'total_flos': 2451481839040512.0, 'train_loss': 0.3559852345784505})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training HyperParameters\n",
    "lr = 2e-3\n",
    "weight_decay = 0.01\n",
    "batch_size = 16\n",
    "num_epochs = 2\n",
    "\n",
    "sft_training_args = TrainingArguments(\n",
    "    # output_dir=\"bert-category-classifier-teacher\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    group_by_length = True\n",
    ")\n",
    "\n",
    "sft_trainer = SFTTrainer(\n",
    "    model = gemma_model_qlora,\n",
    "    args = sft_training_args,\n",
    "    processing_class = tokenizer_gemma,\n",
    "    train_dataset = dataset_dict[\"train\"],\n",
    "    eval_dataset=dataset_dict[\"test\"],\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "sft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/toygunkarabas/Development/NLP & SLP/nlp_slp_env/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_models/fine_tuned_gemma_w_qlora/tokenizer_config.json',\n",
       " './fine_tuned_models/fine_tuned_gemma_w_qlora/special_tokens_map.json',\n",
       " './fine_tuned_models/fine_tuned_gemma_w_qlora/tokenizer.model',\n",
       " './fine_tuned_models/fine_tuned_gemma_w_qlora/added_tokens.json',\n",
       " './fine_tuned_models/fine_tuned_gemma_w_qlora/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_qlora_gemma_model_directory = \"./fine_tuned_models/fine_tuned_gemma_w_qlora\"\n",
    "\n",
    "# Save model\n",
    "gemma_model_qlora.save_pretrained(save_qlora_gemma_model_directory)\n",
    "# Save tokenizer\n",
    "tokenizer_gemma.save_pretrained(save_qlora_gemma_model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model first\n",
    "base_gemma_model = Gemma3ForCausalLM.from_pretrained(gemma_model_id)\n",
    "token_id_0 = tokenizer_gemma.encode(\"0\", add_special_tokens=False)[0]\n",
    "token_id_1 = tokenizer_gemma.encode(\"1\", add_special_tokens=False)[0]\n",
    "token_id_2 = tokenizer_gemma.encode(\"2\", add_special_tokens=False)[0]\n",
    "token_id_3 = tokenizer_gemma.encode(\"3\", add_special_tokens=False)[0]\n",
    "token_id_4 = tokenizer_gemma.encode(\"4\", add_special_tokens=False)[0]\n",
    "token_id_5 = tokenizer_gemma.encode(\"5\", add_special_tokens=False)[0]\n",
    "token_id_6 = tokenizer_gemma.encode(\"6\", add_special_tokens=False)[0]\n",
    "# keep only the 0,1,2,3,4,5,6 tokens from lm_head\n",
    "par = torch.nn.Parameter(torch.vstack([base_gemma_model.lm_head.weight[token_id_0, :], \n",
    "                                       base_gemma_model.lm_head.weight[token_id_1, :],\n",
    "                                       base_gemma_model.lm_head.weight[token_id_2, :],\n",
    "                                       base_gemma_model.lm_head.weight[token_id_3, :],\n",
    "                                       base_gemma_model.lm_head.weight[token_id_4, :],\n",
    "                                       base_gemma_model.lm_head.weight[token_id_5, :],\n",
    "                                       base_gemma_model.lm_head.weight[token_id_6, :]\n",
    "                                    ]))\n",
    "base_gemma_model.lm_head.weight = par\n",
    "# set vocab size as 7 (number of classes). we set this because while computing logits vocab_size parameter will be used!\n",
    "base_gemma_model.vocab_size = 7\n",
    "base_gemma_model.config.vocab_size = 7\n",
    "base_gemma_model.lm_head.out_features=7\n",
    "\n",
    "# Load LoRA model on top of base model\n",
    "loaded_qlora_gemma_model = PeftModel.from_pretrained(base_gemma_model, save_qlora_gemma_model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:33<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319: text: Here is a text:\n",
      "Le porte-parole de Bush, Gordon Johndroe, a qualifié la promesse de la Corée du Nord « d'étape majeure vers l'objectif de la dénucléarisation vérifiable de la péninsule coréenne ».\n",
      "\n",
      "Classify it into one of the following class: [0, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "The correct class is: \n",
      " pred: 2 label: 2\n",
      "\n",
      "320: text: Here is a text:\n",
      "Le médaillé d'or olympique devait nager au 100 m et au 200 m nage libre et dans trois relais aux Jeux du Commonwealth, mais en raison de ses plaintes, sa condition physique a été mise en doute.\n",
      "\n",
      "Classify it into one of the following class: [0, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "The correct class is: \n",
      " pred: 3 label: 3\n",
      "\n",
      "321: text: Here is a text:\n",
      "El ganador olímpico de la medalla de oro debía nadar en el estilo libre de 100 metros y 200 metros, y en tres relevos en los Juegos de la Commonwealth, pero su condición física ha sido puesta en dudas a raíz de sus quejas.\n",
      "\n",
      "Classify it into one of the following class: [0, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "The correct class is: \n",
      " pred: 3 label: 3\n",
      "\n",
      "322: text: Here is a text:\n",
      "Seine tausendste Briefmarke, aus dem Jahr 2000,  zeigt das prächtige Gemälde „Rühmliche Heldentaten schwedischer Könige“ von David Klöcker Ehrenstrahl und ist im Guinness-Buch der Rekorde verzeichnet.\n",
      "\n",
      "Classify it into one of the following class: [0, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "The correct class is: \n",
      " pred: 2 label: 5\n",
      "\n",
      "323: text: Here is a text:\n",
      "Von den 1.400 Personen, die vor den Bundeswahlen 2010 befragt wurden, hat der Anteil derjenigen, die sich dagegen aussprechen, dass Australien zur Republik wird, seit 2008 um 8 Prozent zugenommen.\n",
      "\n",
      "Classify it into one of the following class: [0, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "The correct class is: \n",
      " pred: 2 label: 2\n",
      "\n",
      "324: text: Here is a text:\n",
      "Olimpiyat altın madalya sahibinin 100m ve 200m serbest stil ve İngiliz Milletler Topluluğu Oyunlarında üç bayrak yarışında yüzmesi gerekiyordu fakat şikayetlerinden dolayı zindeliği şüphe altında.\n",
      "\n",
      "Classify it into one of the following class: [0, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "The correct class is: \n",
      " pred: 3 label: 3\n",
      "\n",
      "325: text: Here is a text:\n",
      "Le skieur polonais malvoyant Maciej Krezel et sa guide Anna Ogarzynska ont terminé à la treizième place du Super-G. Le sud coréen Jong Seork Park a terminé à la vingt-quatrième place du Super-G assis masculin.\n",
      "\n",
      "Classify it into one of the following class: [0, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "The correct class is: \n",
      " pred: 3 label: 3\n",
      "\n",
      "326: text: Here is a text:\n",
      "Un sondage effectué auprès de 1 400 personnes avant les élections fédérales de 2010 a révélé que le nombre d'opposants à la transformation de l'Australie en république avait augmenté de 8 % depuis 2008.\n",
      "\n",
      "Classify it into one of the following class: [0, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "The correct class is: \n",
      " pred: 2 label: 2\n",
      "\n",
      "327: text: Here is a text:\n",
      "Der Goldmedaillengewinner bei olympischen Spielen sollte bei den Commonwealth Games in den Disziplinen 100 m und 200 m Freistil sowie in drei Lagenstaffeln schwimmen, aber wegen seiner vielen Beschwerden wurde seine Fitness angezweifelt.\n",
      "\n",
      "Classify it into one of the following class: [0, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "The correct class is: \n",
      " pred: 3 label: 3\n",
      "\n",
      "328: text: Here is a text:\n",
      "Polonya erkekler görme engelli kayakçısı Maciej Krezel ve rehber Anna Ogarzynska, Super-G.'yi on üçüncü olarak bitirdi. Güney Koreli Jong Seork Park, Super-G. erkekler oturan kategorisini yirmi dördüncü olarak bitirdi.\n",
      "\n",
      "Classify it into one of the following class: [0, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "The correct class is: \n",
      " pred: 3 label: 3\n",
      "\n",
      "329: text: Here is a text:\n",
      "Der sehbehinderte polnische Skifahrer Maciej Krezel und die Begleitläuferin Anna Ogarzynska belegten im Super-G den dreizehnten Platz. Der Südkoreaner Jong Seork Park belegte im Super-G der Männer den vierundzwanzigsten Platz.\n",
      "\n",
      "Classify it into one of the following class: [0, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "The correct class is: \n",
      " pred: 3 label: 3\n",
      "\n",
      "330: text: Here is a text:\n",
      "18. yüzyıl pazarı, tarihi camiler ve kiliseler ile şehir geleneksel Türkiye'den daha çok Akdeniz Avrupası havasına sahip olmasına rağmen geniş bulvarlar, cam binalar ve modern alışveriş merkezleri geleneksel kırmızı kiremitli çatılarla çevrilidir.\n",
      "\n",
      "Classify it into one of the following class: [0, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "The correct class is: \n",
      " pred: 1 label: 1\n",
      "\n",
      "331: text: Here is a text:\n",
      "El esquiador del grupo masculino con discapacidad visual, Maciej Krezel, y su guía, Anna Ogarzynska, de Polonia, terminaron en el decimotercer lugar en el Super-G. Jong Seork Park de Corea del Sur obtuvo el vigésimo cuarto puesto en el grupo masculino sentados.\n",
      "\n",
      "Classify it into one of the following class: [0, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "The correct class is: \n",
      " pred: 3 label: 3\n",
      "\n",
      "332: text: Here is a text:\n",
      "Die breiten Boulevards, verglasten Gebäude und modernen Einkaufszentren sind geprägt von klassischen roten Ziegeldächern, dem Markt aus dem 18. Jahrhundert sowie alten Moscheen und Kirchen, obwohl die Stadt eher eine Atmosphäre des mediterranen Europas ausstrahlt als die der traditionellen Türkei.\n",
      "\n",
      "Classify it into one of the following class: [0, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "The correct class is: \n",
      " pred: 5 label: 1\n",
      "\n",
      "333: text: Here is a text:\n",
      "Les larges boulevards, les bâtiments vitrés et les centres commerciaux modernes sont parsemés de toits traditionnels en tuiles rouges, du marché du XVIIIe siècle, et de vieilles mosquées et églises, bien que la ville ait une atmosphère qui rappelle plus l'Europe méditerranéenne que la Turquie traditionnelle.\n",
      "\n",
      "Classify it into one of the following class: [0, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "The correct class is: \n",
      " pred: 6 label: 1\n",
      "\n",
      "334: text: Here is a text:\n",
      "La estructura que presenta el plumaje sugiere que su función no estaba relacionada con el vuelo, sino que las usaban para regular la temperatura o como indicador de la misma. Los investigadores sostienen que, aunque se trata de la cola de un dinosaurio joven, la muestra analizada presenta rasgos del plumaje de un adulto y no de un polluelo.\n",
      "\n",
      "Classify it into one of the following class: [0, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "The correct class is: \n",
      " pred: 0 label: 0\n",
      "\n",
      "335: text: Here is a text:\n",
      "Die Struktur der Federn lässt vermuten, dass sie nicht zum Fliegen,, sondern eher zur Temperaturregulierung oder zum Herzeigen verwendet wurden. Die Forscher vermuteten, dass die Probe, obwohl es sich um den Schwanz eines jungen Dinosauriers handelt, das Gefieder eines Ausgewachsenen zeigt und nicht die Daunen eines Kükens.\n",
      "\n",
      "Classify it into one of the following class: [0, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "The correct class is: \n",
      " pred: 0 label: 0\n",
      "\n",
      "336: text: Here is a text:\n",
      "As NSA, he assisted Carter in diplomatically handling world affairs, such as the Camp David Accords, 1978; normalizing US–China relations thought the late 1970s; the Iranian Revolution, which led to the Iran hostage crisis, 1979; and the Soviet invasion in Afghanistan, 1979.\n",
      "\n",
      "Classify it into one of the following class: [0, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "The correct class is: \n",
      " pred: 2 label: 2\n",
      "\n",
      "337: text: Here is a text:\n",
      "La structure des plumes suggère qu’elles n’étaient pas utilisées en vol mais plutôt pour la régulation ou l’affichage de la température. Les chercheurs ont suggéré que, même s’il s’agit de la queue d’un jeune dinosaure, l’échantillon montre un plumage d’adulte et non le duvet d’un poussin.\n",
      "\n",
      "Classify it into one of the following class: [0, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "The correct class is: \n",
      " pred: 0 label: 0\n",
      "\n",
      "338: text: Here is a text:\n",
      "Tanto los extensos bulevares como las edificaciones con frentes espejados y los centros comerciales vanguardistas están salpicados por techos de tejas rojas tradicionales, del mercado del siglo XVIII y de las antiguas mezquitas e iglesias. Aun así, su ambiente se asemeja más al de la Europa mediterránea que al de la Turquía tradicional.\n",
      "\n",
      "Classify it into one of the following class: [0, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "The correct class is: \n",
      " pred: 6 label: 1\n",
      "\n",
      "339: text: Here is a text:\n",
      "NSA olarak, 1978 Camp David Sözleşmesi; 1970'lerin sonlarında ABD-Çin ilişkilerinin normalleşmesi; 1979’daki İran rehine krizine yol açan İran Devrimi ve 1979’da Afganistan'ın Sovyetler tarafından işgali gibi dünya meselelerinin diplomatik olarak ele alınmasında Carter'a yardımcı oldu.\n",
      "\n",
      "Classify it into one of the following class: [0, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "The correct class is: \n",
      " pred: 2 label: 2\n",
      "\n",
      "340: text: Here is a text:\n",
      "Als nationaler Sicherheitsberater unterstützte er Carter bei der diplomatischen Handhabung von Weltangelegenheiten wie dem Camp-David-Abkommen von 1978, der Normalisierung der Beziehungen zwischen den USA und China Ende der 1970er Jahre, der iranischen Revolution, die zur Geiselkrise im Iran 1979 führte, und der sowjetischen Invasion in Afghanistan 1979.\n",
      "\n",
      "Classify it into one of the following class: [0, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "The correct class is: \n",
      " pred: 2 label: 2\n",
      "\n",
      "341: text: Here is a text:\n",
      "Como consejero de seguridad nacional, asistió a Carter en el manejo diplomático de asuntos internacionales, como los Acuerdos de Camp David de 1978, la normalización de las relaciones entre Estados Unidos y China a fines de la década de 1970, la Revolución iraní de 1979, la que llevó a una crisis de rehenes en Irán, y la invasión soviética en Afganistán, en 1979.\n",
      "\n",
      "Classify it into one of the following class: [0, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "The correct class is: \n",
      " pred: 2 label: 2\n",
      "\n",
      "342: text: Here is a text:\n",
      "Pendant son temps au sein de la NSA, il a aidé Carter à résoudre diplomatiquement des questions internationales telles que les accords de Camp David, en 1978, la normalisation des relations entre les États-Unis et la Chine à la fin des années 1970, la révolution iranienne et ses conséquences comme la prise d'otages en Iran, en 1979, et l'invasion des forces soviétiques en Afghanistan, en 1979.\n",
      "\n",
      "Classify it into one of the following class: [0, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "The correct class is: \n",
      " pred: 2 label: 2\n",
      "\n",
      "Correct: 299 Total: 343 Accuracy: 0.8717201166180758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Tokenize the inputs and sort them by their tokenized length\n",
    "tokenized_inputs = []\n",
    "for i in range(len(dataset_dict[\"validation\"]['text'])):\n",
    "    text = dataset_dict[\"validation\"]['text'][i]\n",
    "    test_str = prompt.format(text, \"\")\n",
    "    tokenized_input = tokenizer_gemma(test_str, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    tokenized_inputs.append((tokenized_input, test_str, dataset_dict[\"validation\"]['labels'][i]))\n",
    "\n",
    "# Sort by tokenized length\n",
    "tokenized_inputs.sort(key=lambda x: x[0]['input_ids'].shape[1])\n",
    "\n",
    "# Step 2: Group the inputs by their tokenized length\n",
    "grouped_inputs = defaultdict(list)\n",
    "for tokenized_input, test_str, label in tokenized_inputs:\n",
    "    length = tokenized_input['input_ids'].shape[1]\n",
    "    grouped_inputs[length].append((tokenized_input, test_str, label))\n",
    "\n",
    "# Step 3: Process each group in batches of 64\n",
    "batch_size = 64\n",
    "all_outputs = []\n",
    "all_strings = []\n",
    "all_labels = []\n",
    "\n",
    "for length, group in tqdm(grouped_inputs.items()):\n",
    "    for i in range(0, len(group), batch_size):\n",
    "        batch = group[i:i+batch_size]\n",
    "        batch_inputs = [item[0] for item in batch]\n",
    "        batch_strings = [item[1] for item in batch]\n",
    "        batch_labels = [item[2] for item in batch]\n",
    "\n",
    "        # Concatenate the batch inputs\n",
    "        input_ids = torch.cat([item['input_ids'] for item in batch_inputs], dim=0).to(\"cpu\")\n",
    "        attention_mask = torch.cat([item['attention_mask'] for item in batch_inputs], dim=0).to(\"cpu\")\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = loaded_qlora_gemma_model.to(\"cpu\")(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            # print(outputs.logits[:, -1].shape)\n",
    "        \n",
    "        # logits are shape (batch_size, sequence_length, num_classes), we want only the last token of each sequence in the batch\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        \n",
    "        # Apply softmax\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = torch.argmax(probabilities, dim=-1)\n",
    "        \n",
    "        all_outputs.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(batch_labels)\n",
    "        all_strings.extend(batch_strings)\n",
    "\n",
    "# Step 4: Do the label assignment\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for i in range(len(all_outputs)):\n",
    "    pred = str(all_outputs[i])\n",
    "    label = str(all_labels[i])\n",
    "    if i > len(all_outputs) - 25:\n",
    "        print(f\"{i}: text: {all_strings[i]}\\n pred: {pred} label: {label}\\n\")\n",
    "\n",
    "    if pred == label:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(f\"Correct: {correct} Total: {total} Accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Lora with Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is an issue while installing unsloth. The issue I got: \"ERROR: Could not build wheels for xformers, which is required to install pyproject.toml-based projects\"\n",
    "\n",
    "# I will check & try it later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning all parameters of roberta model is inefficient and gives bad perfromance on validation data. Although, while the training both training and test losses mostly decreases accuracy remains the same and also in validation set accuracy is 28%. If model classifies all observations as class 0, the accuracy will also be 28%. So, this model does not learn anything with fine tuning.\n",
    "\n",
    "Fine tuning with transfer learning teacher model (roberta model) learned the dataset well compared to all parameter fine tuning approach and results in 76% accuracy in validation set. Morover, fine tuning student model (bert model) with fine tuned teacher model improves the accuracy from 76% to 86%. Rather than fien tuning all parameters, fine tuning only classification head results in faster training and better classification performance.\n",
    "\n",
    "Fine tuning roberta model with LoRA gives similar results with fine tuning all parameters of roberta model. Learning adoptors and classifcaiton head with training hyperparameters may not be enough. However, with QLoRA approach and same training hyperparameters provides superior performances among all fine tuning approach with validation accuracy as 90%.\n",
    "\n",
    "Fine tuning gemma3-1b model is different than the others. With propoer preprocessing model parameters and custom data collator, the fine tuned model's validation accuracy is 87% which makes it second best fine tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "### Transfer Learning & PEFT\n",
    "\n",
    "* https://www.youtube.com/watch?v=bZcKYiwtw1I&ab_channel=NeuralBreakdownwithAVB\n",
    "\n",
    "* https://www.youtube.com/watch?v=eC6Hd1hFvos&list=PLz-ep5RbHosUwDlaic4w8u2NdFgQiX_a_&ab_channel=ShawTalebi\n",
    "\n",
    "* https://www.youtube.com/watch?v=4QHg8Ix8WWQ&list=PLz-ep5RbHosUwDlaic4w8u2NdFgQiX_a_&index=5&ab_channel=ShawTalebi\n",
    "\n",
    "* https://www.youtube.com/watch?v=YJNbgusTSF0&ab_channel=TradeMamba\n",
    "\n",
    "* https://www.youtube.com/watch?v=4nNbg4bWDrQ&ab_channel=Rohan-Paul-AI\n",
    "\n",
    "* https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07\n",
    "\n",
    "### Unsloth\n",
    "\n",
    "* https://github.com/timothelaborie/text_classification_scripts/blob/main/unsloth_classification.ipynb\n",
    "\n",
    "* https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B).ipynb#scrollTo=_rD6fl8EUxnG\n",
    "\n",
    "* https://www.youtube.com/watch?v=Gpyukc6c0w8&ab_channel=MervinPraison\n",
    "\n",
    "* https://www.youtube.com/watch?v=JJWvYQdOVOY&list=WL&index=108&ab_channel=NodematicTutorials\n",
    "\n",
    "* https://www.youtube.com/watch?v=YZW3pkIR-YE&t=10s&ab_channel=PromptEngineering\n",
    "\n",
    "* https://www.youtube.com/watch?v=pxhkDaKzBaY&ab_channel=warpdotdev\n",
    "\n",
    "* https://www.youtube.com/watch?v=jFl5Fewrieo&ab_channel=AIJason\n",
    "\n",
    "* https://www.youtube.com/watch?v=qcNmOItRw4U&ab_channel=DataCamp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_slp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
